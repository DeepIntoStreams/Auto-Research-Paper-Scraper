Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
Neural MP: A Generalist Neural Motion Planner,09/09/2024,"Murtaza Dalal, Jiahui Yang, Russell Mendonca, Youssef Khaky, Ruslan Salakhutdinov, Deepak Pathak","The current paradigm for motion planning generates solutions from scratch for
every new problem, which consumes significant amounts of time and computational
resources. For complex, cluttered scenes, motion planning approaches can often
take minutes to produce a solution, while humans are able to accurately and
safely reach any goal in seconds by leveraging their prior experience. We seek
to do the same by applying data-driven learning at scale to the problem of
motion planning. Our approach builds a large number of complex scenes in
simulation, collects expert data from a motion planner, then distills it into a
reactive generalist policy. We then combine this with lightweight optimization
to obtain a safe path for real world deployment. We perform a thorough
evaluation of our method on 64 motion planning tasks across four diverse
environments with randomized poses, scenes and obstacles, in the real world,
demonstrating an improvement of 23%, 17% and 79% motion planning success rate
over state of the art sampling, optimization and learning based planning
methods. Video results available at mihdalal.github.io/neuralmotionplanner",http://arxiv.org/pdf/2409.05864v1,,False
Celcomen: spatial causal disentanglement for single-cell and tissue perturbation modeling,09/09/2024,"Stathis Megas, Daniel G. Chen, Krzysztof Polanski, Moshe Eliasof, Carola-Bibiane Schonlieb, Sarah A. Teichmann","Celcomen leverages a mathematical causality framework to disentangle intra-
and inter- cellular gene regulation programs in spatial transcriptomics and
single-cell data through a generative graph neural network. It can learn
gene-gene interactions, as well as generate post-perturbation counterfactual
spatial transcriptomics, thereby offering access to experimentally inaccessible
samples. We validated its disentanglement, identifiability, and counterfactual
prediction capabilities through simulations and in clinically relevant human
glioblastoma, human fetal spleen, and mouse lung cancer samples. Celcomen
provides the means to model disease and therapy induced changes allowing for
new insights into single-cell spatially resolved tissue responses relevant to
human health.",http://arxiv.org/pdf/2409.05804v1,,False
NeurLZ: On Systematically Enhancing Lossy Compression Performance for Scientific Data based on Neural Learning with Error Control,09/09/2024,"Wenqi Jia, Youyuan Liu, Zhewen Hu, Jinzhen Wang, Boyuan Zhang, Wei Niu, Junzhou Huang, Stavros Kalafatis, Sian Jin, Miao Yin","Large-scale scientific simulations generate massive datasets that pose
significant challenges for storage and I/O. While traditional lossy compression
techniques can improve performance, balancing compression ratio, data quality,
and throughput remains difficult. To address this, we propose NeurLZ, a novel
cross-field learning-based and error-controlled compression framework for
scientific data. By integrating skipping DNN models, cross-field learning, and
error control, our framework aims to substantially enhance lossy compression
performance. Our contributions are three-fold: (1) We design a lightweight
skipping model to provide high-fidelity detail retention, further improving
prediction accuracy. (2) We adopt a cross-field learning approach to
significantly improve data prediction accuracy, resulting in a substantially
improved compression ratio. (3) We develop an error control approach to provide
strict error bounds according to user requirements. We evaluated NeurLZ on
several real-world HPC application datasets, including Nyx (cosmological
simulation), Miranda (large turbulence simulation), and Hurricane (weather
simulation). Experiments demonstrate that our framework achieves up to a 90%
relative reduction in bit rate under the same data distortion, compared to the
best existing approach.",http://arxiv.org/pdf/2409.05785v1,,False
Real-time optimal control of high-dimensional parametrized systems by deep learning-based reduced order models,09/09/2024,"Matteo Tomasetto, Andrea Manzoni, Francesco Braghin","Steering a system towards a desired target in a very short amount of time is
challenging from a computational standpoint. Indeed, the intrinsically
iterative nature of optimal control problems requires multiple simulations of
the physical system to be controlled. Moreover, the control action needs to be
updated whenever the underlying scenario undergoes variations. Full-order
models based on, e.g., the Finite Element Method, do not meet these
requirements due to the computational burden they usually entail. On the other
hand, conventional reduced order modeling techniques such as the Reduced Basis
method, are intrusive, rely on a linear superimposition of modes, and lack of
efficiency when addressing nonlinear time-dependent dynamics. In this work, we
propose a non-intrusive Deep Learning-based Reduced Order Modeling (DL-ROM)
technique for the rapid control of systems described in terms of parametrized
PDEs in multiple scenarios. In particular, optimal full-order snapshots are
generated and properly reduced by either Proper Orthogonal Decomposition or
deep autoencoders (or a combination thereof) while feedforward neural networks
are exploited to learn the map from scenario parameters to reduced optimal
solutions. Nonlinear dimensionality reduction therefore allows us to consider
state variables and control actions that are both low-dimensional and
distributed. After (i) data generation, (ii) dimensionality reduction, and
(iii) neural networks training in the offline phase, optimal control strategies
can be rapidly retrieved in an online phase for any scenario of interest. The
computational speedup and the high accuracy obtained with the proposed approach
are assessed on different PDE-constrained optimization problems, ranging from
the minimization of energy dissipation in incompressible flows modelled through
Navier-Stokes equations to the thermal active cooling in heat transfer.",http://arxiv.org/pdf/2409.05709v1,,False
On the Convergence of Sigmoid and tanh Fuzzy General Grey Cognitive Maps,09/09/2024,"Xudong Gao, Xiao Guang Gao, Jia Rong, Ni Li, Yifeng Niu, Jun Chen","Fuzzy General Grey Cognitive Map (FGGCM) and Fuzzy Grey Cognitive Map (FGCM)
are extensions of Fuzzy Cognitive Map (FCM) in terms of uncertainty. FGGCM
allows for the processing of general grey number with multiple intervals,
enabling FCM to better address uncertain situations. Although the convergence
of FCM and FGCM has been discussed in many literature, the convergence of FGGCM
has not been thoroughly explored. This paper aims to fill this research gap.
First, metrics for the general grey number space and its vector space is given
and proved using the Minkowski inequality. By utilizing the characteristic that
Cauchy sequences are convergent sequences, the completeness of these two space
is demonstrated. On this premise, utilizing Banach fixed point theorem and
Browder-Gohde-Kirk fixed point theorem, combined with Lagrange's mean value
theorem and Cauchy's inequality, deduces the sufficient conditions for FGGCM to
converge to a unique fixed point when using tanh and sigmoid functions as
activation functions. The sufficient conditions for the kernels and greyness of
FGGCM to converge to a unique fixed point are also provided separately.
Finally, based on Web Experience and Civil engineering FCM, designed
corresponding FGGCM with sigmoid and tanh as activation functions by modifying
the weights to general grey numbers. By comparing with the convergence theorems
of FCM and FGCM, the effectiveness of the theorems proposed in this paper was
verified. It was also demonstrated that the convergence theorems of FCM are
special cases of the theorems proposed in this paper. The study for convergence
of FGGCM is of great significance for guiding the learning algorithm of FGGCM,
which is needed for designing FGGCM with specific fixed points, lays a solid
theoretical foundation for the application of FGGCM in fields such as control,
prediction, and decision support systems.",http://arxiv.org/pdf/2409.05565v1,,False
Optimizing VarLiNGAM for Scalable and Efficient Time Series Causal Discovery,09/09/2024,"Ziyang Jiao, Ce Guo, Wayne Luk","Causal discovery is designed to identify causal relationships in data, a task
that has become increasingly complex due to the computational demands of
traditional methods such as VarLiNGAM, which combines Vector Autoregressive
Model with Linear Non-Gaussian Acyclic Model for time series data.
  This study is dedicated to optimising causal discovery specifically for time
series data, which is common in practical applications. Time series causal
discovery is particularly challenging due to the need to account for temporal
dependencies and potential time lag effects. By designing a specialised dataset
generator and reducing the computational complexity of the VarLiNGAM model from
\( O(m^3 \cdot n) \) to \( O(m^3 + m^2 \cdot n) \), this study significantly
improves the feasibility of processing large datasets. The proposed methods
have been validated on advanced computational platforms and tested across
simulated, real-world, and large-scale datasets, showcasing enhanced efficiency
and performance. The optimised algorithm achieved 7 to 13 times speedup
compared with the original algorithm and around 4.5 times speedup compared with
the GPU-accelerated version on large-scale datasets with feature sizes between
200 and 400.
  Our methods aim to push the boundaries of current causal discovery
capabilities, making them more robust, scalable, and applicable to real-world
scenarios, thus facilitating breakthroughs in various fields such as healthcare
and finance.",http://arxiv.org/pdf/2409.05500v1,,False
Retrofitting Temporal Graph Neural Networks with Transformer,09/09/2024,"Qiang Huang, Xiao Yan, Xin Wang, Susie Xi Rao, Zhichao Han, Fangcheng Fu, Wentao Zhang, Jiawei Jiang","Temporal graph neural networks (TGNNs) outperform regular GNNs by
incorporating time information into graph-based operations. However, TGNNs
adopt specialized models (e.g., TGN, TGAT, and APAN ) and require tailored
training frameworks (e.g., TGL and ETC). In this paper, we propose TF-TGN,
which uses Transformer decoder as the backbone model for TGNN to enjoy
Transformer's codebase for efficient training. In particular, Transformer
achieves tremendous success for language modeling, and thus the community
developed high-performance kernels (e.g., flash-attention and memory-efficient
attention) and efficient distributed training schemes (e.g., PyTorch FSDP,
DeepSpeed, and Megatron-LM). We observe that TGNN resembles language modeling,
i.e., the message aggregation operation between chronologically occurring nodes
and their temporal neighbors in TGNNs can be structured as sequence modeling.
Beside this similarity, we also incorporate a series of algorithm designs
including suffix infilling, temporal graph attention with self-loop, and causal
masking self-attention to make TF-TGN work. During training, existing systems
are slow in transforming the graph topology and conducting graph sampling. As
such, we propose methods to parallelize the CSR format conversion and graph
sampling. We also adapt Transformer codebase to train TF-TGN efficiently with
multiple GPUs. We experiment with 9 graphs and compare with 2 state-of-the-art
TGNN training frameworks. The results show that TF-TGN can accelerate training
by over 2.20 while providing comparable or even superior accuracy to existing
SOTA TGNNs. TF-TGN is available at https://github.com/qianghuangwhu/TF-TGN.",http://arxiv.org/pdf/2409.05477v1,,False
Sequential Posterior Sampling with Diffusion Models,09/09/2024,"Tristan S. W. Stevens, Ois√≠n Nolan, Jean-Luc Robert, Ruud J. G. van Sloun","Diffusion models have quickly risen in popularity for their ability to model
complex distributions and perform effective posterior sampling. Unfortunately,
the iterative nature of these generative models makes them computationally
expensive and unsuitable for real-time sequential inverse problems such as
ultrasound imaging. Considering the strong temporal structure across sequences
of frames, we propose a novel approach that models the transition dynamics to
improve the efficiency of sequential diffusion posterior sampling in
conditional image synthesis. Through modeling sequence data using a video
vision transformer (ViViT) transition model based on previous diffusion
outputs, we can initialize the reverse diffusion trajectory at a lower noise
scale, greatly reducing the number of iterations required for convergence. We
demonstrate the effectiveness of our approach on a real-world dataset of high
frame rate cardiac ultrasound images and show that it achieves the same
performance as a full diffusion trajectory while accelerating inference
25$\times$, enabling real-time posterior sampling. Furthermore, we show that
the addition of a transition model improves the PSNR up to 8\% in cases with
severe motion. Our method opens up new possibilities for real-time applications
of diffusion models in imaging and other domains requiring real-time inference.",http://arxiv.org/pdf/2409.05399v1,,False
Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling,09/09/2024,"Georgios Pantazopoulos, Malvina Nikandrou, Alessandro Suglia, Oliver Lemon, Arash Eshghi","This study explores replacing Transformers in Visual Language Models (VLMs)
with Mamba, a recent structured state space model (SSM) that demonstrates
promising performance in sequence modeling. We test models up to 3B parameters
under controlled conditions, showing that Mamba-based VLMs outperforms
Transformers-based VLMs in captioning, question answering, and reading
comprehension. However, we find that Transformers achieve greater performance
in visual grounding and the performance gap widens with scale. We explore two
hypotheses to explain this phenomenon: 1) the effect of task-agnostic visual
encoding on the updates of the hidden states, and 2) the difficulty in
performing visual grounding from the perspective of in-context multimodal
retrieval. Our results indicate that a task-aware encoding yields minimal
performance gains on grounding, however, Transformers significantly outperform
Mamba at in-context multimodal retrieval. Overall, Mamba shows promising
performance on tasks where the correct output relies on a summary of the image
but struggles when retrieval of explicit information from the context is
required.",http://arxiv.org/pdf/2409.05395v1,,False
On the Convergence Analysis of Over-Parameterized Variational Autoencoders: A Neural Tangent Kernel Perspective,09/09/2024,"Li Wang, Wei Huang","Variational Auto-Encoders (VAEs) have emerged as powerful probabilistic
models for generative tasks. However, their convergence properties have not
been rigorously proven. The challenge of proving convergence is inherently
difficult due to the highly non-convex nature of the training objective and the
implementation of a Stochastic Neural Network (SNN) within VAE architectures.
This paper addresses these challenges by characterizing the optimization
trajectory of SNNs utilized in VAEs through the lens of Neural Tangent Kernel
(NTK) techniques. These techniques govern the optimization and generalization
behaviors of ultra-wide neural networks. We provide a mathematical proof of VAE
convergence under mild assumptions, thus advancing the theoretical
understanding of VAE optimization dynamics. Furthermore, we establish a novel
connection between the optimization problem faced by over-parameterized SNNs
and the Kernel Ridge Regression (KRR) problem. Our findings not only contribute
to the theoretical foundation of VAEs but also open new avenues for
investigating the optimization of generative models using advanced kernel
methods. Our theoretical claims are verified by experimental simulations.",http://arxiv.org/pdf/2409.05349v1,,False
GDFlow: Anomaly Detection with NCDE-based Normalizing Flow for Advanced Driver Assistance System,09/09/2024,"Kangjun Lee, Minha Kim, Youngho Jun, Simon S. Woo","For electric vehicles, the Adaptive Cruise Control (ACC) in Advanced Driver
Assistance Systems (ADAS) is designed to assist braking based on driving
conditions, road inclines, predefined deceleration strengths, and user braking
patterns. However, the driving data collected during the development of ADAS
are generally limited and lack diversity. This deficiency leads to late or
aggressive braking for different users. Crucially, it is necessary to
effectively identify anomalies, such as unexpected or inconsistent braking
patterns in ADAS, especially given the challenge of working with unlabelled,
limited, and noisy datasets from real-world electric vehicles. In order to
tackle the aforementioned challenges in ADAS, we propose Graph Neural
Controlled Differential Equation Normalizing Flow (GDFlow), a model that
leverages Normalizing Flow (NF) with Neural Controlled Differential Equations
(NCDE) to learn the distribution of normal driving patterns continuously.
Compared to the traditional clustering or anomaly detection algorithms, our
approach effectively captures the spatio-temporal information from different
sensor data and more accurately models continuous changes in driving patterns.
Additionally, we introduce a quantile-based maximum likelihood objective to
improve the likelihood estimate of the normal data near the boundary of the
distribution, enhancing the model's ability to distinguish between normal and
anomalous patterns. We validate GDFlow using real-world electric vehicle
driving data that we collected from Hyundai IONIQ5 and GV80EV, achieving
state-of-the-art performance compared to six baselines across four dataset
configurations of different vehicle types and drivers. Furthermore, our model
outperforms the latest anomaly detection methods across four time series
benchmark datasets. Our approach demonstrates superior efficiency in inference
time compared to existing methods.",http://arxiv.org/pdf/2409.05346v1,,False
Resource-Efficient Generative AI Model Deployment in Mobile Edge Networks,09/09/2024,"Yuxin Liang, Peng Yang, Yuanyuan He, Feng Lyu","The surging development of Artificial Intelligence-Generated Content (AIGC)
marks a transformative era of the content creation and production. Edge servers
promise attractive benefits, e.g., reduced service delay and backhaul traffic
load, for hosting AIGC services compared to cloud-based solutions. However, the
scarcity of available resources on the edge pose significant challenges in
deploying generative AI models. In this paper, by characterizing the resource
and delay demands of typical generative AI models, we find that the consumption
of storage and GPU memory, as well as the model switching delay represented by
I/O delay during the preloading phase, are significant and vary across models.
These multidimensional coupling factors render it difficult to make efficient
edge model deployment decisions. Hence, we present a collaborative edge-cloud
framework aiming to properly manage generative AI model deployment on the edge.
Specifically, we formulate edge model deployment problem considering
heterogeneous features of models as an optimization problem, and propose a
model-level decision selection algorithm to solve it. It enables pooled
resource sharing and optimizes the trade-off between resource consumption and
delay in edge generative AI model deployment. Simulation results validate the
efficacy of the proposed algorithm compared with baselines, demonstrating its
potential to reduce overall costs by providing feature-aware model deployment
decisions.",http://arxiv.org/pdf/2409.05303v1,,False
Learning Submodular Sequencing from Samples,09/09/2024,"Jing Yuan, Shaojie Tang","This paper addresses the problem of sequential submodular maximization:
selecting and ranking items in a sequence to optimize some composite submodular
function. In contrast to most of the previous works, which assume access to the
utility function, we assume that we are given only a set of samples. Each
sample includes a random sequence of items and its associated utility. We
present an algorithm that, given polynomially many samples drawn from a
two-stage uniform distribution, achieves an approximation ratio dependent on
the curvature of individual submodular functions. Our results apply in a wide
variety of real-world scenarios, such as ranking products in online retail
platforms, where complete knowledge of the utility function is often impossible
to obtain. Our algorithm gives an empirically useful solution in such contexts,
thus proving that limited data can be of great use in sequencing tasks. From a
technical perspective, our results extend prior work on ``optimization from
samples'' by generalizing from optimizing a set function to a
sequence-dependent function.",http://arxiv.org/pdf/2409.05265v1,,False
