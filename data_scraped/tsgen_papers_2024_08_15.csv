Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
Transformers and Large Language Models for Efficient Intrusion Detection Systems: A Comprehensive Survey,14/08/2024,Hamza Kheddar,"With significant advancements in Transformers LLMs, NLP has extended its
reach into many research fields due to its enhanced capabilities in text
generation and user interaction. One field benefiting greatly from these
advancements is cybersecurity. In cybersecurity, many parameters that need to
be protected and exchanged between senders and receivers are in the form of
text and tabular data, making NLP a valuable tool in enhancing the security
measures of communication protocols. This survey paper provides a comprehensive
analysis of the utilization of Transformers and LLMs in cyber-threat detection
systems. The methodology of paper selection and bibliometric analysis is
outlined to establish a rigorous framework for evaluating existing research.
The fundamentals of Transformers are discussed, including background
information on various cyber-attacks and datasets commonly used in this field.
The survey explores the application of Transformers in IDSs, focusing on
different architectures such as Attention-based models, LLMs like BERT and GPT,
CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.
Furthermore, it explores the diverse environments and applications where
Transformers and LLMs-based IDS have been implemented, including computer
networks, IoT devices, critical infrastructure protection, cloud computing,
SDN, as well as in autonomous vehicles. The paper also addresses research
challenges and future directions in this area, identifying key issues such as
interpretability, scalability, and adaptability to evolving threats, and more.
Finally, the conclusion summarizes the findings and highlights the significance
of Transformers and LLMs in enhancing cyber-threat detection capabilities,
while also outlining potential avenues for further research and development.",http://arxiv.org/pdf/2408.07583v1,,False
Dominating Set Reconfiguration with Answer Set Programming,14/08/2024,"Masato Kato, Torsten Schaub, Takehide Soh, Naoyuki Tamura, Mutsunori Banbara","The dominating set reconfiguration problem is defined as determining, for a
given dominating set problem and two among its feasible solutions, whether one
is reachable from the other via a sequence of feasible solutions subject to a
certain adjacency relation. This problem is PSPACE-complete in general. The
concept of the dominating set is known to be quite useful for analyzing
wireless networks, social networks, and sensor networks. We develop an approach
to solve the dominating set reconfiguration problem based on Answer Set
Programming (ASP). Our declarative approach relies on a high-level ASP
encoding, and both the grounding and solving tasks are delegated to an
ASP-based combinatorial reconfiguration solver. To evaluate the effectiveness
of our approach, we conduct experiments on a newly created benchmark set.",http://arxiv.org/pdf/2408.07510v1,,False
Faster Stochastic Optimization with Arbitrary Delays via Asynchronous Mini-Batching,14/08/2024,"Amit Attia, Ofir Gaash, Tomer Koren","We consider the problem of asynchronous stochastic optimization, where an
optimization algorithm makes updates based on stale stochastic gradients of the
objective that are subject to an arbitrary (possibly adversarial) sequence of
delays. We present a procedure which, for any given $q \in (0,1]$, transforms
any standard stochastic first-order method to an asynchronous method with
convergence guarantee depending on the $q$-quantile delay of the sequence. This
approach leads to convergence rates of the form $O(\tau_q/qT+\sigma/\sqrt{qT})$
for non-convex and $O(\tau_q^2/(q T)^2+\sigma/\sqrt{qT})$ for convex smooth
problems, where $\tau_q$ is the $q$-quantile delay, generalizing and improving
on existing results that depend on the average delay. We further show a method
that automatically adapts to all quantiles simultaneously, without any prior
knowledge of the delays, achieving convergence rates of the form $O(\inf_{q}
\tau_q/qT+\sigma/\sqrt{qT})$ for non-convex and $O(\inf_{q} \tau_q^2/(q
T)^2+\sigma/\sqrt{qT})$ for convex smooth problems. Our technique is based on
asynchronous mini-batching with a careful batch-size selection and filtering of
stale gradients.",http://arxiv.org/pdf/2408.07503v1,,False
Large Language Models Prompting With Episodic Memory,14/08/2024,"Dai Do, Quan Tran, Svetha Venkatesh, Hung Le","Prompt optimization is essential for enhancing the performance of Large
Language Models (LLMs) in a range of Natural Language Processing (NLP) tasks,
particularly in scenarios of few-shot learning where training examples are
incorporated directly into the prompt. Despite the growing interest in
optimizing prompts with few-shot examples, existing methods for prompt
optimization are often resource-intensive or perform inadequately. In this
work, we propose PrOmpting with Episodic Memory (POEM), a novel prompt
optimization technique that is simple, efficient, and demonstrates strong
generalization capabilities. We approach prompt optimization as a Reinforcement
Learning (RL) challenge, using episodic memory to archive combinations of input
data, permutations of few-shot examples, and the rewards observed during
training. In the testing phase, we optimize the sequence of examples for each
test query by selecting the sequence that yields the highest total rewards from
the top-k most similar training examples in the episodic memory. Our results
show that POEM outperforms recent techniques like TEMPERA and RLPrompt by over
5.3% in various text classification tasks. Furthermore, our approach adapts
well to broader language understanding tasks, consistently outperforming
conventional heuristic methods for ordering examples.",http://arxiv.org/pdf/2408.07465v1,,False
"Real-world validation of safe reinforcement learning, model predictive control and decision tree-based home energy management systems",14/08/2024,"Julian Ruddick, Glenn Ceusters, Gilles Van Kriekinge, Evgenii Genov, Thierry Coosemans, Maarten Messagie","Recent advancements in machine learning based energy management approaches,
specifically reinforcement learning with a safety layer (OptLayerPolicy) and a
metaheuristic algorithm generating a decision tree control policy (TreeC), have
shown promise. However, their effectiveness has only been demonstrated in
computer simulations. This paper presents the real-world validation of these
methods, comparing against model predictive control and simple rule-based
control benchmark. The experiments were conducted on the electrical
installation of 4 reproductions of residential houses, which all have their own
battery, photovoltaic and dynamic load system emulating a non-controllable
electrical load and a controllable electric vehicle charger. The results show
that the simple rules, TreeC, and model predictive control-based methods
achieved similar costs, with a difference of only 0.6%. The reinforcement
learning based method, still in its training phase, obtained a cost 25.5\%
higher to the other methods. Additional simulations show that the costs can be
further reduced by using a more representative training dataset for TreeC and
addressing errors in the model predictive control implementation caused by its
reliance on accurate data from various sources. The OptLayerPolicy safety layer
allows safe online training of a reinforcement learning agent in the
real-world, given an accurate constraint function formulation. The proposed
safety layer method remains error-prone, nonetheless, it is found beneficial
for all investigated methods. The TreeC method, which does require building a
realistic simulation for training, exhibits the safest operational performance,
exceeding the grid limit by only 27.1 Wh compared to 593.9 Wh for reinforcement
learning.",http://arxiv.org/pdf/2408.07435v1,,False
An Adaptive Importance Sampling for Locally Stable Point Processes,14/08/2024,"Hee-Geon Kang, Sunggon Kim","The problem of finding the expected value of a statistic of a locally stable
point process in a bounded region is addressed. We propose an adaptive
importance sampling for solving the problem. In our proposal, we restrict the
importance point process to the family of homogeneous Poisson point processes,
which enables us to generate quickly independent samples of the importance
point process. The optimal intensity of the importance point process is found
by applying the cross-entropy minimization method. In the proposed scheme, the
expected value of the function and the optimal intensity are iteratively
estimated in an adaptive manner. We show that the proposed estimator converges
to the target value almost surely, and prove the asymptotic normality of it. We
explain how to apply the proposed scheme to the estimation of the intensity of
a stationary pairwise interaction point process. The performance of the
proposed scheme is compared numerically with the Markov chain Monte Carlo
simulation and the perfect sampling.",http://arxiv.org/pdf/2408.07372v1,,False
LiPCoT: Linear Predictive Coding based Tokenizer for Self-supervised Learning of Time Series Data via Language Models,14/08/2024,Md Fahim Anjum,"Language models have achieved remarkable success in various natural language
processing tasks. However, their application to time series data, a crucial
component in many domains, remains limited. This paper proposes LiPCoT (Linear
Predictive Coding based Tokenizer for time series), a novel tokenizer that
encodes time series data into a sequence of tokens, enabling self-supervised
learning of time series using existing Language model architectures such as
BERT. Unlike traditional time series tokenizers that rely heavily on CNN
encoder for time series feature generation, LiPCoT employs stochastic modeling
through linear predictive coding to create a latent space for time series
providing a compact yet rich representation of the inherent stochastic nature
of the data. Furthermore, LiPCoT is computationally efficient and can
effectively handle time series data with varying sampling rates and lengths,
overcoming common limitations of existing time series tokenizers. In this
proof-of-concept work, we present the effectiveness of LiPCoT in classifying
Parkinson's disease (PD) using an EEG dataset from 46 participants. In
particular, we utilize LiPCoT to encode EEG data into a small vocabulary of
tokens and then use BERT for self-supervised learning and the downstream task
of PD classification. We benchmark our approach against several
state-of-the-art CNN-based deep learning architectures for PD detection. Our
results reveal that BERT models utilizing self-supervised learning outperformed
the best-performing existing method by 7.1% in precision, 2.3% in recall, 5.5%
in accuracy, 4% in AUC, and 5% in F1-score highlighting the potential for
self-supervised learning even on small datasets. Our work will inform future
foundational models for time series, particularly for self-supervised learning.",http://arxiv.org/pdf/2408.07292v1,,False
