Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
Geometry-Aware Generative Autoencoders for Warped Riemannian Metric Learning and Generative Modeling on Data Manifolds,16/10/2024,"Xingzhi Sun, Danqi Liao, Kincaid MacDonald, Yanlei Zhang, Chen Liu, Guillaume Huguet, Guy Wolf, Ian Adelstein, Tim G. J. Rudner, Smita Krishnaswamy","Rapid growth of high-dimensional datasets in fields such as single-cell RNA
sequencing and spatial genomics has led to unprecedented opportunities for
scientific discovery, but it also presents unique computational and statistical
challenges. Traditional methods struggle with geometry-aware data generation,
interpolation along meaningful trajectories, and transporting populations via
feasible paths. To address these issues, we introduce Geometry-Aware Generative
Autoencoder (GAGA), a novel framework that combines extensible manifold
learning with generative modeling. GAGA constructs a neural network embedding
space that respects the intrinsic geometries discovered by manifold learning
and learns a novel warped Riemannian metric on the data space. This warped
metric is derived from both the points on the data manifold and negative
samples off the manifold, allowing it to characterize a meaningful geometry
across the entire latent space. Using this metric, GAGA can uniformly sample
points on the manifold, generate points along geodesics, and interpolate
between populations across the learned manifold. GAGA shows competitive
performance in simulated and real world datasets, including a 30% improvement
over the state-of-the-art methods in single-cell population-level trajectory
inference.",http://arxiv.org/pdf/2410.12779v1,,False
Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions,16/10/2024,"Zhenyu Jiang, Yuqi Xie, Jinhan Li, Ye Yuan, Yifeng Zhu, Yuke Zhu","Humanoid robots, with their human-like embodiment, have the potential to
integrate seamlessly into human environments. Critical to their coexistence and
cooperation with humans is the ability to understand natural language
communications and exhibit human-like behaviors. This work focuses on
generating diverse whole-body motions for humanoid robots from language
descriptions. We leverage human motion priors from extensive human motion
datasets to initialize humanoid motions and employ the commonsense reasoning
capabilities of Vision Language Models (VLMs) to edit and refine these motions.
Our approach demonstrates the capability to produce natural, expressive, and
text-aligned humanoid motions, validated through both simulated and real-world
experiments. More videos can be found at
https://ut-austin-rpl.github.io/Harmon/.",http://arxiv.org/pdf/2410.12773v1,,False
Neural-based Control for CubeSat Docking Maneuvers,16/10/2024,"Matteo Stoisa, Federica Paganelli Azza, Luca Romanelli, Mattia Varile","Autonomous Rendezvous and Docking (RVD) have been extensively studied in
recent years, addressing the stringent requirements of spacecraft dynamics
variations and the limitations of GNC systems. This paper presents an
innovative approach employing Artificial Neural Networks (ANN) trained through
Reinforcement Learning (RL) for autonomous spacecraft guidance and control
during the final phase of the rendezvous maneuver. The proposed strategy is
easily implementable onboard and offers fast adaptability and robustness to
disturbances by learning control policies from experience rather than relying
on predefined models. Extensive Monte Carlo simulations within a relevant
environment are conducted in 6DoF settings to validate our approach, along with
hardware tests that demonstrate deployment feasibility. Our findings highlight
the efficacy of RL in assuring the adaptability and efficiency of spacecraft
RVD, offering insights into future mission expectations.",http://arxiv.org/pdf/2410.12703v1,,False
Generative Neural Reparameterization for Differentiable PDE-constrained Optimization,16/10/2024,Archis S. Joglekar,"Partial-differential-equation (PDE)-constrained optimization is a well-worn
technique for acquiring optimal parameters of systems governed by PDEs.
However, this approach is limited to providing a single set of optimal
parameters per optimization. Given a differentiable PDE solver, if the free
parameters are reparameterized as the output of a neural network, that neural
network can be trained to learn a map from a probability distribution to the
distribution of optimal parameters. This proves useful in the case where there
are many well performing local minima for the PDE. We apply this technique to
train a neural network that generates optimal parameters that minimize
laser-plasma instabilities relevant to laser fusion and show that the neural
network generates many well performing and diverse minima.",http://arxiv.org/pdf/2410.12683v1,,False
Position Specific Scoring Is All You Need? Revisiting Protein Sequence Classification Tasks,16/10/2024,"Sarwan Ali, Taslim Murad, Prakash Chourasia, Haris Mansoor, Imdad Ullah Khan, Pin-Yu Chen, Murray Patterson","Understanding the structural and functional characteristics of proteins are
crucial for developing preventative and curative strategies that impact fields
from drug discovery to policy development. An important and popular technique
for examining how amino acids make up these characteristics of the protein
sequences with position-specific scoring (PSS). While the string kernel is
crucial in natural language processing (NLP), it is unclear if string kernels
can extract biologically meaningful information from protein sequences, despite
the fact that they have been shown to be effective in the general sequence
analysis tasks. In this work, we propose a weighted PSS kernel matrix (or
W-PSSKM), that combines a PSS representation of protein sequences, which
encodes the frequency information of each amino acid in a sequence, with the
notion of the string kernel. This results in a novel kernel function that
outperforms many other approaches for protein sequence classification. We
perform extensive experimentation to evaluate the proposed method. Our findings
demonstrate that the W-PSSKM significantly outperforms existing baselines and
state-of-the-art methods and achieves up to 45.1\% improvement in
classification accuracy.",http://arxiv.org/pdf/2410.12655v1,,False
Constrained Posterior Sampling: Time Series Generation with Hard Constraints,16/10/2024,"Sai Shankar Narasimhan, Shubhankar Agarwal, Litu Rout, Sanjay Shakkottai, Sandeep P. Chinchali","Generating realistic time series samples is crucial for stress-testing models
and protecting user privacy by using synthetic data. In engineering and
safety-critical applications, these samples must meet certain hard constraints
that are domain-specific or naturally imposed by physics or nature. Consider,
for example, generating electricity demand patterns with constraints on peak
demand times. This can be used to stress-test the functioning of power grids
during adverse weather conditions. Existing approaches for generating
constrained time series are either not scalable or degrade sample quality. To
address these challenges, we introduce Constrained Posterior Sampling (CPS), a
diffusion-based sampling algorithm that aims to project the posterior mean
estimate into the constraint set after each denoising update. Notably, CPS
scales to a large number of constraints (~100) without requiring additional
training. We provide theoretical justifications highlighting the impact of our
projection step on sampling. Empirically, CPS outperforms state-of-the-art
methods in sample quality and similarity to real time series by around 10% and
42%, respectively, on real-world stocks, traffic, and air quality datasets.",http://arxiv.org/pdf/2410.12652v1,,False
MING: A Functional Approach to Learning Molecular Generative Models,16/10/2024,"Van Khoa Nguyen, Maciej Falkiewicz, Giangiacomo Mercatali, Alexandros Kalousis","Traditional molecule generation methods often rely on sequence or graph-based
representations, which can limit their expressive power or require complex
permutation-equivariant architectures. This paper introduces a novel paradigm
for learning molecule generative models based on functional representations.
Specifically, we propose Molecular Implicit Neural Generation (MING), a
diffusion-based model that learns molecular distributions in function space.
Unlike standard diffusion processes in data space, MING employs a novel
functional denoising probabilistic process, which jointly denoises the
information in both the function's input and output spaces by leveraging an
expectation-maximization procedure for latent implicit neural representations
of data. This approach allows for a simple yet effective model design that
accurately captures underlying function distributions. Experimental results on
molecule-related datasets demonstrate MING's superior performance and ability
to generate plausible molecular samples, surpassing state-of-the-art data-space
methods while offering a more streamlined architecture and significantly faster
generation times.",http://arxiv.org/pdf/2410.12522v1,,False
HELM: Hierarchical Encoding for mRNA Language Modeling,16/10/2024,"Mehdi Yazdani-Jahromi, Mangal Prakash, Tommaso Mansi, Artem Moskalev, Rui Liao","Messenger RNA (mRNA) plays a crucial role in protein synthesis, with its
codon structure directly impacting biological properties. While Language Models
(LMs) have shown promise in analyzing biological sequences, existing approaches
fail to account for the hierarchical nature of mRNA's codon structure. We
introduce Hierarchical Encoding for mRNA Language Modeling (HELM), a novel
pre-training strategy that incorporates codon-level hierarchical structure into
language model training. HELM modulates the loss function based on codon
synonymity, aligning the model's learning process with the biological reality
of mRNA sequences. We evaluate HELM on diverse mRNA datasets and tasks,
demonstrating that HELM outperforms standard language model pre-training as
well as existing foundation model baselines on six diverse downstream property
prediction tasks and an antibody region annotation tasks on average by around
8\%. Additionally, HELM enhances the generative capabilities of language model,
producing diverse mRNA sequences that better align with the underlying true
data distribution compared to non-hierarchical baselines.",http://arxiv.org/pdf/2410.12459v1,,False
Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance,16/10/2024,"Yaxi Lu, Shenzhi Yang, Cheng Qian, Guirong Chen, Qinyu Luo, Yesai Wu, Huadong Wang, Xin Cong, Zhong Zhang, Yankai Lin, Weiwen Liu, Yasheng Wang, Zhiyuan Liu, Fangming Liu, Maosong Sun","Agents powered by large language models have shown remarkable abilities in
solving complex tasks. However, most agent systems remain reactive, limiting
their effectiveness in scenarios requiring foresight and autonomous
decision-making. In this paper, we tackle the challenge of developing proactive
agents capable of anticipating and initiating tasks without explicit human
instructions. We propose a novel data-driven approach for this problem.
Firstly, we collect real-world human activities to generate proactive task
predictions. These predictions are then labeled by human annotators as either
accepted or rejected. The labeled data is used to train a reward model that
simulates human judgment and serves as an automatic evaluator of the
proactiveness of LLM agents. Building on this, we develop a comprehensive data
generation pipeline to create a diverse dataset, ProactiveBench, containing
6,790 events. Finally, we demonstrate that fine-tuning models with the proposed
ProactiveBench can significantly elicit the proactiveness of LLM agents.
Experimental results show that our fine-tuned model achieves an F1-Score of
66.47% in proactively offering assistance, outperforming all open-source and
close-source models. These results highlight the potential of our method in
creating more proactive and effective agent systems, paving the way for future
advancements in human-agent collaboration.",http://arxiv.org/pdf/2410.12361v1,,False
A linguistic analysis of undesirable outcomes in the era of generative AI,16/10/2024,"Daniele Gambetta, Gizem Gezici, Fosca Giannotti, Dino Pedreschi, Alistair Knott, Luca Pappalardo","Recent research has focused on the medium and long-term impacts of generative
AI, posing scientific and societal challenges mainly due to the detection and
reliability of machine-generated information, which is projected to form the
major content on the Web soon. Prior studies show that LLMs exhibit a lower
performance in generation tasks (model collapse) as they undergo a fine-tuning
process across multiple generations on their own generated content
(self-consuming loop). In this paper, we present a comprehensive simulation
framework built upon the chat version of LLama2, focusing particularly on the
linguistic aspects of the generated content, which has not been fully examined
in existing studies. Our results show that the model produces less lexical rich
content across generations, reducing diversity. The lexical richness has been
measured using the linguistic measures of entropy and TTR as well as
calculating the POSTags frequency. The generated content has also been examined
with an $n$-gram analysis, which takes into account the word order, and
semantic networks, which consider the relation between different words. These
findings suggest that the model collapse occurs not only by decreasing the
content diversity but also by distorting the underlying linguistic patterns of
the generated text, which both highlight the critical importance of carefully
choosing and curating the initial input text, which can alleviate the model
collapse problem. Furthermore, we conduct a qualitative analysis of the
fine-tuned models of the pipeline to compare their performances on generic NLP
tasks to the original model. We find that autophagy transforms the initial
model into a more creative, doubtful and confused one, which might provide
inaccurate answers and include conspiracy theories in the model responses,
spreading false and biased information on the Web.",http://arxiv.org/pdf/2410.12341v1,,False
Reversal of Thought: Enhancing Large Language Models with Preference-Guided Reverse Reasoning Warm-up,16/10/2024,"Jiahao Yuan, Dehui Du, Hao Zhang, Zixiang Di, Usman Naseem","Large language models (LLMs) have shown remarkable performance in reasoning
tasks but face limitations in mathematical and complex logical reasoning.
Existing methods to improve LLMs' logical capabilities either involve traceable
or verifiable logical sequences that generate more reliable responses by
constructing logical structures yet increase computational costs, or introduces
rigid logic template rules, reducing flexibility. In this paper, we propose
Reversal of Thought (RoT), a novel framework aimed at enhancing the logical
reasoning abilities of LLMs. RoT utilizes a Preference-Guided Reverse Reasoning
warm-up strategy, which integrates logical symbols for pseudocode planning
through meta-cognitive mechanisms and pairwise preference self-evaluation to
generate task-specific prompts solely through demonstrations, aligning with
LLMs' cognitive preferences shaped by Reinforcement Learning with Human
Feedback (RLHF). Through reverse reasoning, we ultilize a Cognitive Preference
Manager to assess knowledge boundaries and further expand LLMs' reasoning
capabilities by aggregating solution logic for known tasks and stylistic
templates for unknown tasks. Experiments across various tasks demonstrate that
RoT surpasses existing baselines in both reasoning accuracy and efficiency.",http://arxiv.org/pdf/2410.12323v1,,False
Towards LLM-based Cognitive Models of Students with Misconceptions,16/10/2024,"Shashank Sonkar, Xinghe Chen, Naiming Liu, Richard G. Baraniuk, Mrinmaya Sachan","Accurately modeling student cognition is crucial for developing effective
AI-driven educational technologies. A key challenge is creating realistic
student models that satisfy two essential properties: (1) accurately
replicating specific misconceptions, and (2) correctly solving problems where
these misconceptions are not applicable. This dual requirement reflects the
complex nature of student understanding, where misconceptions coexist with
correct knowledge. This paper investigates whether Large Language Models (LLMs)
can be instruction-tuned to meet this dual requirement and effectively simulate
student thinking in algebra. We introduce MalAlgoPy, a novel Python library
that generates datasets reflecting authentic student solution patterns through
a graph-based representation of algebraic problem-solving. Utilizing MalAlgoPy,
we define and examine Cognitive Student Models (CSMs) - LLMs instruction tuned
to faithfully emulate realistic student behavior. Our findings reveal that LLMs
trained on misconception examples can efficiently learn to replicate errors.
However, the training diminishes the model's ability to solve problems
correctly, particularly for problem types where the misconceptions are not
applicable, thus failing to satisfy second property of CSMs. We demonstrate
that by carefully calibrating the ratio of correct to misconception examples in
the training data - sometimes as low as 0.25 - it is possible to develop CSMs
that satisfy both properties. Our insights enhance our understanding of
AI-based student models and pave the way for effective adaptive learning
systems.",http://arxiv.org/pdf/2410.12294v1,,False
CATCH: Channel-Aware multivariate Time Series Anomaly Detection via Frequency Patching,16/10/2024,"Xingjian Wu, Xiangfei Qiu, Zhengyu Li, Yihang Wang, Jilin Hu, Chenjuan Guo, Hui Xiong, Bin Yang","Anomaly detection in multivariate time series is challenging as heterogeneous
subsequence anomalies may occur. Reconstruction-based methods, which focus on
learning nomral patterns in the frequency domain to detect diverse abnormal
subsequences, achieve promising resutls, while still falling short on capturing
fine-grained frequency characteristics and channel correlations. To contend
with the limitations, we introduce CATCH, a framework based on frequency
patching. We propose to patchify the frequency domain into frequency bands,
which enhances its ability to capture fine-grained frequency characteristics.
To perceive appropriate channel correlations, we propose a Channel Fusion
Module (CFM), which features a patch-wise mask generator and a masked-attention
mechanism. Driven by a bi-level multi-objective optimization algorithm, the CFM
is encouraged to iteratively discover appropriate patch-wise channel
correlations, and to cluster relevant channels while isolating adverse effects
from irrelevant channels. Extensive experiments on 9 real-world datasets and 12
synthetic datasets demonstrate that CATCH achieves state-of-the-art
performance.",http://arxiv.org/pdf/2410.12261v1,,False
Irregularity-Informed Time Series Analysis: Adaptive Modelling of Spatial and Temporal Dynamics,16/10/2024,"Liangwei Nathan Zheng, Zhengyang Li, Chang George Dong, Wei Emma Zhang, Lin Yue, Miao Xu, Olaf Maennel, Weitong Chen","Irregular Time Series Data (IRTS) has shown increasing prevalence in
real-world applications. We observed that IRTS can be divided into two
specialized types: Natural Irregular Time Series (NIRTS) and Accidental
Irregular Time Series (AIRTS). Various existing methods either ignore the
impacts of irregular patterns or statically learn the irregular dynamics of
NIRTS and AIRTS data and suffer from limited data availability due to the
sparsity of IRTS. We proposed a novel transformer-based framework for general
irregular time series data that treats IRTS from four views: Locality, Time,
Spatio and Irregularity to motivate the data usage to the highest potential.
Moreover, we design a sophisticated irregularity-gate mechanism to adaptively
select task-relevant information from irregularity, which improves the
generalization ability to various IRTS data. We implement extensive experiments
to demonstrate the resistance of our work to three highly missing ratio
datasets (88.4\%, 94.9\%, 60\% missing value) and investigate the significance
of the irregularity information for both NIRTS and AIRTS by additional ablation
study. We release our implementation in
https://github.com/IcurasLW/MTSFormer-Irregular_Time_Series.git",http://arxiv.org/pdf/2410.12257v1,,False
Improving the Generalization of Unseen Crowd Behaviors for Reinforcement Learning based Local Motion Planners,16/10/2024,"Wen Zheng Terence Ng, Jianda Chen, Sinno Jialin Pan, Tianwei Zhang","Deploying a safe mobile robot policy in scenarios with human pedestrians is
challenging due to their unpredictable movements. Current Reinforcement
Learning-based motion planners rely on a single policy to simulate pedestrian
movements and could suffer from the over-fitting issue. Alternatively, framing
the collision avoidance problem as a multi-agent framework, where agents
generate dynamic movements while learning to reach their goals, can lead to
conflicts with human pedestrians due to their homogeneity.
  To tackle this problem, we introduce an efficient method that enhances agent
diversity within a single policy by maximizing an information-theoretic
objective. This diversity enriches each agent's experiences, improving its
adaptability to unseen crowd behaviors. In assessing an agent's robustness
against unseen crowds, we propose diverse scenarios inspired by pedestrian
crowd behaviors. Our behavior-conditioned policies outperform existing works in
these challenging scenes, reducing potential collisions without additional time
or travel.",http://arxiv.org/pdf/2410.12232v1,10.1109/ICRA57147.2024.10610641,False
Global Censored Quantile Random Forest,16/10/2024,"Siyu Zhou, Limin Peng","In recent years, censored quantile regression has enjoyed an increasing
popularity for survival analysis while many existing works rely on linearity
assumptions. In this work, we propose a Global Censored Quantile Random Forest
(GCQRF) for predicting a conditional quantile process on data subject to right
censoring, a forest-based flexible, competitive method able to capture complex
nonlinear relationships. Taking into account the randomness in trees and
connecting the proposed method to a randomized incomplete infinite degree
U-process (IDUP), we quantify the prediction process' variation without
assuming an infinite forest and establish its weak convergence. Moreover,
feature importance ranking measures based on out-of-sample predictive accuracy
are proposed. We demonstrate the superior predictive accuracy of the proposed
method over a number of existing alternatives and illustrate the use of the
proposed importance ranking measures on both simulated and real data.",http://arxiv.org/pdf/2410.12209v1,,False
Abnormality Forecasting: Time Series Anomaly Prediction via Future Context Modeling,16/10/2024,"Sinong Zhao, Wenrui Wang, Hongzuo Xu, Zhaoyang Yu, Qingsong Wen, Gang Wang, xiaoguang Liu, Guansong Pang","Identifying anomalies from time series data plays an important role in
various fields such as infrastructure security, intelligent operation and
maintenance, and space exploration. Current research focuses on detecting the
anomalies after they occur, which can lead to significant financial/reputation
loss or infrastructure damage. In this work we instead study a more practical
yet very challenging problem, time series anomaly prediction, aiming at
providing early warnings for abnormal events before their occurrence. To tackle
this problem, we introduce a novel principled approach, namely future context
modeling (FCM). Its key insight is that the future abnormal events in a target
window can be accurately predicted if their preceding observation window
exhibits any subtle difference to normal data. To effectively capture such
differences, FCM first leverages long-term forecasting models to generate a
discriminative future context based on the observation data, aiming to amplify
those subtle but unusual difference. It then models a normality correlation of
the observation data with the forecasting future context to complement the
normality modeling of the observation data in foreseeing possible abnormality
in the target window. A joint variate-time attention learning is also
introduced in FCM to leverage both temporal signals and features of the time
series data for more discriminative normality modeling in the aforementioned
two views. Comprehensive experiments on five datasets demonstrate that FCM
gains good recall rate (70\%+) on multiple datasets and significantly
outperforms all baselines in F1 score. Code is available at
https://github.com/mala-lab/FCM.",http://arxiv.org/pdf/2410.12206v1,,False
Reinforcement Learning with LTL and $ω$-Regular Objectives via Optimality-Preserving Translation to Average Rewards,16/10/2024,"Xuan-Bach Le, Dominik Wagner, Leon Witzman, Alexander Rabinovich, Luke Ong","Linear temporal logic (LTL) and, more generally, $\omega$-regular objectives
are alternatives to the traditional discount sum and average reward objectives
in reinforcement learning (RL), offering the advantage of greater
comprehensibility and hence explainability. In this work, we study the
relationship between these objectives. Our main result is that each RL problem
for $\omega$-regular objectives can be reduced to a limit-average reward
problem in an optimality-preserving fashion, via (finite-memory) reward
machines. Furthermore, we demonstrate the efficacy of this approach by showing
that optimal policies for limit-average problems can be found asymptotically by
solving a sequence of discount-sum problems approximately. Consequently, we
resolve an open problem: optimal policies for LTL and $\omega$-regular
objectives can be learned asymptotically.",http://arxiv.org/pdf/2410.12175v1,,False
