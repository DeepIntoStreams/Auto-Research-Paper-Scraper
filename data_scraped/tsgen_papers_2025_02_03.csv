Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
Decoding-based Regression,31/01/2025,"Xingyou Song, Dara Bahri","Language models have recently been shown capable of performing regression
tasks wherein numeric predictions are represented as decoded strings. In this
work, we provide theoretical grounds for this capability and furthermore
investigate the utility of causal auto-regressive sequence models when they are
applied to any feature representation. We find that, despite being trained in
the usual way - for next-token prediction via cross-entropy loss -
decoding-based regression is as performant as traditional approaches for
tabular regression tasks, while being flexible enough to capture arbitrary
distributions, such as in the task of density estimation.",http://arxiv.org/pdf/2501.19383v1,,False
CoSTI: Consistency Models for (a faster) Spatio-Temporal Imputation,31/01/2025,"Javier Solís-García, Belén Vega-Márquez, Juan A. Nepomuceno, Isabel A. Nepomuceno-Chamorro","Multivariate Time Series Imputation (MTSI) is crucial for many applications,
such as healthcare monitoring and traffic management, where incomplete data can
compromise decision-making. Existing state-of-the-art methods, like Denoising
Diffusion Probabilistic Models (DDPMs), achieve high imputation accuracy;
however, they suffer from significant computational costs and are notably
time-consuming due to their iterative nature. In this work, we propose CoSTI,
an innovative adaptation of Consistency Models (CMs) for the MTSI domain. CoSTI
employs Consistency Training to achieve comparable imputation quality to DDPMs
while drastically reducing inference times, making it more suitable for
real-time applications. We evaluate CoSTI across multiple datasets and missing
data scenarios, demonstrating up to a 98% reduction in imputation time with
performance on par with diffusion-based models. This work bridges the gap
between efficiency and accuracy in generative imputation tasks, providing a
scalable solution for handling missing data in critical spatio-temporal
systems.",http://arxiv.org/pdf/2501.19364v1,,False
Pathological MRI Segmentation by Synthetic Pathological Data Generation in Fetuses and Neonates,31/01/2025,"Misha P. T Kaandorp, Damola Agbelese, Hosna Asma-ull, Hyun-Gi Kim, Kelly Payette, Patrice Grehten, Gennari Antonio Giulio, Levente István Lánczi, Andras Jakab","Developing new methods for the automated analysis of clinical fetal and
neonatal MRI data is limited by the scarcity of annotated pathological datasets
and privacy concerns that often restrict data sharing, hindering the
effectiveness of deep learning models. We address this in two ways. First, we
introduce Fetal&Neonatal-DDPM, a novel diffusion model framework designed to
generate high-quality synthetic pathological fetal and neonatal MRIs from
semantic label images. Second, we enhance training data by modifying healthy
label images through morphological alterations to simulate conditions such as
ventriculomegaly, cerebellar and pontocerebellar hypoplasia, and microcephaly.
By leveraging Fetal&Neonatal-DDPM, we synthesize realistic pathological MRIs
from these modified pathological label images. Radiologists rated the synthetic
MRIs as significantly (p < 0.05) superior in quality and diagnostic value
compared to real MRIs, demonstrating features such as blood vessels and choroid
plexus, and improved alignment with label annotations. Synthetic pathological
data enhanced state-of-the-art nnUNet segmentation performance, particularly
for severe ventriculomegaly cases, with the greatest improvements achieved in
ventricle segmentation (Dice scores: 0.9253 vs. 0.7317). This study underscores
the potential of generative AI as transformative tool for data augmentation,
offering improved segmentation performance in pathological cases. This
development represents a significant step towards improving analysis and
segmentation accuracy in prenatal imaging, and also offers new ways for data
anonymization through the generation of pathologic image data.",http://arxiv.org/pdf/2501.19338v1,,False
Capturing Temporal Dynamics in Large-Scale Canopy Tree Height Estimation,31/01/2025,"Jan Pauls, Max Zimmer, Berkant Turan, Sassan Saatchi, Philippe Ciais, Sebastian Pokutta, Fabian Gieseke","With the rise in global greenhouse gas emissions, accurate large-scale tree
canopy height maps are essential for understanding forest structure, estimating
above-ground biomass, and monitoring ecological disruptions. To this end, we
present a novel approach to generate large-scale, high-resolution canopy height
maps over time. Our model accurately predicts canopy height over multiple years
given Sentinel-2 time series satellite data. Using GEDI LiDAR data as the
ground truth for training the model, we present the first 10m resolution
temporal canopy height map of the European continent for the period 2019-2022.
As part of this product, we also offer a detailed canopy height map for 2020,
providing more precise estimates than previous studies. Our pipeline and the
resulting temporal height map are publicly available, enabling comprehensive
large-scale monitoring of forests and, hence, facilitating future research and
ecological analyses. For an interactive viewer, see
https://europetreemap.projects.earthengine.app/view/temporalcanopyheight.",http://arxiv.org/pdf/2501.19328v1,,False
MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented Reinforcement in Embodied Systems,31/01/2025,"Anirudh Chari, Suraj Reddy, Aditya Tiwari, Richard Lian, Brian Zhou","While large language models (LLMs) have shown promising capabilities as
zero-shot planners for embodied agents, their inability to learn from
experience and build persistent mental models limits their robustness in
complex open-world environments like Minecraft. We introduce MINDSTORES, an
experience-augmented planning framework that enables embodied agents to build
and leverage mental models through natural interaction with their environment.
Drawing inspiration from how humans construct and refine cognitive mental
models, our approach extends existing zero-shot LLM planning by maintaining a
database of past experiences that informs future planning iterations. The key
innovation is representing accumulated experiences as natural language
embeddings of (state, task, plan, outcome) tuples, which can then be
efficiently retrieved and reasoned over by an LLM planner to generate insights
and guide plan refinement for novel states and tasks. Through extensive
experiments in the MineDojo environment, a simulation environment for agents in
Minecraft that provides low-level controls for Minecraft, we find that
MINDSTORES learns and applies its knowledge significantly better than existing
memory-based LLM planners while maintaining the flexibility and generalization
benefits of zero-shot approaches, representing an important step toward more
capable embodied AI systems that can learn continuously through natural
experience.",http://arxiv.org/pdf/2501.19318v1,,False
Synthetic User Behavior Sequence Generation with Large Language Models for Smart Homes,31/01/2025,"Zhiyao Xu, Dan Zhao, Qingsong Zou, Jingyu Xiao, Yong Jiang, Zhenhui Yuan, Qing Li","In recent years, as smart home systems have become more widespread, security
concerns within these environments have become a growing threat. Currently,
most smart home security solutions, such as anomaly detection and behavior
prediction models, are trained using fixed datasets that are precollected.
However, the process of dataset collection is time-consuming and lacks the
flexibility needed to adapt to the constantly evolving smart home environment.
Additionally, the collection of personal data raises significant privacy
concerns for users. Lately, large language models (LLMs) have emerged as a
powerful tool for a wide range of tasks across diverse application domains,
thanks to their strong capabilities in natural language processing, reasoning,
and problem-solving. In this paper, we propose an LLM-based synthetic dataset
generation IoTGen framework to enhance the generalization of downstream smart
home intelligent models. By generating new synthetic datasets that reflect
changes in the environment, smart home intelligent models can be retrained to
overcome the limitations of fixed and outdated data, allowing them to better
align with the dynamic nature of real-world home environments. Specifically, we
first propose a Structure Pattern Perception Compression (SPPC) method tailored
for IoT behavior data, which preserves the most informative content in the data
while significantly reducing token consumption. Then, we propose a systematic
approach to create prompts and implement data generation to automatically
generate IoT synthetic data with normative and reasonable properties, assisting
task models in adaptive training to improve generalization and real-world
performance.",http://arxiv.org/pdf/2501.19298v1,,False
A Zero-Shot Generalization Framework for LLM-Driven Cross-Domain Sequential Recommendation,31/01/2025,"Yunzhe Li, Junting Wang, Hari Sundaram, Zhining Liu","Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions
in unseen domains without the need for additional training or fine-tuning,
making it particularly valuable in data-sparse environments where traditional
models struggle. Recent advancements in large language models (LLMs) have
greatly improved ZCDSR by leveraging rich pretrained representations to
facilitate cross-domain knowledge transfer. However, a key challenge persists:
domain semantic bias, which arises from variations in vocabulary and content
focus across domains. This misalignment leads to inconsistencies in item
embeddings and hinders generalization.
  To address this issue, we propose a novel framework designed to enhance
LLM-based ZCDSR by improving cross-domain alignment at both the item and
sequential levels. At the item level, we introduce a generalization loss that
promotes inter-domain compactness by aligning embeddings of similar items
across domains while maintaining intra-domain diversity to preserve unique item
characteristics. This prevents embeddings from becoming overly generic while
ensuring effective transferability. At the sequential level, we develop a
method for transferring user behavioral patterns by clustering user sequences
in the source domain and applying attention-based aggregation for target domain
inference. This dynamic adaptation of user embeddings allows effective
zero-shot recommendations without requiring target-domain interactions.
  Comprehensive experiments across multiple datasets and domains demonstrate
that our framework significantly improves sequential recommendation performance
in the ZCDSR setting. By mitigating domain bias and enhancing the
transferability of sequential patterns, our method provides a scalable and
robust approach for achieving more effective zero-shot recommendations across
domains.",http://arxiv.org/pdf/2501.19232v1,,False
Efficient Reasoning with Hidden Thinking,31/01/2025,"Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, Jiuxiang Gu","Chain-of-Thought (CoT) reasoning has become a powerful framework for
improving complex problem-solving capabilities in Multimodal Large Language
Models (MLLMs). However, the verbose nature of textual reasoning introduces
significant inefficiencies. In this work, we propose $\textbf{Heima}$ (as
hidden llama), an efficient reasoning framework that leverages reasoning CoTs
at hidden latent space. We design the Heima Encoder to condense each
intermediate CoT into a compact, higher-level hidden representation using a
single thinking token, effectively minimizing verbosity and reducing the
overall number of tokens required during the reasoning process. Meanwhile, we
design corresponding Heima Decoder with traditional Large Language Models
(LLMs) to adaptively interpret the hidden representations into variable-length
textual sequence, reconstructing reasoning processes that closely resemble the
original CoTs. Experimental results across diverse reasoning MLLM benchmarks
demonstrate that Heima model achieves higher generation efficiency while
maintaining or even better zero-shot task accuracy. Moreover, the effective
reconstruction of multimodal reasoning processes with Heima Decoder validates
both the robustness and interpretability of our approach.",http://arxiv.org/pdf/2501.19201v1,,False
A Variational Perspective on Generative Protein Fitness Optimization,31/01/2025,"Lea Bogensperger, Dominik Narnhofer, Ahmed Allam, Konrad Schindler, Michael Krauthammer","The goal of protein fitness optimization is to discover new protein variants
with enhanced fitness for a given use. The vast search space and the sparsely
populated fitness landscape, along with the discrete nature of protein
sequences, pose significant challenges when trying to determine the gradient
towards configurations with higher fitness. We introduce Variational Latent
Generative Protein Optimization (VLGPO), a variational perspective on fitness
optimization. Our method embeds protein sequences in a continuous latent space
to enable efficient sampling from the fitness distribution and combines a
(learned) flow matching prior over sequence mutations with a fitness predictor
to guide optimization towards sequences with high fitness. VLGPO achieves
state-of-the-art results on two different protein benchmarks of varying
complexity. Moreover, the variational design with explicit prior and likelihood
functions offers a flexible plug-and-play framework that can be easily
customized to suit various protein design tasks.",http://arxiv.org/pdf/2501.19200v1,,False
Imitation Game for Adversarial Disillusion with Multimodal Generative Chain-of-Thought Role-Play,31/01/2025,"Ching-Chun Chang, Fan-Yun Chen, Shih-Hong Gu, Kai Gao, Hanrui Wang, Isao Echizen","As the cornerstone of artificial intelligence, machine perception confronts a
fundamental threat posed by adversarial illusions. These adversarial attacks
manifest in two primary forms: deductive illusion, where specific stimuli are
crafted based on the victim model's general decision logic, and inductive
illusion, where the victim model's general decision logic is shaped by specific
stimuli. The former exploits the model's decision boundaries to create a
stimulus that, when applied, interferes with its decision-making process. The
latter reinforces a conditioned reflex in the model, embedding a backdoor
during its learning phase that, when triggered by a stimulus, causes aberrant
behaviours. The multifaceted nature of adversarial illusions calls for a
unified defence framework, addressing vulnerabilities across various forms of
attack. In this study, we propose a disillusion paradigm based on the concept
of an imitation game. At the heart of the imitation game lies a multimodal
generative agent, steered by chain-of-thought reasoning, which observes,
internalises and reconstructs the semantic essence of a sample, liberated from
the classic pursuit of reversing the sample to its original state. As a proof
of concept, we conduct experimental simulations using a multimodal generative
dialogue agent and evaluates the methodology under a variety of attack
scenarios.",http://arxiv.org/pdf/2501.19143v1,,False
SpikingSoft: A Spiking Neuron Controller for Bio-inspired Locomotion with Soft Snake Robots,31/01/2025,"Chuhan Zhang, Cong Wang, Wei Pan, Cosimo Della Santina","Inspired by the dynamic coupling of moto-neurons and physical elasticity in
animals, this work explores the possibility of generating locomotion gaits by
utilizing physical oscillations in a soft snake by means of a low-level spiking
neural mechanism. To achieve this goal, we introduce the Double Threshold
Spiking neuron model with adjustable thresholds to generate varied output
patterns. This neuron model can excite the natural dynamics of soft robotic
snakes, and it enables distinct movements, such as turning or moving forward,
by simply altering the neural thresholds. Finally, we demonstrate that our
approach, termed SpikingSoft, naturally pairs and integrates with reinforcement
learning. The high-level agent only needs to adjust the two thresholds to
generate complex movement patterns, thus strongly simplifying the learning of
reactive locomotion. Simulation results demonstrate that the proposed
architecture significantly enhances the performance of the soft snake robot,
enabling it to achieve target objectives with a 21.6% increase in success rate,
a 29% reduction in time to reach the target, and smoother movements compared to
the vanilla reinforcement learning controllers or Central Pattern Generator
controller acting in torque space.",http://arxiv.org/pdf/2501.19072v1,,False
Text-to-CAD Generation Through Infusing Visual Feedback in Large Language Models,31/01/2025,"Ruiyu Wang, Yu Yuan, Shizhao Sun, Jiang Bian","Creating Computer-Aided Design (CAD) models requires significant expertise
and effort. Text-to-CAD, which converts textual descriptions into CAD
parametric sequences, is crucial in streamlining this process. Recent studies
have utilized ground-truth parametric sequences, known as sequential signals,
as supervision to achieve this goal. However, CAD models are inherently
multimodal, comprising parametric sequences and corresponding rendered visual
objects. Besides,the rendering process from parametric sequences to visual
objects is many-to-one. Therefore, both sequential and visual signals are
critical for effective training. In this work, we introduce CADFusion, a
framework that uses Large Language Models (LLMs) as the backbone and alternates
between two training stages: the sequential learning (SL) stage and the visual
feedback (VF) stage. In the SL stage, we train LLMs using ground-truth
parametric sequences, enabling the generation of logically coherent parametric
sequences. In the VF stage, we reward parametric sequences that render into
visually preferred objects and penalize those that do not, allowing LLMs to
learn how rendered visual objects are perceived and evaluated. These two stages
alternate throughout the training, ensuring balanced learning and preserving
benefits of both signals. Experiments demonstrate that CADFusion significantly
improves performance, both qualitatively and quantitatively.",http://arxiv.org/pdf/2501.19054v1,,False
Swarm-Gen: Fast Generation of Diverse Feasible Swarm Behaviors,31/01/2025,"Simon Idoko, B. Bhanu Teja, K. Madhava Krishna, Arun Kumar Singh","Coordination behavior in robot swarms is inherently multi-modal in nature.
That is, there are numerous ways in which a swarm of robots can avoid
inter-agent collisions and reach their respective goals. However, the problem
of generating diverse and feasible swarm behaviors in a scalable manner remains
largely unaddressed. In this paper, we fill this gap by combining generative
models with a safety-filter (SF). Specifically, we sample diverse trajectories
from a learned generative model which is subsequently projected onto the
feasible set using the SF. We experiment with two choices for generative
models, namely: Conditional Variational Autoencoder (CVAE) and Vector-Quantized
Variational Autoencoder (VQ-VAE). We highlight the trade-offs these two models
provide in terms of computation time and trajectory diversity. We develop a
custom solver for our SF and equip it with a neural network that predicts
context-specific initialization. Thecinitialization network is trained in a
self-supervised manner, taking advantage of the differentiability of the SF
solver. We provide two sets of empirical results. First, we demonstrate that we
can generate a large set of multi-modal, feasible trajectories, simulating
diverse swarm behaviors, within a few tens of milliseconds. Second, we show
that our initialization network provides faster convergence of our SF solver
vis-a-vis other alternative heuristics.",http://arxiv.org/pdf/2501.19042v1,,False
Towards the Worst-case Robustness of Large Language Models,31/01/2025,"Huanran Chen, Yinpeng Dong, Zeming Wei, Hang Su, Jun Zhu","Recent studies have revealed the vulnerability of Large Language Models
(LLMs) to adversarial attacks, where the adversary crafts specific input
sequences to induce harmful, violent, private, or incorrect outputs. Although
various defenses have been proposed, they have not been evaluated by strong
adaptive attacks, leaving the worst-case robustness of LLMs still intractable.
By developing a stronger white-box attack, our evaluation results indicate that
most typical defenses achieve nearly 0\% robustness.To solve this, we propose
\textit{DiffTextPure}, a general defense that diffuses the (adversarial) input
prompt using any pre-defined smoothing distribution, and purifies the diffused
input using a pre-trained language model. Theoretically, we derive tight
robustness lower bounds for all smoothing distributions using Fractal Knapsack
or 0-1 Knapsack solvers. Under this framework, we certify the robustness of a
specific case -- smoothing LLMs using a uniform kernel -- against \textit{any
possible attack} with an average $\ell_0$ perturbation of 2.02 or an average
suffix length of 6.41.",http://arxiv.org/pdf/2501.19040v1,,False
Scalable Multi-phase Word Embedding Using Conjunctive Propositional Clauses,31/01/2025,"Ahmed K. Kadhim, Lei Jiao, Rishad Shafik, Ole-Christoffer Granmo, Bimal Bhattarai","The Tsetlin Machine (TM) architecture has recently demonstrated effectiveness
in Machine Learning (ML), particularly within Natural Language Processing
(NLP). It has been utilized to construct word embedding using conjunctive
propositional clauses, thereby significantly enhancing our understanding and
interpretation of machine-derived decisions. The previous approach performed
the word embedding over a sequence of input words to consolidate the
information into a cohesive and unified representation. However, that approach
encounters scalability challenges as the input size increases. In this study,
we introduce a novel approach incorporating two-phase training to discover
contextual embeddings of input sequences. Specifically, this method
encapsulates the knowledge for each input word within the dataset's vocabulary,
subsequently constructing embeddings for a sequence of input words utilizing
the extracted knowledge. This technique not only facilitates the design of a
scalable model but also preserves interpretability. Our experimental findings
revealed that the proposed method yields competitive performance compared to
the previous approaches, demonstrating promising results in contrast to
human-generated benchmarks. Furthermore, we applied the proposed approach to
sentiment analysis on the IMDB dataset, where the TM embedding and the TM
classifier, along with other interpretable classifiers, offered a transparent
end-to-end solution with competitive performance.",http://arxiv.org/pdf/2501.19018v1,,False
BCAT: A Block Causal Transformer for PDE Foundation Models for Fluid Dynamics,31/01/2025,"Yuxuan Liu, Jingmin Sun, Hayden Schaeffer","We introduce BCAT, a PDE foundation model designed for autoregressive
prediction of solutions to two dimensional fluid dynamics problems. Our
approach uses a block causal transformer architecture to model next frame
predictions, leveraging previous frames as contextual priors rather than
relying solely on sub-frames or pixel-based inputs commonly used in image
generation methods. This block causal framework more effectively captures the
spatial dependencies inherent in nonlinear spatiotemporal dynamics and physical
phenomena. In an ablation study, next frame prediction demonstrated a 2.9x
accuracy improvement over next token prediction. BCAT is trained on a diverse
range of fluid dynamics datasets, including incompressible and compressible
Navier-Stokes equations across various geometries and parameter regimes, as
well as the shallow-water equations. The model's performance was evaluated on 6
distinct downstream prediction tasks and tested on about 8K trajectories to
measure robustness on a variety of fluid dynamics simulations. BCAT achieved an
average relative error of 1.92% across all evaluation tasks, outperforming
prior approaches on standard benchmarks.",http://arxiv.org/pdf/2501.18972v1,,False
Language Games as the Pathway to Artificial Superhuman Intelligence,31/01/2025,"Ying Wen, Ziyu Wan, Shao Zhang","The evolution of large language models (LLMs) toward artificial superhuman
intelligence (ASI) hinges on data reproduction, a cyclical process in which
models generate, curate and retrain on novel data to refine capabilities.
Current methods, however, risk getting stuck in a data reproduction trap:
optimizing outputs within fixed human-generated distributions in a closed loop
leads to stagnation, as models merely recombine existing knowledge rather than
explore new frontiers. In this paper, we propose language games as a pathway to
expanded data reproduction, breaking this cycle through three mechanisms: (1)
\textit{role fluidity}, which enhances data diversity and coverage by enabling
multi-agent systems to dynamically shift roles across tasks; (2) \textit{reward
variety}, embedding multiple feedback criteria that can drive complex
intelligent behaviors; and (3) \textit{rule plasticity}, iteratively evolving
interaction constraints to foster learnability, thereby injecting continual
novelty. By scaling language games into global sociotechnical ecosystems,
human-AI co-evolution generates unbounded data streams that drive open-ended
exploration. This framework redefines data reproduction not as a closed loop
but as an engine for superhuman intelligence.",http://arxiv.org/pdf/2501.18924v1,,False
Trustworthy Evaluation of Generative AI Models,31/01/2025,"Zijun Gao, Yan Sun","Generative AI (GenAI) models have recently achieved remarkable empirical
performance in various applications, however, their evaluations yet lack
uncertainty quantification. In this paper, we propose a method to compare two
generative models based on an unbiased estimator of their relative performance
gap. Statistically, our estimator achieves parametric convergence rate and
asymptotic normality, which enables valid inference. Computationally, our
method is efficient and can be accelerated by parallel computing and leveraging
pre-storing intermediate results. On simulated datasets with known ground
truth, we show our approach effectively controls type I error and achieves
power comparable with commonly used metrics. Furthermore, we demonstrate the
performance of our method in evaluating diffusion models on real image datasets
with statistical confidence.",http://arxiv.org/pdf/2501.18897v1,,False
CAAT-EHR: Cross-Attentional Autoregressive Transformer for Multimodal Electronic Health Record Embeddings,31/01/2025,"Mohammad Al Olaimat, Serdar Bozdag","Electronic health records (EHRs) provide a comprehensive source of
longitudinal patient data, encompassing structured modalities such as
laboratory results, imaging data, and vital signs, and unstructured clinical
notes. These datasets, after necessary preprocessing to clean and format the
data for analysis, often remain in their raw EHR form, representing numerical
or categorical values without further transformation into task-agnostic
embeddings. While such raw EHR data enables predictive modeling, its reliance
on manual feature engineering or downstream task-specific optimization limits
its utility for general-purpose applications. Deep learning (DL) techniques,
such as recurrent neural networks (RNNs) and Transformers, have facilitated
predictive tasks like disease progression and diagnosis prediction. However,
these methods often struggle to fully exploit the temporal and multimodal
dependencies inherent in EHR data due to their reliance on pre-processed but
untransformed raw EHR inputs. In this study, we introduce CAAT-EHR, a novel
architecture designed to bridge this gap by generating robust, task-agnostic
longitudinal embeddings from raw EHR data. CAAT-EHR leverages self- and
cross-attention mechanisms in its encoder to integrate temporal and contextual
relationships across multiple modalities, transforming the data into enriched
embeddings that capture complex dependencies. An autoregressive decoder
complements the encoder by predicting future time points data during
pre-training, ensuring that the resulting embeddings maintain temporal
consistency and alignment. CAAT-EHR eliminates the need for manual feature
engineering and enables seamless transferability across diverse downstream
tasks. Extensive evaluations on benchmark datasets, demonstrate the superiority
of CAAT-EHR-generated embeddings over pre-processed raw EHR data and other
baseline approaches.",http://arxiv.org/pdf/2501.18891v1,,False
"QMe14S, A Comprehensive and Efficient Spectral Dataset for Small Organic Molecules",31/01/2025,"Mingzhi Yuan, Zihan Zou, Wei Hu","Developing machine learning protocols for molecular simulations requires
comprehensive and efficient datasets. Here we introduce the QMe14S dataset,
comprising 186,102 small organic molecules featuring 14 elements (H, B, C, N,
O, F, Al, Si, P, S, Cl, As, Se, Br) and 47 functional groups. Using density
functional theory at the B3LYP/TZVP level, we optimized the geometries and
calculated properties including energy, atomic charge, atomic force, dipole
moment, quadrupole moment, polarizability, octupole moment, first
hyperpolarizability, and Hessian. At the same level, we obtained the harmonic
IR, Raman and NMR spectra. Furthermore, we conducted ab initio molecular
dynamics simulations to generate dynamic configurations and extract
nonequilibrium properties, including energy, forces, and Hessians. By
leveraging our E(3)-equivariant message-passing neural network (DetaNet), we
demonstrated that models trained on QMe14S outperform those trained on the
previously developed QM9S dataset in simulating molecular spectra. The QMe14S
dataset thus serves as a comprehensive benchmark for molecular simulations,
offering valuable insights into structure-property relationships.",http://arxiv.org/pdf/2501.18876v1,,False
Neural SDEs as a Unified Approach to Continuous-Domain Sequence Modeling,31/01/2025,"Macheng Shen, Chen Cheng","Inspired by the ubiquitous use of differential equations to model continuous
dynamics across diverse scientific and engineering domains, we propose a novel
and intuitive approach to continuous sequence modeling. Our method interprets
time-series data as \textit{discrete samples from an underlying continuous
dynamical system}, and models its time evolution using Neural Stochastic
Differential Equation (Neural SDE), where both the flow (drift) and diffusion
terms are parameterized by neural networks. We derive a principled maximum
likelihood objective and a \textit{simulation-free} scheme for efficient
training of our Neural SDE model. We demonstrate the versatility of our
approach through experiments on sequence modeling tasks across both embodied
and generative AI. Notably, to the best of our knowledge, this is the first
work to show that SDE-based continuous-time modeling also excels in such
complex scenarios, and we hope that our work opens up new avenues for research
of SDE models in high-dimensional and temporally intricate domains.",http://arxiv.org/pdf/2501.18871v1,,False
Partially Rewriting a Transformer in Natural Language,31/01/2025,"Gonçalo Paulo, Nora Belrose","The greatest ambition of mechanistic interpretability is to completely
rewrite deep neural networks in a format that is more amenable to human
understanding, while preserving their behavior and performance. In this paper,
we attempt to partially rewrite a large language model using simple natural
language explanations. We first approximate one of the feedforward networks in
the LLM with a wider MLP with sparsely activating neurons - a transcoder - and
use an automated interpretability pipeline to generate explanations for these
neurons. We then replace the first layer of this sparse MLP with an LLM-based
simulator, which predicts the activation of each neuron given its explanation
and the surrounding context. Finally, we measure the degree to which these
modifications distort the model's final output. With our pipeline, the model's
increase in loss is statistically similar to entirely replacing the sparse MLP
output with the zero vector. We employ the same protocol, this time using a
sparse autoencoder, on the residual stream of the same layer and obtain similar
results. These results suggest that more detailed explanations are needed to
improve performance substantially above the zero ablation baseline.",http://arxiv.org/pdf/2501.18838v1,,False
Learning Hamiltonian Dynamics with Bayesian Data Assimilation,31/01/2025,"Taehyeun Kim, Tae-Geun Kim, Anouck Girard, Ilya Kolmanovsky","In this paper, we develop a neural network-based approach for time-series
prediction in unknown Hamiltonian dynamical systems. Our approach leverages a
surrogate model and learns the system dynamics using generalized coordinates
(positions) and their conjugate momenta while preserving a constant
Hamiltonian. To further enhance long-term prediction accuracy, we introduce an
Autoregressive Hamiltonian Neural Network, which incorporates autoregressive
prediction errors into the training objective. Additionally, we employ Bayesian
data assimilation to refine predictions in real-time using online measurement
data. Numerical experiments on a spring-mass system and highly elliptic orbits
under gravitational perturbations demonstrate the effectiveness of the proposed
method, highlighting its potential for accurate and robust long-term
predictions.",http://arxiv.org/pdf/2501.18808v1,,False
