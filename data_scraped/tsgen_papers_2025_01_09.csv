Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
Natural Variational Annealing for Multimodal Optimization,08/01/2025,"Tâm Le Minh, Julyan Arbel, Thomas Möllenhoff, Mohammad Emtiyaz Khan, Florence Forbes","We introduce a new multimodal optimization approach called Natural
Variational Annealing (NVA) that combines the strengths of three foundational
concepts to simultaneously search for multiple global and local modes of
black-box nonconvex objectives. First, it implements a simultaneous search by
using variational posteriors, such as, mixtures of Gaussians. Second, it
applies annealing to gradually trade off exploration for exploitation. Finally,
it learns the variational search distribution using natural-gradient learning
where updates resemble well-known and easy-to-implement algorithms. The three
concepts come together in NVA giving rise to new algorithms and also allowing
us to incorporate ""fitness shaping"", a core concept from evolutionary
algorithms. We assess the quality of search on simulations and compare them to
methods using gradient descent and evolution strategies. We also provide an
application to a real-world inverse problem in planetary science.",http://arxiv.org/pdf/2501.04667v1,,False
A Statistical Theory of Contrastive Pre-training and Multimodal Generative AI,08/01/2025,"Kazusato Oko, Licong Lin, Yuhang Cai, Song Mei","Multi-modal generative AI systems, such as those combining vision and
language, rely on contrastive pre-training to learn representations across
different modalities. While their practical benefits are widely acknowledged, a
rigorous theoretical understanding of the contrastive pre-training framework
remains limited. This paper develops a theoretical framework to explain the
success of contrastive pre-training in downstream tasks, such as zero-shot
classification, conditional diffusion models, and vision-language models. We
introduce the concept of approximate sufficient statistics, a generalization of
the classical sufficient statistics, and show that near-minimizers of the
contrastive pre-training loss are approximately sufficient, making them
adaptable to diverse downstream tasks. We further propose the Joint Generative
Hierarchical Model for the joint distribution of images and text, showing that
transformers can efficiently approximate relevant functions within this model
via belief propagation. Building on this framework, we derive sample complexity
guarantees for multi-modal learning based on contrastive pre-trained
representations. Numerical simulations validate these theoretical findings,
demonstrating the strong generalization performance of contrastively
pre-trained transformers in various multi-modal tasks.",http://arxiv.org/pdf/2501.04641v1,,False
Accelerated Extragradient-Type Methods -- Part 2: Generalization and Sublinear Convergence Rates under Co-Hypomonotonicity,08/01/2025,"Quoc Tran-Dinh, Nghia Nguyen-Trung","Following the first part of our project, this paper comprehensively studies
two types of extragradient-based methods: anchored extragradient and Nesterov's
accelerated extragradient for solving [non]linear inclusions (and, in
particular, equations), primarily under the Lipschitz continuity and the
co-hypomonotonicity assumptions. We unify and generalize a class of anchored
extragradient methods for monotone inclusions to a wider range of schemes
encompassing existing algorithms as special cases. We establish
$\mathcal{O}(1/k)$ last-iterate convergence rates on the residual norm of the
underlying mapping for this general framework and then specialize it to obtain
convergence guarantees for specific instances, where $k$ denotes the iteration
counter. We extend our approach to a class of anchored Tseng's
forward-backward-forward splitting methods to obtain a broader class of
algorithms for solving co-hypomonotone inclusions. Again, we analyze
$\mathcal{O}(1/k)$ last-iterate convergence rates for this general scheme and
specialize it to obtain convergence results for existing and new variants. We
generalize and unify Nesterov's accelerated extra-gradient method to a new
class of algorithms that covers existing schemes as special instances while
generating new variants. For these schemes, we can prove $\mathcal{O}(1/k)$
last-iterate convergence rates for the residual norm under co-hypomonotonicity,
covering a class of nonmonotone problems. We propose another novel class of
Nesterov's accelerated extragradient methods to solve inclusions.
Interestingly, these algorithms achieve both $\mathcal{O}(1/k)$ and $o(1/k)$
last-iterate convergence rates, and also the convergence of iterate sequences
under co-hypomonotonicity and Lipschitz continuity. Finally, we provide a set
of numerical experiments encompassing different scenarios to validate our
algorithms and theoretical guarantees.",http://arxiv.org/pdf/2501.04585v1,,False
A Plug-and-Play Bregman ADMM Module for Inferring Event Branches in Temporal Point Processes,08/01/2025,"Qingmei Wang, Yuxin Wu, Yujie Long, Jing Huang, Fengyuan Ran, Bing Su, Hongteng Xu","An event sequence generated by a temporal point process is often associated
with a hidden and structured event branching process that captures the
triggering relations between its historical and current events. In this study,
we design a new plug-and-play module based on the Bregman ADMM (BADMM)
algorithm, which infers event branches associated with event sequences in the
maximum likelihood estimation framework of temporal point processes (TPPs).
Specifically, we formulate the inference of event branches as an optimization
problem for the event transition matrix under sparse and low-rank constraints,
which is embedded in existing TPP models or their learning paradigms. We can
implement this optimization problem based on subspace clustering and sparse
group-lasso, respectively, and solve it using the Bregman ADMM algorithm, whose
unrolling leads to the proposed BADMM module. When learning a classic TPP
(e.g., Hawkes process) by the expectation-maximization algorithm, the BADMM
module helps derive structured responsibility matrices in the E-step.
Similarly, the BADMM module helps derive low-rank and sparse attention maps for
the neural TPPs with self-attention layers. The structured responsibility
matrices and attention maps, which work as learned event transition matrices,
indicate event branches, e.g., inferring isolated events and those key events
triggering many subsequent events. Experiments on both synthetic and real-world
data show that plugging our BADMM module into existing TPP models and learning
paradigms can improve model performance and provide us with interpretable
structured event branches. The code is available at
\url{https://github.com/qingmeiwangdaily/BADMM_TPP}.",http://arxiv.org/pdf/2501.04529v1,,False
"Integrating remote sensing data assimilation, deep learning and large language model for interactive wheat breeding yield prediction",08/01/2025,"Guofeng Yang, Nanfei Jin, Wenjie Ai, Zhonghua Zheng, Yuhong He, Yong He","Yield is one of the core goals of crop breeding. By predicting the potential
yield of different breeding materials, breeders can screen these materials at
various growth stages to select the best performing. Based on unmanned aerial
vehicle remote sensing technology, high-throughput crop phenotyping data in
breeding areas is collected to provide data support for the breeding decisions
of breeders. However, the accuracy of current yield predictions still requires
improvement, and the usability and user-friendliness of yield forecasting tools
remain suboptimal. To address these challenges, this study introduces a hybrid
method and tool for crop yield prediction, designed to allow breeders to
interactively and accurately predict wheat yield by chatting with a large
language model (LLM). First, the newly designed data assimilation algorithm is
used to assimilate the leaf area index into the WOFOST model. Then, selected
outputs from the assimilation process, along with remote sensing inversion
results, are used to drive the time-series temporal fusion transformer model
for wheat yield prediction. Finally, based on this hybrid method and leveraging
an LLM with retrieval augmented generation technology, we developed an
interactive yield prediction Web tool that is user-friendly and supports
sustainable data updates. This tool integrates multi-source data to assist
breeding decision-making. This study aims to accelerate the identification of
high-yield materials in the breeding process, enhance breeding efficiency, and
enable more scientific and smart breeding decisions.",http://arxiv.org/pdf/2501.04487v1,,False
"A Digital Shadow for Modeling, Studying and Preventing Urban Crime",08/01/2025,"Juan Palma-Borda, Eduardo Guzmán, María-Victoria Belmonte","Crime is one of the greatest threats to urban security. Around 80 percent of
the world's population lives in countries with high levels of criminality. Most
of the crimes committed in the cities take place in their urban environments.
This paper presents the development and validation of a digital shadow platform
for modeling and simulating urban crime. This digital shadow has been
constructed using data-driven agent-based modeling and simulation techniques,
which are suitable for capturing dynamic interactions among individuals and
with their environment. Our approach transforms and integrates well-known
criminological theories and the expert knowledge of law enforcement agencies
(LEA), policy makers, and other stakeholders under a theoretical model, which
is in turn combined with real crime, spatial (cartographic) and socio-economic
data into an urban model characterizing the daily behavior of citizens. The
digital shadow has also been instantiated for the city of Malaga, for which we
had over 300,000 complaints available. This instance has been calibrated with
those complaints and other geographic and socio-economic information of the
city. To the best of our knowledge, our digital shadow is the first for large
urban areas that has been calibrated with a large dataset of real crime reports
and with an accurate representation of the urban environment. The performance
indicators of the model after being calibrated, in terms of the metrics widely
used in predictive policing, suggest that our simulated crime generation
matches the general pattern of crime in the city according to historical data.
Our digital shadow platform could be an interesting tool for modeling and
predicting criminal behavior in an urban environment on a daily basis and,
thus, a useful tool for policy makers, criminologists, sociologists, LEAs, etc.
to study and prevent urban crime.",http://arxiv.org/pdf/2501.04435v1,,False
Dual-Force: Enhanced Offline Diversity Maximization under Imitation Constraints,08/01/2025,"Pavel Kolev, Marin Vlastelica, Georg Martius","While many algorithms for diversity maximization under imitation constraints
are online in nature, many applications require offline algorithms without
environment interactions. Tackling this problem in the offline setting,
however, presents significant challenges that require non-trivial, multi-stage
optimization processes with non-stationary rewards. In this work, we present a
novel offline algorithm that enhances diversity using an objective based on Van
der Waals (VdW) force and successor features, and eliminates the need to learn
a previously used skill discriminator. Moreover, by conditioning the value
function and policy on a pre-trained Functional Reward Encoding (FRE), our
method allows for better handling of non-stationary rewards and provides
zero-shot recall of all skills encountered during training, significantly
expanding the set of skills learned in prior work. Consequently, our algorithm
benefits from receiving a consistently strong diversity signal (VdW), and
enjoys more stable and efficient training. We demonstrate the effectiveness of
our method in generating diverse skills for two robotic tasks in simulation:
locomotion of a quadruped and local navigation with obstacle traversal.",http://arxiv.org/pdf/2501.04426v1,,False
"User Simulation in the Era of Generative AI: User Modeling, Synthetic Data Generation, and System Evaluation",08/01/2025,"Krisztian Balog, ChengXiang Zhai","User simulation is an emerging interdisciplinary topic with multiple critical
applications in the era of Generative AI. It involves creating an intelligent
agent that mimics the actions of a human user interacting with an AI system,
enabling researchers to model and analyze user behaviour, generate synthetic
data for training, and evaluate interactive AI systems in a controlled and
reproducible manner. User simulation has profound implications for diverse
fields and plays a vital role in the pursuit of Artificial General
Intelligence. This paper provides an overview of user simulation, highlighting
its key applications, connections to various disciplines, and outlining future
research directions to advance this increasingly important technology.",http://arxiv.org/pdf/2501.04410v1,,False
Decoding EEG Speech Perception with Transformers and VAE-based Data Augmentation,08/01/2025,"Terrance Yu-Hao Chen, Yulin Chen, Pontus Soederhaell, Sadrishya Agrawal, Kateryna Shapovalenko","Decoding speech from non-invasive brain signals, such as
electroencephalography (EEG), has the potential to advance brain-computer
interfaces (BCIs), with applications in silent communication and assistive
technologies for individuals with speech impairments. However, EEG-based speech
decoding faces major challenges, such as noisy data, limited datasets, and poor
performance on complex tasks like speech perception. This study attempts to
address these challenges by employing variational autoencoders (VAEs) for EEG
data augmentation to improve data quality and applying a state-of-the-art
(SOTA) sequence-to-sequence deep learning architecture, originally successful
in electromyography (EMG) tasks, to EEG-based speech decoding. Additionally, we
adapt this architecture for word classification tasks. Using the Brennan
dataset, which contains EEG recordings of subjects listening to narrated
speech, we preprocess the data and evaluate both classification and
sequence-to-sequence models for EEG-to-words/sentences tasks. Our experiments
show that VAEs have the potential to reconstruct artificial EEG data for
augmentation. Meanwhile, our sequence-to-sequence model achieves more promising
performance in generating sentences compared to our classification model,
though both remain challenging tasks. These findings lay the groundwork for
future research on EEG speech perception decoding, with possible extensions to
speech production tasks such as silent or imagined speech.",http://arxiv.org/pdf/2501.04359v1,,False
Neural Parameter Estimation with Incomplete Data,08/01/2025,"Matthew Sainsbury-Dale, Andrew Zammit-Mangion, Noel Cressie, Raphaël Huser","Advancements in artificial intelligence (AI) and deep learning have led to
neural networks being used to generate lightning-speed answers to complex
questions, to paint like Monet, or to write like Proust. Leveraging their
computational speed and flexibility, neural networks are also being used to
facilitate fast, likelihood-free statistical inference. However, it is not
straightforward to use neural networks with data that for various reasons are
incomplete, which precludes their use in many applications. A recently proposed
approach to remedy this issue inputs an appropriately padded data vector and a
vector that encodes the missingness pattern to a neural network. While
computationally efficient, this ""masking"" approach can result in statistically
inefficient inferences. Here, we propose an alternative approach that is based
on the Monte Carlo expectation-maximization (EM) algorithm. Our EM approach is
likelihood-free, substantially faster than the conventional EM algorithm as it
does not require numerical optimization at each iteration, and more
statistically efficient than the masking approach. This research represents a
prototype problem that indicates how improvements could be made in AI by
introducing Bayesian statistical thinking. We compare the two approaches to
missingness using simulated incomplete data from two models: a spatial Gaussian
process model, and a spatial Potts model. The utility of the methodology is
shown on Arctic sea-ice data and cryptocurrency data.",http://arxiv.org/pdf/2501.04330v1,,False
FSC-loss: A Frequency-domain Structure Consistency Learning Approach for Signal Data Recovery and Reconstruction,08/01/2025,"Liwen Zhang, Zhaoji Miao, Fan Yang, Gen Shi, Jie He, Yu An, Hui Hui, Jie Tian","A core challenge for signal data recovery is to model the distribution of
signal matrix (SM) data based on measured low-quality data in biomedical
engineering of magnetic particle imaging (MPI). For acquiring the
high-resolution (high-quality) SM, the number of meticulous measurements at
numerous positions in the field-of-view proves time-consuming (measurement of a
37x37x37 SM takes about 32 hours). To improve reconstructed signal quality and
shorten SM measurement time, existing methods explore to generating
high-resolution SM based on time-saving measured low-resolution SM (a 9x9x9 SM
just takes about 0.5 hours). However, previous methods show poor performance
for high-frequency signal recovery in SM. To achieve a high-resolution SM
recovery and shorten its acquisition time, we propose a frequency-domain
structure consistency loss function and data component embedding strategy to
model global and local structural information of SM. We adopt a
transformer-based network to evaluate this function and the strategy. We
evaluate our methods and state-of-the-art (SOTA) methods on the two simulation
datasets and four public measured SMs in Open MPI Data. The results show that
our method outperforms the SOTA methods in high-frequency structural signal
recovery. Additionally, our method can recover a high-resolution SM with clear
high-frequency structure based on a down-sampling factor of 16 less than 15
seconds, which accelerates the acquisition time over 60 times faster than the
measurement-based HR SM with the minimum error (nRMSE=0.041). Moreover, our
method is applied in our three in-house MPI systems, and boost their
performance for signal reconstruction.",http://arxiv.org/pdf/2501.04308v1,,False
Physics-Informed Super-Resolution Diffusion for 6D Phase Space Diagnostics,08/01/2025,Alexander Scheinker,"Adaptive physics-informed super-resolution diffusion is developed for
non-invasive virtual diagnostics of the 6D phase space density of charged
particle beams. An adaptive variational autoencoder (VAE) embeds initial beam
condition images and scalar measurements to a low-dimensional latent space from
which a 326 pixel 6D tensor representation of the beam's 6D phase space density
is generated. Projecting from a 6D tensor generates physically consistent 2D
projections. Physics-guided super-resolution diffusion transforms
low-resolution images of the 6D density to high resolution 256x256 pixel
images. Un-supervised adaptive latent space tuning enables tracking of
time-varying beams without knowledge of time-varying initial conditions. The
method is demonstrated with experimental data and multi-particle simulations at
the HiRES UED. The general approach is applicable to a wide range of complex
dynamic systems evolving in high-dimensional phase space. The method is shown
to be robust to distribution shift without re-training.",http://arxiv.org/pdf/2501.04305v1,,False
Circuit Complexity Bounds for Visual Autoregressive Model,08/01/2025,"Yekun Ke, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song","Understanding the expressive ability of a specific model is essential for
grasping its capacity limitations. Recently, several studies have established
circuit complexity bounds for Transformer architecture. Besides, the Visual
AutoRegressive (VAR) model has risen to be a prominent method in the field of
image generation, outperforming previous techniques, such as Diffusion
Transformers, in generating high-quality images. We investigate the circuit
complexity of the VAR model and establish a bound in this study. Our primary
result demonstrates that the VAR model is equivalent to a simulation by a
uniform $\mathsf{TC}^0$ threshold circuit with hidden dimension $d \leq O(n)$
and $\mathrm{poly}(n)$ precision. This is the first study to rigorously
highlight the limitations in the expressive power of VAR models despite their
impressive performance. We believe our findings will offer valuable insights
into the inherent constraints of these models and guide the development of more
efficient and expressive architectures in the future.",http://arxiv.org/pdf/2501.04299v1,,False
