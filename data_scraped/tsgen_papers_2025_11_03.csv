Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
Continuous Autoregressive Language Models,31/10/2025,"Chenze Shao, Darren Li, Fandong Meng, Jie Zhou","The efficiency of large language models (LLMs) is fundamentally limited by
their sequential, token-by-token generation process. We argue that overcoming
this bottleneck requires a new design axis for LLM scaling: increasing the
semantic bandwidth of each generative step. To this end, we introduce
Continuous Autoregressive Language Models (CALM), a paradigm shift from
discrete next-token prediction to continuous next-vector prediction. CALM uses
a high-fidelity autoencoder to compress a chunk of K tokens into a single
continuous vector, from which the original tokens can be reconstructed with
over 99.9\% accuracy. This allows us to model language as a sequence of
continuous vectors instead of discrete tokens, which reduces the number of
generative steps by a factor of K. The paradigm shift necessitates a new
modeling toolkit; therefore, we develop a comprehensive likelihood-free
framework that enables robust training, evaluation, and controllable sampling
in the continuous domain. Experiments show that CALM significantly improves the
performance-compute trade-off, achieving the performance of strong discrete
baselines at a significantly lower computational cost. More importantly, these
findings establish next-vector prediction as a powerful and scalable pathway
towards ultra-efficient language models. Code:
https://github.com/shaochenze/calm. Project:
https://shaochenze.github.io/blog/2025/CALM.",http://arxiv.org/pdf/2510.27688v1,,False
MolChord: Structure-Sequence Alignment for Protein-Guided Drug Design,31/10/2025,"Wei Zhang, Zekun Guo, Yingce Xia, Peiran Jin, Shufang Xie, Tao Qin, Xiang-Yang Li","Structure-based drug design (SBDD), which maps target proteins to candidate
molecular ligands, is a fundamental task in drug discovery. Effectively
aligning protein structural representations with molecular representations, and
ensuring alignment between generated drugs and their pharmacological
properties, remains a critical challenge. To address these challenges, we
propose MolChord, which integrates two key techniques: (1) to align protein and
molecule structures with their textual descriptions and sequential
representations (e.g., FASTA for proteins and SMILES for molecules), we
leverage NatureLM, an autoregressive model unifying text, small molecules, and
proteins, as the molecule generator, alongside a diffusion-based structure
encoder; and (2) to guide molecules toward desired properties, we curate a
property-aware dataset by integrating preference data and refine the alignment
process using Direct Preference Optimization (DPO). Experimental results on
CrossDocked2020 demonstrate that our approach achieves state-of-the-art
performance on key evaluation metrics, highlighting its potential as a
practical tool for SBDD.",http://arxiv.org/pdf/2510.27671v1,,False
Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models,31/10/2025,"Boyi Wei, Zora Che, Nathaniel Li, Udari Madhushani Sehwag, Jasper Götting, Samira Nedungadi, Julian Michael, Summer Yue, Dan Hendrycks, Peter Henderson, Zifan Wang, Seth Donoughe, Mantas Mazeika","Open-weight bio-foundation models present a dual-use dilemma. While holding
great promise for accelerating scientific research and drug development, they
could also enable bad actors to develop more deadly bioweapons. To mitigate the
risk posed by these models, current approaches focus on filtering biohazardous
data during pre-training. However, the effectiveness of such an approach
remains unclear, particularly against determined actors who might fine-tune
these models for malicious use. To address this gap, we propose \eval, a
framework to evaluate the robustness of procedures that are intended to reduce
the dual-use capabilities of bio-foundation models. \eval assesses models'
virus understanding through three lenses, including sequence modeling,
mutational effects prediction, and virulence prediction. Our results show that
current filtering practices may not be particularly effective: Excluded
knowledge can be rapidly recovered in some cases via fine-tuning, and exhibits
broader generalizability in sequence modeling. Furthermore, dual-use signals
may already reside in the pretrained representations, and can be elicited via
simple linear probing. These findings highlight the challenges of data
filtering as a standalone procedure, underscoring the need for further research
into robust safety and security strategies for open-weight bio-foundation
models.",http://arxiv.org/pdf/2510.27629v1,,False
Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs,31/10/2025,"Sushil Samuel Dinesh, Shinkyu Park","This paper presents a framework that leverages pre-trained foundation models
for robotic manipulation without domain-specific training. The framework
integrates off-the-shelf models, combining multimodal perception from
foundation models with a general-purpose reasoning model capable of robust task
sequencing. Scene graphs, dynamically maintained within the framework, provide
spatial awareness and enable consistent reasoning about the environment. The
framework is evaluated through a series of tabletop robotic manipulation
experiments, and the results highlight its potential for building robotic
manipulation systems directly on top of off-the-shelf foundation models.",http://arxiv.org/pdf/2510.27558v1,,False
EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities,31/10/2025,"Travis Davies, Yiqi Huang, Alexi Gladstone, Yunxin Liu, Xiang Chen, Heng Ji, Huxian Liu, Luhui Hu","Implicit policies parameterized by generative models, such as Diffusion
Policy, have become the standard for policy learning and Vision-Language-Action
(VLA) models in robotics. However, these approaches often suffer from high
computational cost, exposure bias, and unstable inference dynamics, which lead
to divergence under distribution shifts. Energy-Based Models (EBMs) address
these issues by learning energy landscapes end-to-end and modeling equilibrium
dynamics, offering improved robustness and reduced exposure bias. Yet, policies
parameterized by EBMs have historically struggled to scale effectively. Recent
work on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs
to high-dimensional spaces, but their potential for solving core challenges in
physically embodied models remains underexplored. We introduce a new
energy-based architecture, EBT-Policy, that solves core issues in robotic and
real-world settings. Across simulated and real-world tasks, EBT-Policy
consistently outperforms diffusion-based policies, while requiring less
training and inference computation. Remarkably, on some tasks it converges
within just two inference steps, a 50x reduction compared to Diffusion Policy's
100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior
models, such as zero-shot recovery from failed action sequences using only
behavior cloning and without explicit retry training. By leveraging its scalar
energy for uncertainty-aware inference and dynamic compute allocation,
EBT-Policy offers a promising path toward robust, generalizable robot behavior
under distribution shifts.",http://arxiv.org/pdf/2510.27545v1,,False
"Mechanics of Learned Reasoning 1: TempoBench, A Benchmark for Interpretable Deconstruction of Reasoning System Performance",31/10/2025,"Nikolaus Holzer, William Fishell, Baishakhi Ray, Mark Santolucito","Large Language Models (LLMs) are increasingly excelling and outpacing human
performance on many tasks. However, to improve LLM reasoning, researchers
either rely on ad-hoc generated datasets or formal mathematical proof systems
such as the Lean proof assistant. Whilst ad-hoc generated methods can capture
the decision chains of real-world reasoning processes, they may encode some
inadvertent bias in the space of reasoning they cover; they also cannot be
formally verified. On the other hand, systems like Lean can guarantee
verifiability, but are not well-suited to capture the nature of agentic
decision chain-based tasks. This creates a gap both in performance for
functions such as business agents or code assistants, and in the usefulness of
LLM reasoning benchmarks, whereby these fall short in reasoning structure or
real-world alignment. We introduce TempoBench, the first formally grounded and
verifiable diagnostic benchmark that parametrizes difficulty to systematically
analyze how LLMs perform reasoning. TempoBench uses two evaluation benchmarks
to break down reasoning ability. First, temporal trace evaluation (TTE) tests
the ability of an LLM to understand and simulate the execution of a given
multi-step reasoning system. Subsequently, temporal causal evaluation (TCE)
tests an LLM's ability to perform multi-step causal reasoning and to distill
cause-and-effect relations from complex systems. We find that models score
65.6% on TCE-normal, and 7.5% on TCE-hard. This shows that state-of-the-art
LLMs clearly understand the TCE task but perform poorly as system complexity
increases. Our code is available at our
\href{https://github.com/nik-hz/tempobench}{GitHub repository}.",http://arxiv.org/pdf/2510.27544v1,,False
Leveraging Generic Time Series Foundation Models for EEG Classification,31/10/2025,"Théo Gnassounou, Yessin Moakher, Shifeng Xie, Vasilii Feofanov, Ievgen Redko","Foundation models for time series are emerging as powerful general-purpose
backbones, yet their potential for domain-specific biomedical signals such as
electroencephalography (EEG) remains rather unexplored. In this work, we
investigate the applicability a recently proposed time series classification
foundation model, to a different EEG tasks such as motor imagery classification
and sleep stage prediction. We test two pretraining regimes: (a) pretraining on
heterogeneous real-world time series from multiple domains, and (b) pretraining
on purely synthetic data. We find that both variants yield strong performance,
consistently outperforming EEGNet, a widely used convolutional baseline, and
CBraMod, the most recent EEG-specific foundation model. These results suggest
that generalist time series foundation models, even when pretrained on data of
non-neural origin or on synthetic signals, can transfer effectively to EEG. Our
findings highlight the promise of leveraging cross-domain pretrained models for
brain signal analysis, suggesting that EEG may benefit from advances in the
broader time series literature.",http://arxiv.org/pdf/2510.27522v1,,False
BiSparse-AAS: Bilinear Sparse Attention and Adaptive Spans Framework for Scalable and Efficient Text Summarization,31/10/2025,"Desta Haileselassie Hagos, Legand L. Burge, Anietie Andy, Anis Yazidi, Vladimir Vlassov","Transformer-based architectures have advanced text summarization, yet their
quadratic complexity limits scalability on long documents. This paper
introduces BiSparse-AAS (Bilinear Sparse Attention with Adaptive Spans), a
novel framework that combines sparse attention, adaptive spans, and bilinear
attention to address these limitations. Sparse attention reduces computational
costs by focusing on the most relevant parts of the input, while adaptive spans
dynamically adjust the attention ranges. Bilinear attention complements both by
modeling complex token interactions within this refined context. BiSparse-AAS
consistently outperforms state-of-the-art baselines in both extractive and
abstractive summarization tasks, achieving average ROUGE improvements of about
68.1% on CNN/DailyMail and 52.6% on XSum, while maintaining strong performance
on OpenWebText and Gigaword datasets. By addressing efficiency, scalability,
and long-sequence modeling, BiSparse-AAS provides a unified, practical solution
for real-world text summarization applications.",http://arxiv.org/pdf/2510.27516v1,,False
InertialAR: Autoregressive 3D Molecule Generation with Inertial Frames,31/10/2025,"Haorui Li, Weitao Du, Yuqiang Li, Hongyu Guo, Shengchao Liu","Transformer-based autoregressive models have emerged as a unifying paradigm
across modalities such as text and images, but their extension to 3D molecule
generation remains underexplored. The gap stems from two fundamental
challenges: (1) tokenizing molecules into a canonical 1D sequence of tokens
that is invariant to both SE(3) transformations and atom index permutations,
and (2) designing an architecture capable of modeling hybrid atom-based tokens
that couple discrete atom types with continuous 3D coordinates. To address
these challenges, we introduce InertialAR. InertialAR devises a canonical
tokenization that aligns molecules to their inertial frames and reorders atoms
to ensure SE(3) and permutation invariance. Moreover, InertialAR equips the
attention mechanism with geometric awareness via geometric rotary positional
encoding (GeoRoPE). In addition, it utilizes a hierarchical autoregressive
paradigm to predict the next atom-based token, predicting the atom type first
and then its 3D coordinates via Diffusion loss. Experimentally, InertialAR
achieves state-of-the-art performance on 7 of the 10 evaluation metrics for
unconditional molecule generation across QM9, GEOM-Drugs, and B3LYP. Moreover,
it significantly outperforms strong baselines in controllable generation for
targeted chemical functionality, attaining state-of-the-art results across all
5 metrics.",http://arxiv.org/pdf/2510.27497v1,,False
CoMViT: An Efficient Vision Backbone for Supervised Classification in Medical Imaging,31/10/2025,"Aon Safdar, Mohamed Saadeldin","Vision Transformers (ViTs) have demonstrated strong potential in medical
imaging; however, their high computational demands and tendency to overfit on
small datasets limit their applicability in real-world clinical scenarios. In
this paper, we present CoMViT, a compact and generalizable Vision Transformer
architecture optimized for resource-constrained medical image analysis. CoMViT
integrates a convolutional tokenizer, diagonal masking, dynamic temperature
scaling, and pooling-based sequence aggregation to improve performance and
generalization. Through systematic architectural optimization, CoMViT achieves
robust performance across twelve MedMNIST datasets while maintaining a
lightweight design with only ~4.5M parameters. It matches or outperforms deeper
CNN and ViT variants, offering up to 5-20x parameter reduction without
sacrificing accuracy. Qualitative Grad-CAM analyses show that CoMViT
consistently attends to clinically relevant regions despite its compact size.
These results highlight the potential of principled ViT redesign for developing
efficient and interpretable models in low-resource medical imaging settings.",http://arxiv.org/pdf/2510.27442v1,,False
Learning Soft Robotic Dynamics with Active Exploration,31/10/2025,"Hehui Zheng, Bhavya Sukhija, Chenhao Li, Klemens Iten, Andreas Krause, Robert K. Katzschmann","Soft robots offer unmatched adaptability and safety in unstructured
environments, yet their compliant, high-dimensional, and nonlinear dynamics
make modeling for control notoriously difficult. Existing data-driven
approaches often fail to generalize, constrained by narrowly focused task
demonstrations or inefficient random exploration. We introduce SoftAE, an
uncertainty-aware active exploration framework that autonomously learns
task-agnostic and generalizable dynamics models of soft robotic systems. SoftAE
employs probabilistic ensemble models to estimate epistemic uncertainty and
actively guides exploration toward underrepresented regions of the state-action
space, achieving efficient coverage of diverse behaviors without task-specific
supervision. We evaluate SoftAE on three simulated soft robotic platforms -- a
continuum arm, an articulated fish in fluid, and a musculoskeletal leg with
hybrid actuation -- and on a pneumatically actuated continuum soft arm in the
real world. Compared with random exploration and task-specific model-based
reinforcement learning, SoftAE produces more accurate dynamics models, enables
superior zero-shot control on unseen tasks, and maintains robustness under
sensing noise, actuation delays, and nonlinear material effects. These results
demonstrate that uncertainty-driven active exploration can yield scalable,
reusable dynamics models across diverse soft robotic morphologies, representing
a step toward more autonomous, adaptable, and data-efficient control in
compliant robots.",http://arxiv.org/pdf/2510.27428v1,,False
Dialogue as Discovery: Navigating Human Intent Through Principled Inquiry,31/10/2025,"Jianwen Sun, Yukang Feng, Yifan Chang, Chuanhao Li, Zizhen Li, Jiaxin Ai, Fanrui Zhang, Yu Dai, Kaipeng Zhang","A fundamental bottleneck in human-AI collaboration is the ""intention
expression gap,"" the difficulty for humans to effectively convey complex,
high-dimensional thoughts to AI. This challenge often traps users in
inefficient trial-and-error loops and is exacerbated by the diverse expertise
levels of users. We reframe this problem from passive instruction following to
a Socratic collaboration paradigm, proposing an agent that actively probes for
information to resolve its uncertainty about user intent. we name the proposed
agent Nous, trained to acquire proficiency in this inquiry policy. The core
mechanism of Nous is a training framework grounded in the first principles of
information theory. Within this framework, we define the information gain from
dialogue as an intrinsic reward signal, which is fundamentally equivalent to
the reduction of Shannon entropy over a structured task space. This reward
design enables us to avoid reliance on costly human preference annotations or
external reward models. To validate our framework, we develop an automated
simulation pipeline to generate a large-scale, preference-based dataset for the
challenging task of scientific diagram generation. Comprehensive experiments,
including ablations, subjective and objective evaluations, and tests across
user expertise levels, demonstrate the effectiveness of our proposed framework.
Nous achieves leading efficiency and output quality, while remaining robust to
varying user expertise. Moreover, its design is domain-agnostic, and we show
evidence of generalization beyond diagram generation. Experimental results
prove that our work offers a principled, scalable, and adaptive paradigm for
resolving uncertainty about user intent in complex human-AI collaboration.",http://arxiv.org/pdf/2510.27410v1,,False
On the Equivalence of Optimal Transport Problem and Action Matching with Optimal Vector Fields,31/10/2025,"Nikita Kornilov, Alexander Korotin","Flow Matching (FM) method in generative modeling maps arbitrary probability
distributions by constructing an interpolation between them and then learning
the vector field that defines ODE for this interpolation. Recently, it was
shown that FM can be modified to map distributions optimally in terms of the
quadratic cost function for any initial interpolation. To achieve this, only
specific optimal vector fields, which are typical for solutions of Optimal
Transport (OT) problems, need to be considered during FM loss minimization. In
this note, we show that considering only optimal vector fields can lead to OT
in another approach: Action Matching (AM). Unlike FM, which learns a vector
field for a manually chosen interpolation between given distributions, AM
learns the vector field that defines ODE for an entire given sequence of
distributions.",http://arxiv.org/pdf/2510.27385v1,,False
Spiking Neural Networks: The Future of Brain-Inspired Computing,31/10/2025,Sales G. Aribe Jr,"Spiking Neural Networks (SNNs) represent the latest generation of neural
computation, offering a brain-inspired alternative to conventional Artificial
Neural Networks (ANNs). Unlike ANNs, which depend on continuous-valued signals,
SNNs operate using distinct spike events, making them inherently more
energy-efficient and temporally dynamic. This study presents a comprehensive
analysis of SNN design models, training algorithms, and multi-dimensional
performance metrics, including accuracy, energy consumption, latency, spike
count, and convergence behavior. Key neuron models such as the Leaky
Integrate-and-Fire (LIF) and training strategies, including surrogate gradient
descent, ANN-to-SNN conversion, and Spike-Timing Dependent Plasticity (STDP),
are examined in depth. Results show that surrogate gradient-trained SNNs
closely approximate ANN accuracy (within 1-2%), with faster convergence by the
20th epoch and latency as low as 10 milliseconds. Converted SNNs also achieve
competitive performance but require higher spike counts and longer simulation
windows. STDP-based SNNs, though slower to converge, exhibit the lowest spike
counts and energy consumption (as low as 5 millijoules per inference), making
them optimal for unsupervised and low-power tasks. These findings reinforce the
suitability of SNNs for energy-constrained, latency-sensitive, and adaptive
applications such as robotics, neuromorphic vision, and edge AI systems. While
promising, challenges persist in hardware standardization and scalable
training. This study concludes that SNNs, with further refinement, are poised
to propel the next phase of neuromorphic computing.",http://arxiv.org/pdf/2510.27379v1,10.14445/22315381/IJETT-V73I10P104,False
Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A Small-Data Pipeline with LoRA and Wan2.1 I2V,31/10/2025,"Meftun Akarsu, Kerem Catay, Sedat Bin Vedat, Enes Kutay Yarkan, Ilke Senturk, Arda Sar, Dafne Eksioglu","We present a practical pipeline for fine-tuning open-source video diffusion
transformers to synthesize cinematic scenes for television and film production
from small datasets. The proposed two-stage process decouples visual style
learning from motion generation. In the first stage, Low-Rank Adaptation (LoRA)
modules are integrated into the cross-attention layers of the Wan2.1 I2V-14B
model to adapt its visual representations using a compact dataset of short
clips from Ay Yapim's historical television film El Turco. This enables
efficient domain transfer within hours on a single GPU. In the second stage,
the fine-tuned model produces stylistically consistent keyframes that preserve
costume, lighting, and color grading, which are then temporally expanded into
coherent 720p sequences through the model's video decoder. We further apply
lightweight parallelization and sequence partitioning strategies to accelerate
inference without quality degradation. Quantitative and qualitative evaluations
using FVD, CLIP-SIM, and LPIPS metrics, supported by a small expert user study,
demonstrate measurable improvements in cinematic fidelity and temporal
stability over the base model. The complete training and inference pipeline is
released to support reproducibility and adaptation across cinematic domains.",http://arxiv.org/pdf/2510.27364v1,10.5281/zenodo.17370356,False
Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in Enterprise Environments,31/10/2025,"Harsh Vishwakarma, Ankush Agarwal, Ojas Patil, Chaitanya Devaguptapu, Mahesh Chandran","Enterprise systems are crucial for enhancing productivity and decision-making
among employees and customers. Integrating LLM based systems into enterprise
systems enables intelligent automation, personalized experiences, and efficient
information retrieval, driving operational efficiency and strategic growth.
However, developing and evaluating such systems is challenging due to the
inherent complexity of enterprise environments, where data is fragmented across
multiple sources and governed by sophisticated access controls. We present
EnterpriseBench, a comprehensive benchmark that simulates enterprise settings,
featuring 500 diverse tasks across software engineering, HR, finance, and
administrative domains. Our benchmark uniquely captures key enterprise
characteristics including data source fragmentation, access control
hierarchies, and cross-functional workflows. Additionally, we provide a novel
data generation pipeline that creates internally consistent enterprise tasks
from organizational metadata. Experiments with state-of-the-art LLM agents
demonstrate that even the most capable models achieve only 41.8% task
completion, highlighting significant opportunities for improvement in
enterprise-focused AI systems.",http://arxiv.org/pdf/2510.27287v1,,False
FMint-SDE: A Multimodal Foundation Model for Accelerating Numerical Simulation of SDEs via Error Correction,31/10/2025,"Jiaxin Yuan, Haizhao Yang, Maria Cameron","Fast and accurate simulation of dynamical systems is a fundamental challenge
across scientific and engineering domains. Traditional numerical integrators
often face a trade-off between accuracy and computational efficiency, while
existing neural network-based approaches typically require training a separate
model for each case. To overcome these limitations, we introduce a novel
multi-modal foundation model for large-scale simulations of differential
equations: FMint-SDE (Foundation Model based on Initialization for stochastic
differential equations). Based on a decoder-only transformer with in-context
learning, FMint-SDE leverages numerical and textual modalities to learn a
universal error-correction scheme. It is trained using prompted sequences of
coarse solutions generated by conventional solvers, enabling broad
generalization across diverse systems. We evaluate our models on a suite of
challenging SDE benchmarks spanning applications in molecular dynamics,
mechanical systems, finance, and biology. Experimental results show that our
approach achieves a superior accuracy-efficiency tradeoff compared to classical
solvers, underscoring the potential of FMint-SDE as a general-purpose
simulation tool for dynamical systems.",http://arxiv.org/pdf/2510.27173v1,,False
SERVIMON: AI-Driven Predictive Maintenance and Real-Time Monitoring for Astronomical Observatories,31/10/2025,"Emilio Mastriani, Alessandro Costa, Federico Incardona, Kevin Munari, Sebastiano Spinello","Objective: ServiMon is designed to offer a scalable and intelligent pipeline
for data collection and auditing to monitor distributed astronomical systems
such as the ASTRI Mini-Array. The system enhances quality control, predictive
maintenance, and real-time anomaly detection for telescope operations. Methods:
ServiMon integrates cloud-native technologies-including Prometheus, Grafana,
Cassandra, Kafka, and InfluxDB-for telemetry collection and processing. It
employs machine learning algorithms, notably Isolation Forest, to detect
anomalies in Cassandra performance metrics. Key indicators such as read/write
latency, throughput, and memory usage are continuously monitored, stored as
time-series data, and preprocessed for feature engineering. Anomalies detected
by the model are logged in InfluxDB v2 and accessed via Flux for real-time
monitoring and visualization. Results: AI-based anomaly detection increases
system resilience by identifying performance degradation at an early stage,
minimizing downtime, and optimizing telescope operations. Additionally,
ServiMon supports astrostatistical analysis by correlating telemetry with
observational data, thus enhancing scientific data quality. AI-generated alerts
also improve real-time monitoring, enabling proactive system management.
Conclusion: ServiMon's scalable framework proves effective for predictive
maintenance and real-time monitoring of astronomical infrastructures. By
leveraging cloud and edge computing, it is adaptable to future large-scale
experiments, optimizing both performance and cost. The combination of machine
learning and big data analytics makes ServiMon a robust and flexible solution
for modern and next-generation observational astronomy.",http://arxiv.org/pdf/2510.27146v1,,False
QiNN-QJ: A Quantum-inspired Neural Network with Quantum Jump for Multimodal Sentiment Analysis,31/10/2025,"Yiwei Chen, Kehuan Yan, Yu Pan, Daoyi Dong","Quantum theory provides non-classical principles, such as superposition and
entanglement, that inspires promising paradigms in machine learning. However,
most existing quantum-inspired fusion models rely solely on unitary or
unitary-like transformations to generate quantum entanglement. While
theoretically expressive, such approaches often suffer from training
instability and limited generalizability. In this work, we propose a
Quantum-inspired Neural Network with Quantum Jump (QiNN-QJ) for multimodal
entanglement modelling. Each modality is firstly encoded as a quantum pure
state, after which a differentiable module simulating the QJ operator
transforms the separable product state into the entangled representation. By
jointly learning Hamiltonian and Lindblad operators, QiNN-QJ generates
controllable cross-modal entanglement among modalities with dissipative
dynamics, where structured stochasticity and steady-state attractor properties
serve to stabilize training and constrain entanglement shaping. The resulting
entangled states are projected onto trainable measurement vectors to produce
predictions. In addition to achieving superior performance over the
state-of-the-art models on benchmark datasets, including CMU-MOSI, CMU-MOSEI,
and CH-SIMS, QiNN-QJ facilitates enhanced post-hoc interpretability through
von-Neumann entanglement entropy. This work establishes a principled framework
for entangled multimodal fusion and paves the way for quantum-inspired
approaches in modelling complex cross-modal correlations.",http://arxiv.org/pdf/2510.27091v1,,False
