Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
Nonlinear Schr√∂dinger Network,19/07/2024,"Yiming Zhou, Callen MacPhee, Tingyi Zhou, Bahram Jalali","Deep neural networks (DNNs) have achieved exceptional performance across
various fields by learning complex nonlinear mappings from large-scale
datasets. However, they encounter challenges such as high computational costs
and limited interpretability. To address these issues, hybrid approaches that
integrate physics with AI are gaining interest. This paper introduces a novel
physics-based AI model called the ""Nonlinear Schr\""odinger Network"", which
treats the Nonlinear Schr\""odinger Equation (NLSE) as a general-purpose
trainable model for learning complex patterns including nonlinear mappings and
memory effects from data. Existing physics-informed machine learning methods
use neural networks to approximate the solutions of partial differential
equations (PDEs). In contrast, our approach directly treats the PDE as a
trainable model to obtain general nonlinear mappings that would otherwise
require neural networks. As a physics-inspired approach, it offers a more
interpretable and parameter-efficient alternative to traditional black-box
neural networks, achieving comparable or better accuracy in time series
classification tasks while significantly reducing the number of required
parameters. Notably, the trained Nonlinear Schr\""odinger Network is
interpretable, with all parameters having physical meanings as properties of a
virtual physical system that transforms the data to a more separable space.
This interpretability allows for insight into the underlying dynamics of the
data transformation process. Applications to time series forecasting have also
been explored. While our current implementation utilizes the NLSE, the proposed
method of using physics equations as trainable models to learn nonlinear
mappings from data is not limited to the NLSE and may be extended to other
master equations of physics.",http://arxiv.org/pdf/2407.14504v1,,False
TTT: A Temporal Refinement Heuristic for Tenuously Tractable Discrete Time Reachability Problems,19/07/2024,"Chelsea Sidrane, Jana Tumova","Reachable set computation is an important tool for analyzing control systems.
Simulating a control system can show that the system is generally functioning
as desired, but a formal tool like reachability analysis can provide a
guarantee of correctness. For linear systems, reachability analysis is
straightforward and fast, but as more complex components are added to the
control system such as nonlinear dynamics or a neural network controller,
reachability analysis may slow down or become overly conservative. To address
these challenges, much literature has focused on spatial refinement, e.g.,
tuning the discretization of the input sets and intermediate reachable sets.
However, this paper addresses a different dimension: temporal refinement. The
basic idea of temporal refinement is to automatically choose when along the
horizon of the reachability problem to execute slow symbolic queries which
incur less approximation error versus fast concrete queries which incur more
approximation error. Temporal refinement can be combined with other refinement
approaches and offers an additional ``tuning knob'' with which to trade off
tractability and tightness in approximate reachable set computation. Here, we
introduce an automatic framework for performing temporal refinement and we
demonstrate the effectiveness of this technique on computing approximate
reachable sets for nonlinear systems with neural network control policies. We
demonstrate the calculation of reachable sets of varying approximation error
under varying computational budget and show that our algorithm is able to
generate approximate reachable sets with a similar amount of error to the
baseline approach in 20-70% less time.",http://arxiv.org/pdf/2407.14394v1,,False
GLAudio Listens to the Sound of the Graph,19/07/2024,"Aurelio Sulser, Johann Wenckstern, Clara Kuempel","We propose GLAudio: Graph Learning on Audio representation of the node
features and the connectivity structure. This novel architecture propagates the
node features through the graph network according to the discrete wave equation
and then employs a sequence learning architecture to learn the target node
function from the audio wave signal. This leads to a new paradigm of learning
on graph-structured data, in which information propagation and information
processing are separated into two distinct steps. We theoretically characterize
the expressivity of our model, introducing the notion of the receptive field of
a vertex, and investigate our model's susceptibility to over-smoothing and
over-squashing both theoretically as well as experimentally on various graph
datasets.",http://arxiv.org/pdf/2407.14387v1,,False
Hyperparameter Optimization for Driving Strategies Based on Reinforcement Learning,19/07/2024,"Nihal Acharya Adde, Hanno Gottschalk, Andreas Ebert","This paper focuses on hyperparameter optimization for autonomous driving
strategies based on Reinforcement Learning. We provide a detailed description
of training the RL agent in a simulation environment. Subsequently, we employ
Efficient Global Optimization algorithm that uses Gaussian Process fitting for
hyperparameter optimization in RL. Before this optimization phase, Gaussian
process interpolation is applied to fit the surrogate model, for which the
hyperparameter set is generated using Latin hypercube sampling. To accelerate
the evaluation, parallelization techniques are employed. Following the
hyperparameter optimization procedure, a set of hyperparameters is identified,
resulting in a noteworthy enhancement in overall driving performance. There is
a substantial increase of 4\% when compared to existing manually tuned
parameters and the hyperparameters discovered during the initialization process
using Latin hypercube sampling. After the optimization, we analyze the obtained
results thoroughly and conduct a sensitivity analysis to assess the robustness
and generalization capabilities of the learned autonomous driving strategies.
The findings from this study contribute to the advancement of Gaussian process
based Bayesian optimization to optimize the hyperparameters for autonomous
driving in RL, providing valuable insights for the development of efficient and
reliable autonomous driving systems.",http://arxiv.org/pdf/2407.14262v1,,False
Longhorn: State Space Models are Amortized Online Learners,19/07/2024,"Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, Qiang Liu","The most fundamental capability of modern AI methods such as Large Language
Models (LLMs) is the ability to predict the next token in a long sequence of
tokens, known as ``sequence modeling."" Although the Transformers model is the
current dominant approach to sequence modeling, its quadratic computational
cost with respect to sequence length is a significant drawback. State-space
models (SSMs) offer a promising alternative due to their linear decoding
efficiency and high parallelizability during training. However, existing SSMs
often rely on seemingly ad hoc linear recurrence designs. In this work, we
explore SSM design through the lens of online learning, conceptualizing SSMs as
meta-modules for specific online learning problems. This approach links SSM
design to formulating precise online learning objectives, with state transition
rules derived from optimizing these objectives. Based on this insight, we
introduce a novel deep SSM architecture based on the implicit update for
optimizing an online regression objective. Our experimental results show that
our models outperform state-of-the-art SSMs, including the Mamba model, on
standard sequence modeling benchmarks and language modeling tasks.",http://arxiv.org/pdf/2407.14207v1,,False
Enhancing Variable Importance in Random Forests: A Novel Application of Global Sensitivity Analysis,19/07/2024,"Giulia Vannucci, Roberta Siciliano, Andrea Saltelli","The present work provides an application of Global Sensitivity Analysis to
supervised machine learning methods such as Random Forests. These methods act
as black boxes, selecting features in high--dimensional data sets as to provide
accurate classifiers in terms of prediction when new data are fed into the
system. In supervised machine learning, predictors are generally ranked by
importance based on their contribution to the final prediction. Global
Sensitivity Analysis is primarily used in mathematical modelling to investigate
the effect of the uncertainties of the input variables on the output. We apply
it here as a novel way to rank the input features by their importance to the
explainability of the data generating process, shedding light on how the
response is determined by the dependence structure of its predictors. A
simulation study shows that our proposal can be used to explore what advances
can be achieved either in terms of efficiency, explanatory ability, or simply
by way of confirming existing results.",http://arxiv.org/pdf/2407.14194v1,,False
Machine learning emulation of precipitation from km-scale regional climate simulations using a diffusion model,19/07/2024,"Henry Addison, Elizabeth Kendon, Suman Ravuri, Laurence Aitchison, Peter AG Watson","High-resolution climate simulations are very valuable for understanding
climate change impacts and planning adaptation measures. This has motivated use
of regional climate models at sufficiently fine resolution to capture important
small-scale atmospheric processes, such as convective storms. However, these
regional models have very high computational costs, limiting their
applicability. We present CPMGEM, a novel application of a generative machine
learning model, a diffusion model, to skilfully emulate precipitation
simulations from such a high-resolution model over England and Wales at much
lower cost. This emulator enables stochastic generation of high-resolution
(8.8km), daily-mean precipitation samples conditioned on coarse-resolution
(60km) weather states from a global climate model. The output is fine enough
for use in applications such as flood inundation modelling. The emulator
produces precipitation predictions with realistic intensities and spatial
structures and captures most of the 21st century climate change signal. We show
evidence that the emulator has skill for extreme events up to and including
1-in-100 year intensities. Potential applications include producing
high-resolution precipitation predictions for large-ensemble climate
simulations and downscaling different climate models and climate change
scenarios to better sample uncertainty in climate changes at local-scale.",http://arxiv.org/pdf/2407.14158v1,,False
ParamsDrag: Interactive Parameter Space Exploration via Image-Space Dragging,19/07/2024,"Guan Li, Yang Liu, Guihua Shan, Shiyu Cheng, Weiqun Cao, Junpeng Wang, Ko-Chih Wang","Numerical simulation serves as a cornerstone in scientific modeling, yet the
process of fine-tuning simulation parameters poses significant challenges.
Conventionally, parameter adjustment relies on extensive numerical simulations,
data analysis, and expert insights, resulting in substantial computational
costs and low efficiency. The emergence of deep learning in recent years has
provided promising avenues for more efficient exploration of parameter spaces.
However, existing approaches often lack intuitive methods for precise parameter
adjustment and optimization. To tackle these challenges, we introduce
ParamsDrag, a model that facilitates parameter space exploration through direct
interaction with visualizations. Inspired by DragGAN, our ParamsDrag model
operates in three steps. First, the generative component of ParamsDrag
generates visualizations based on the input simulation parameters. Second, by
directly dragging structure-related features in the visualizations, users can
intuitively understand the controlling effect of different parameters. Third,
with the understanding from the earlier step, users can steer ParamsDrag to
produce dynamic visual outcomes. Through experiments conducted on real-world
simulations and comparisons with state-of-the-art deep learning-based
approaches, we demonstrate the efficacy of our solution.",http://arxiv.org/pdf/2407.14100v1,,False
Time Series Generative Learning with Application to Brain Imaging Analysis,19/07/2024,"Zhenghao Li, Sanyou Wu, Long Feng","This paper focuses on the analysis of sequential image data, particularly
brain imaging data such as MRI, fMRI, CT, with the motivation of understanding
the brain aging process and neurodegenerative diseases. To achieve this goal,
we investigate image generation in a time series context. Specifically, we
formulate a min-max problem derived from the $f$-divergence between neighboring
pairs to learn a time series generator in a nonparametric manner. The generator
enables us to generate future images by transforming prior lag-k observations
and a random vector from a reference distribution. With a deep neural network
learned generator, we prove that the joint distribution of the generated
sequence converges to the latent truth under a Markov and a conditional
invariance condition. Furthermore, we extend our generation mechanism to a
panel data scenario to accommodate multiple samples. The effectiveness of our
mechanism is evaluated by generating real brain MRI sequences from the
Alzheimer's Disease Neuroimaging Initiative. These generated image sequences
can be used as data augmentation to enhance the performance of further
downstream tasks, such as Alzheimer's disease detection.",http://arxiv.org/pdf/2407.14003v1,,False
Double Gradient Reversal Network for Single-Source Domain Generalization in Multi-mode Fault Diagnosis,19/07/2024,"Guangqiang Li, M. Amine Atoui, Xiangshun Li","Domain generalization achieves fault diagnosis on unseen modes. In process
industrial systems, fault samples are limited, and only single-mode fault data
can be obtained. Extracting domain-invariant fault features from single-mode
data for unseen mode fault diagnosis poses challenges. Existing methods utilize
a generator module to simulate samples of unseen modes. However, multi-mode
samples contain complex spatiotemporal information, which brings significant
difficulties to accurate sample generation. Therefore, double gradient reversal
network (DGRN) is proposed. First, the model is pre-trained to acquire fault
knowledge from the single seen mode. Then, pseudo-fault feature generation
strategy is designed by Adaptive instance normalization, to simulate fault
features of unseen mode. The dual adversarial training strategy is created to
enhance the diversity of pseudo-fault features, which models unseen modes with
significant distribution differences. Subsequently, domain-invariant feature
extraction strategy is constructed by contrastive learning and adversarial
learning. This strategy extracts common features of faults and helps multi-mode
fault diagnosis. Finally, the experiments were conducted on Tennessee Eastman
process and continuous stirred-tank reactor. The experiments demonstrate that
DGRN achieves high classification accuracy on unseen modes while maintaining a
small model size.",http://arxiv.org/pdf/2407.13978v1,,False
"A Unified Confidence Sequence for Generalized Linear Models, with Applications to Bandits",19/07/2024,"Junghyun Lee, Se-Young Yun, Kwang-Sung Jun","We present a unified likelihood ratio-based confidence sequence (CS) for any
(self-concordant) generalized linear models (GLMs) that is guaranteed to be
convex and numerically tight. We show that this is on par or improves upon
known CSs for various GLMs, including Gaussian, Bernoulli, and Poisson. In
particular, for the first time, our CS for Bernoulli has a poly(S)-free radius
where S is the norm of the unknown parameter. Our first technical novelty is
its derivation, which utilizes a time-uniform PAC-Bayesian bound with a uniform
prior/posterior, despite the latter being a rather unpopular choice for
deriving CSs. As a direct application of our new CS, we propose a simple and
natural optimistic algorithm called OFUGLB applicable to any generalized linear
bandits (GLB; Filippi et al. (2010)). Our analysis shows that the celebrated
optimistic approach simultaneously attains state-of-the-art regrets for various
self-concordant (not necessarily bounded) GLBs, and even poly(S)-free for
bounded GLBs, including logistic bandits. The regret analysis, our second
technical novelty, follows from combining our new CS with a new proof technique
that completely avoids the previously widely used self-concordant control lemma
(Faury et al., 2020, Lemma 9). Finally, we verify numerically that OFUGLB
significantly outperforms the prior state-of-the-art (Lee et al., 2024) for
logistic bandits.",http://arxiv.org/pdf/2407.13977v1,,False
Dimension-reduced Reconstruction Map Learning for Parameter Estimation in Likelihood-Free Inference Problems,19/07/2024,"Rui Zhang, Oksana A. Chkrebtii, Dongbin Xiu","Many application areas rely on models that can be readily simulated but lack
a closed-form likelihood, or an accurate approximation under arbitrary
parameter values. Existing parameter estimation approaches in this setting are
generally approximate. Recent work on using neural network models to
reconstruct the mapping from the data space to the parameters from a set of
synthetic parameter-data pairs suffers from the curse of dimensionality,
resulting in inaccurate estimation as the data size grows. We propose a
dimension-reduced approach to likelihood-free estimation which combines the
ideas of reconstruction map estimation with dimension-reduction approaches
based on subject-specific knowledge. We examine the properties of
reconstruction map estimation with and without dimension reduction and explore
the trade-off between approximation error due to information loss from reducing
the data dimension and approximation error. Numerical examples show that the
proposed approach compares favorably with reconstruction map estimation,
approximate Bayesian computation, and synthetic likelihood estimation.",http://arxiv.org/pdf/2407.13971v1,,False
