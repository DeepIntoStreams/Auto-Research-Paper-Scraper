Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
Unbounded: A Generative Infinite Game of Character Life Simulation,24/10/2024,"Jialu Li, Yuanzhen Li, Neal Wadhwa, Yael Pritch, David E. Jacobs, Michael Rubinstein, Mohit Bansal, Nataniel Ruiz","We introduce the concept of a generative infinite game, a video game that
transcends the traditional boundaries of finite, hard-coded systems by using
generative models. Inspired by James P. Carse's distinction between finite and
infinite games, we leverage recent advances in generative AI to create
Unbounded: a game of character life simulation that is fully encapsulated in
generative models. Specifically, Unbounded draws inspiration from sandbox life
simulations and allows you to interact with your autonomous virtual character
in a virtual world by feeding, playing with and guiding it - with open-ended
mechanics generated by an LLM, some of which can be emergent. In order to
develop Unbounded, we propose technical innovations in both the LLM and visual
generation domains. Specifically, we present: (1) a specialized, distilled
large language model (LLM) that dynamically generates game mechanics,
narratives, and character interactions in real-time, and (2) a new dynamic
regional image prompt Adapter (IP-Adapter) for vision models that ensures
consistent yet flexible visual generation of a character across multiple
environments. We evaluate our system through both qualitative and quantitative
analysis, showing significant improvements in character life simulation, user
instruction following, narrative coherence, and visual consistency for both
characters and the environments compared to traditional related approaches.",http://arxiv.org/pdf/2410.18975v1,,False
Schema-Guided Culture-Aware Complex Event Simulation with Multi-Agent Role-Play,24/10/2024,"Sha Li, Revanth Gangi Reddy, Khanh Duy Nguyen, Qingyun Wang, May Fung, Chi Han, Jiawei Han, Kartik Natarajan, Clare R. Voss, Heng Ji","Complex news events, such as natural disasters and socio-political conflicts,
require swift responses from the government and society. Relying on historical
events to project the future is insufficient as such events are sparse and do
not cover all possible conditions and nuanced situations. Simulation of these
complex events can help better prepare and reduce the negative impact. We
develop a controllable complex news event simulator guided by both the event
schema representing domain knowledge about the scenario and user-provided
assumptions representing case-specific conditions. As event dynamics depend on
the fine-grained social and cultural context, we further introduce a
geo-diverse commonsense and cultural norm-aware knowledge enhancement
component. To enhance the coherence of the simulation, apart from the global
timeline of events, we take an agent-based approach to simulate the individual
character states, plans, and actions. By incorporating the schema and cultural
norms, our generated simulations achieve much higher coherence and
appropriateness and are received favorably by participants from a humanitarian
assistance organization.",http://arxiv.org/pdf/2410.18935v1,,False
ANAVI: Audio Noise Awareness using Visuals of Indoor environments for NAVIgation,24/10/2024,"Vidhi Jain, Rishi Veerapaneni, Yonatan Bisk","We propose Audio Noise Awareness using Visuals of Indoors for NAVIgation for
quieter robot path planning. While humans are naturally aware of the noise they
make and its impact on those around them, robots currently lack this awareness.
A key challenge in achieving audio awareness for robots is estimating how loud
will the robot's actions be at a listener's location? Since sound depends upon
the geometry and material composition of rooms, we train the robot to passively
perceive loudness using visual observations of indoor environments. To this
end, we generate data on how loud an 'impulse' sounds at different listener
locations in simulated homes, and train our Acoustic Noise Predictor (ANP).
Next, we collect acoustic profiles corresponding to different actions for
navigation. Unifying ANP with action acoustics, we demonstrate experiments with
wheeled (Hello Robot Stretch) and legged (Unitree Go2) robots so that these
robots adhere to the noise constraints of the environment. See code and data at
https://anavi-corl24.github.io/",http://arxiv.org/pdf/2410.18932v1,,False
SkillMimicGen: Automated Demonstration Generation for Efficient Skill Learning and Deployment,24/10/2024,"Caelan Garrett, Ajay Mandlekar, Bowen Wen, Dieter Fox","Imitation learning from human demonstrations is an effective paradigm for
robot manipulation, but acquiring large datasets is costly and
resource-intensive, especially for long-horizon tasks. To address this issue,
we propose SkillMimicGen (SkillGen), an automated system for generating
demonstration datasets from a few human demos. SkillGen segments human demos
into manipulation skills, adapts these skills to new contexts, and stitches
them together through free-space transit and transfer motion. We also propose a
Hybrid Skill Policy (HSP) framework for learning skill initiation, control, and
termination components from SkillGen datasets, enabling skills to be sequenced
using motion planning at test-time. We demonstrate that SkillGen greatly
improves data generation and policy learning performance over a
state-of-the-art data generation framework, resulting in the capability to
produce data for large scene variations, including clutter, and agents that are
on average 24% more successful. We demonstrate the efficacy of SkillGen by
generating over 24K demonstrations across 18 task variants in simulation from
just 60 human demonstrations, and training proficient, often near-perfect, HSP
agents. Finally, we apply SkillGen to 3 real-world manipulation tasks and also
demonstrate zero-shot sim-to-real transfer on a long-horizon assembly task.
Videos, and more at https://skillgen.github.io.",http://arxiv.org/pdf/2410.18907v1,,False
Generation of synthetic financial time series by diffusion models,24/10/2024,"Tomonori Takahashi, Takayuki Mizuno","Despite its practical significance, generating realistic synthetic financial
time series is challenging due to statistical properties known as stylized
facts, such as fat tails, volatility clustering, and seasonality patterns.
Various generative models, including generative adversarial networks (GANs) and
variational autoencoders (VAEs), have been employed to address this challenge,
although no model yet satisfies all the stylized facts. We alternatively
propose utilizing diffusion models, specifically denoising diffusion
probabilistic models (DDPMs), to generate synthetic financial time series. This
approach employs wavelet transformation to convert multiple time series (into
images), such as stock prices, trading volumes, and spreads. Given these
converted images, the model gains the ability to generate images that can be
transformed back into realistic time series by inverse wavelet transformation.
We demonstrate that our proposed approach satisfies stylized facts.",http://arxiv.org/pdf/2410.18897v1,,False
Bilinear Sequence Regression: A Model for Learning from Long Sequences of High-dimensional Tokens,24/10/2024,"Vittorio Erba, Emanuele Troiani, Luca Biggio, Antoine Maillard, Lenka Zdeborov√°","Current progress in artificial intelligence is centered around so-called
large language models that consist of neural networks processing long sequences
of high-dimensional vectors called tokens. Statistical physics provides
powerful tools to study the functioning of learning with neural networks and
has played a recognized role in the development of modern machine learning. The
statistical physics approach relies on simplified and analytically tractable
models of data. However, simple tractable models for long sequences of
high-dimensional tokens are largely underexplored. Inspired by the crucial role
models such as the single-layer teacher-student perceptron (aka generalized
linear regression) played in the theory of fully connected neural networks, in
this paper, we introduce and study the bilinear sequence regression (BSR) as
one of the most basic models for sequences of tokens. We note that modern
architectures naturally subsume the BSR model due to the skip connections.
Building on recent methodological progress, we compute the Bayes-optimal
generalization error for the model in the limit of long sequences of
high-dimensional tokens, and provide a message-passing algorithm that matches
this performance. We quantify the improvement that optimal learning brings with
respect to vectorizing the sequence of tokens and learning via simple linear
regression. We also unveil surprising properties of the gradient descent
algorithms in the BSR model.",http://arxiv.org/pdf/2410.18858v1,,False
Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced Time Series Classification,24/10/2024,"Xiaoyu Tao, Tingyue Pan, Mingyue Cheng, Yucong Luo","Leveraging large language models (LLMs) has garnered increasing attention and
introduced novel perspectives in time series classification. However, existing
approaches often overlook the crucial dynamic temporal information inherent in
time series data and face challenges in aligning this data with textual
semantics. To address these limitations, we propose HiTime, a hierarchical
multi-modal model that seamlessly integrates temporal information into LLMs for
multivariate time series classification (MTSC). Our model employs a
hierarchical feature encoder to capture diverse aspects of time series data
through both data-specific and task-specific embeddings. To facilitate semantic
space alignment between time series and text, we introduce a dual-view
contrastive alignment module that bridges the gap between modalities.
Additionally, we adopt a hybrid prompting strategy to fine-tune the pre-trained
LLM in a parameter-efficient manner. By effectively incorporating dynamic
temporal features and ensuring semantic alignment, HiTime enables LLMs to
process continuous time series data and achieves state-of-the-art
classification performance through text generation. Extensive experiments on
benchmark datasets demonstrate that HiTime significantly enhances time series
classification accuracy compared to most competitive baseline methods. Our
findings highlight the potential of integrating temporal features into LLMs,
paving the way for advanced time series analysis. The code is publicly
available for further research and validation. Our codes are publicly
available1.",http://arxiv.org/pdf/2410.18686v1,,False
Multi-agent cooperation through learning-aware policy gradients,24/10/2024,"Alexander Meulemans, Seijin Kobayashi, Johannes von Oswald, Nino Scherrer, Eric Elmoznino, Blake Richards, Guillaume Lajoie, Blaise Ag√ºera y Arcas, Jo√£o Sacramento","Self-interested individuals often fail to cooperate, posing a fundamental
challenge for multi-agent learning. How can we achieve cooperation among
self-interested, independent learning agents? Promising recent work has shown
that in certain tasks cooperation can be established between learning-aware
agents who model the learning dynamics of each other. Here, we present the
first unbiased, higher-derivative-free policy gradient algorithm for
learning-aware reinforcement learning, which takes into account that other
agents are themselves learning through trial and error based on multiple noisy
trials. We then leverage efficient sequence models to condition behavior on
long observation histories that contain traces of the learning dynamics of
other agents. Training long-context policies with our algorithm leads to
cooperative behavior and high returns on standard social dilemmas, including a
challenging environment where temporally-extended action coordination is
required. Finally, we derive from the iterated prisoner's dilemma a novel
explanation for how and when cooperation arises among self-interested
learning-aware agents.",http://arxiv.org/pdf/2410.18636v1,,False
LOGO -- Long cOntext aliGnment via efficient preference Optimization,24/10/2024,"Zecheng Tang, Zechen Sun, Juntao Li, Qiaoming Zhu, Min Zhang","Long-context models(LCMs) have shown great potential in processing long input
sequences(even more than 100M tokens) conveniently and effectively. With
significant progress, recent research has pointed out that LCMs can accurately
locate token-level salient information within the context. Yet, the generation
performance of these LCMs is far from satisfactory and might result in
misaligned responses, such as hallucinations. To enhance the generation
capability of LCMs, existing works have investigated the effects of data size
and quality for both pre-training and instruction tuning. Though achieving
meaningful improvement, previous methods fall short in either effectiveness or
efficiency. In this paper, we introduce LOGO(Long cOntext aliGnment via
efficient preference Optimization), a training strategy that first introduces
preference optimization for long-context alignment. To overcome the GPU
memory-bound issue caused by the long sequence, LOGO employs a reference-free
preference optimization strategy and adopts a position synthesis method to
construct the training data. By training with only 0.3B data on a single
8$\times$A800 GPU machine for 16 hours, LOGO allows the Llama-3-8B-Instruct-80K
model to achieve comparable performance with GPT-4 in real-world long-context
tasks while preserving the model's original capabilities on other tasks, e.g.,
language modeling and MMLU. Moreover, LOGO can extend the model's context
window size while enhancing its generation performance.",http://arxiv.org/pdf/2410.18533v1,,False
Gene-Metabolite Association Prediction with Interactive Knowledge Transfer Enhanced Graph for Metabolite Production,24/10/2024,"Kexuan Xin, Qingyun Wang, Junyu Chen, Pengfei Yu, Huimin Zhao, Heng Ji","In the rapidly evolving field of metabolic engineering, the quest for
efficient and precise gene target identification for metabolite production
enhancement presents significant challenges. Traditional approaches, whether
knowledge-based or model-based, are notably time-consuming and labor-intensive,
due to the vast scale of research literature and the approximation nature of
genome-scale metabolic model (GEM) simulations. Therefore, we propose a new
task, Gene-Metabolite Association Prediction based on metabolic graphs, to
automate the process of candidate gene discovery for a given pair of metabolite
and candidate-associated genes, as well as presenting the first benchmark
containing 2474 metabolites and 1947 genes of two commonly used microorganisms
Saccharomyces cerevisiae (SC) and Issatchenkia orientalis (IO). This task is
challenging due to the incompleteness of the metabolic graphs and the
heterogeneity among distinct metabolisms. To overcome these limitations, we
propose an Interactive Knowledge Transfer mechanism based on Metabolism Graph
(IKT4Meta), which improves the association prediction accuracy by integrating
the knowledge from different metabolism graphs. First, to build a bridge
between two graphs for knowledge transfer, we utilize Pretrained Language
Models (PLMs) with external knowledge of genes and metabolites to help generate
inter-graph links, significantly alleviating the impact of heterogeneity.
Second, we propagate intra-graph links from different metabolic graphs using
inter-graph links as anchors. Finally, we conduct the gene-metabolite
association prediction based on the enriched metabolism graphs, which integrate
the knowledge from multiple microorganisms. Experiments on both types of
organisms demonstrate that our proposed methodology outperforms baselines by up
to 12.3% across various link prediction frameworks.",http://arxiv.org/pdf/2410.18475v1,,False
Structure Language Models for Protein Conformation Generation,24/10/2024,"Jiarui Lu, Xiaoyin Chen, Stephen Zhewen Lu, Chence Shi, Hongyu Guo, Yoshua Bengio, Jian Tang","Proteins adopt multiple structural conformations to perform their diverse
biological functions, and understanding these conformations is crucial for
advancing drug discovery. Traditional physics-based simulation methods often
struggle with sampling equilibrium conformations and are computationally
expensive. Recently, deep generative models have shown promise in generating
protein conformations as a more efficient alternative. However, these methods
predominantly rely on the diffusion process within a 3D geometric space, which
typically centers around the vicinity of metastable states and is often
inefficient in terms of runtime. In this paper, we introduce Structure Language
Modeling (SLM) as a novel framework for efficient protein conformation
generation. Specifically, the protein structures are first encoded into a
compact latent space using a discrete variational auto-encoder, followed by
conditional language modeling that effectively captures sequence-specific
conformation distributions. This enables a more efficient and interpretable
exploration of diverse ensemble modes compared to existing methods. Based on
this general framework, we instantiate SLM with various popular LM
architectures as well as proposing the ESMDiff, a novel BERT-like structure
language model fine-tuned from ESM3 with masked diffusion. We verify our
approach in various scenarios, including the equilibrium dynamics of BPTI,
conformational change pairs, and intrinsically disordered proteins. SLM
provides a highly efficient solution, offering a 20-100x speedup than existing
methods in generating diverse conformations, shedding light on promising
avenues for future research.",http://arxiv.org/pdf/2410.18403v1,,False
Contextual Biasing to Improve Domain-specific Custom Vocabulary Audio Transcription without Explicit Fine-Tuning of Whisper Model,24/10/2024,"Vishakha Lall, Yisi Liu","OpenAI's Whisper Automated Speech Recognition model excels in generalizing
across diverse datasets and domains. However, this broad adaptability can lead
to diminished performance in tasks requiring recognition of specific
vocabularies. Addressing this challenge typically involves fine-tuning the
model, which demands extensive labeled audio data that is often difficult to
acquire and unavailable for specific domains. In this study, we propose a
method to enhance transcription accuracy without explicit fine-tuning or
altering model parameters, using a relatively small training dataset. Our
method leverages contextual biasing, to direct Whisper model's output towards a
specific vocabulary by integrating a neural-symbolic prefix tree structure to
guide the model's transcription output. To validate our approach, we conducted
experiments using a validation dataset comprising maritime data collected
within a simulated training environment. A comparison between the original
Whisper models of varying parameter sizes and our biased model revealed a
notable reduction in transcription word error rate and enhanced performance of
downstream applications. Our findings suggest that this methodology holds
promise for improving speech-to-text translation performance in domains
characterized by limited vocabularies.",http://arxiv.org/pdf/2410.18363v1,,False
