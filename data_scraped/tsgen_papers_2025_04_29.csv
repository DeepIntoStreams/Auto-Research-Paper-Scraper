Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
Modelling of Underwater Vehicles using Physics-Informed Neural Networks with Control,28/04/2025,"Abdelhakim Amer, David Felsager, Yury Brodskiy, Andriy Sarabakha","Physics-informed neural networks (PINNs) integrate physical laws with
data-driven models to improve generalization and sample efficiency. This work
introduces an open-source implementation of the Physics-Informed Neural Network
with Control (PINC) framework, designed to model the dynamics of an underwater
vehicle. Using initial states, control actions, and time inputs, PINC extends
PINNs to enable physically consistent transitions beyond the training domain.
Various PINC configurations are tested, including differing loss functions,
gradient-weighting schemes, and hyperparameters. Validation on a simulated
underwater vehicle demonstrates more accurate long-horizon predictions compared
to a non-physics-informed baseline",http://arxiv.org/pdf/2504.20019v1,,False
Automated decision-making for dynamic task assignment at scale,28/04/2025,"Riccardo Lo Bianco, Willem van Jaarsveld, Jeroen Middelhuis, Luca Begnardi, Remco Dijkman","The Dynamic Task Assignment Problem (DTAP) concerns matching resources to
tasks in real time while minimizing some objectives, like resource costs or
task cycle time. In this work, we consider a DTAP variant where every task is a
case composed of a stochastic sequence of activities. The DTAP, in this case,
involves the decision of which employee to assign to which activity to process
requests as quickly as possible. In recent years, Deep Reinforcement Learning
(DRL) has emerged as a promising tool for tackling this DTAP variant, but most
research is limited to solving small-scale, synthetic problems, neglecting the
challenges posed by real-world use cases. To bridge this gap, this work
proposes a DRL-based Decision Support System (DSS) for real-world scale DTAPS.
To this end, we introduce a DRL agent with two novel elements: a graph
structure for observations and actions that can effectively represent any DTAP
and a reward function that is provably equivalent to the objective of
minimizing the average cycle time of tasks. The combination of these two
novelties allows the agent to learn effective and generalizable assignment
policies for real-world scale DTAPs. The proposed DSS is evaluated on five DTAP
instances whose parameters are extracted from real-world logs through process
mining. The experimental evaluation shows how the proposed DRL agent matches or
outperforms the best baseline in all DTAP instances and generalizes on
different time horizons and across instances.",http://arxiv.org/pdf/2504.19933v1,,False
NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks,28/04/2025,"Chia-Yu Hung, Qi Sun, Pengfei Hong, Amir Zadeh, Chuan Li, U-Xuan Tan, Navonil Majumder, Soujanya Poria","Existing Visual-Language-Action (VLA) models have shown promising performance
in zero-shot scenarios, demonstrating impressive task execution and reasoning
capabilities. However, a significant challenge arises from the limitations of
visual encoding, which can result in failures during tasks such as object
grasping. Moreover, these models typically suffer from high computational
overhead due to their large sizes, often exceeding 7B parameters. While these
models excel in reasoning and task planning, the substantial computational
overhead they incur makes them impractical for real-time robotic environments,
where speed and efficiency are paramount. To address the limitations of
existing VLA models, we propose NORA, a 3B-parameter model designed to reduce
computational overhead while maintaining strong task performance. NORA adopts
the Qwen-2.5-VL-3B multimodal model as its backbone, leveraging its superior
visual-semantic understanding to enhance visual reasoning and action grounding.
Additionally, our \model{} is trained on 970k real-world robot demonstrations
and equipped with the FAST+ tokenizer for efficient action sequence generation.
Experimental results demonstrate that NORA outperforms existing large-scale VLA
models, achieving better task performance with significantly reduced
computational overhead, making it a more practical solution for real-time
robotic autonomy.",http://arxiv.org/pdf/2504.19854v1,,False
Model-based controller assisted domain randomization in deep reinforcement learning: application to nonlinear powertrain control,28/04/2025,"Heisei Yonezawa, Ansei Yonezawa, Itsuro Kajiwara","Complex mechanical systems such as vehicle powertrains are inherently subject
to multiple nonlinearities and uncertainties arising from parametric
variations. Modeling and calibration errors are therefore unavoidable, making
the transfer of control systems from simulation to real-world systems a
critical challenge. Traditional robust controls have limitations in handling
certain types of nonlinearities and uncertainties, requiring a more practical
approach capable of comprehensively compensating for these various constraints.
This study proposes a new robust control approach using the framework of deep
reinforcement learning (DRL). The key strategy lies in the synergy among domain
randomization-based DRL, long short-term memory (LSTM)-based actor and critic
networks, and model-based control (MBC). The problem setup is modeled via the
latent Markov decision process (LMDP), a set of vanilla MDPs, for a controlled
system subject to uncertainties and nonlinearities. In LMDP, the dynamics of an
environment simulator is randomized during training to improve the robustness
of the control system to real testing environments. The randomization increases
training difficulties as well as conservativeness of the resultant control
system; therefore, progress is assisted by concurrent use of a model-based
controller based on a nominal system model. Compared to traditional DRL-based
controls, the proposed controller design is smarter in that we can achieve a
high level of generalization ability with a more compact neural network
architecture and a smaller amount of training data. The proposed approach is
verified via practical application to active damping for a complex powertrain
system with nonlinearities and parametric variations. Comparative tests
demonstrate the high robustness of the proposed approach.",http://arxiv.org/pdf/2504.19715v1,,False
Rulebook: bringing co-routines to reinforcement learning environments,28/04/2025,"Massimo Fioravanti, Samuele Pasini, Giovanni Agosta","Reinforcement learning (RL) algorithms, due to their reliance on external
systems to learn from, require digital environments (e.g., simulators) with
very simple interfaces, which in turn constrain significantly the
implementation of such environments. In particular, these environments are
implemented either as separate processes or as state machines, leading to
synchronization and communication overheads in the first case, and to
unstructured programming in the second.
  We propose a new domain-specific, co-routine-based, compiled language, called
Rulebook, designed to automatically generate the state machine required to
interact with machine learning (ML) algorithms and similar applications, with
no performance overhead. Rulebook allows users to express programs without
needing to be aware of the specific interface required by the ML components. By
decoupling the execution model of the program from the syntactical encoding of
the program, and thus without the need for manual state management, Rulebook
allows to create larger and more sophisticated environments at a lower
development cost.",http://arxiv.org/pdf/2504.19625v1,,False
WILD: a new in-the-Wild Image Linkage Dataset for synthetic image attribution,28/04/2025,"Pietro Bongini, Sara Mandelli, Andrea Montibeller, Mirko Casu, Orazio Pontorno, Claudio Ragaglia, Luca Zanchetta, Mattia Aquilina, Taiba Majid Wani, Luca Guarnera, Benedetta Tondi, Paolo Bestagini, Irene Amerini, Francesco Denatale, Sebastiano Battiato, Mauro Barni","Synthetic image source attribution is an open challenge, with an increasing
number of image generators being released yearly. The complexity and the sheer
number of available generative techniques, as well as the scarcity of
high-quality open source datasets of diverse nature for this task, make
training and benchmarking synthetic image source attribution models very
challenging. WILD is a new in-the-Wild Image Linkage Dataset designed to
provide a powerful training and benchmarking tool for synthetic image
attribution models. The dataset is built out of a closed set of 10 popular
commercial generators, which constitutes the training base of attribution
models, and an open set of 10 additional generators, simulating a real-world
in-the-wild scenario. Each generator is represented by 1,000 images, for a
total of 10,000 images in the closed set and 10,000 images in the open set.
Half of the images are post-processed with a wide range of operators. WILD
allows benchmarking attribution models in a wide range of tasks, including
closed and open set identification and verification, and robust attribution
with respect to post-processing and adversarial attacks. Models trained on WILD
are expected to benefit from the challenging scenario represented by the
dataset itself. Moreover, an assessment of seven baseline methodologies on
closed and open set attribution is presented, including robustness tests with
respect to post-processing.",http://arxiv.org/pdf/2504.19595v1,,False
Quantifying Memory Utilization with Effective State-Size,28/04/2025,"Rom N. Parnichkun, Neehal Tumma, Armin W. Thomas, Alessandro Moro, Qi An, Taiji Suzuki, Atsushi Yamashita, Michael Poli, Stefano Massaroli","The need to develop a general framework for architecture analysis is becoming
increasingly important, given the expanding design space of sequence models. To
this end, we draw insights from classical signal processing and control theory,
to develop a quantitative measure of \textit{memory utilization}: the internal
mechanisms through which a model stores past information to produce future
outputs. This metric, which we call \textbf{\textit{effective state-size}}
(ESS), is tailored to the fundamental class of systems with
\textit{input-invariant} and \textit{input-varying linear operators},
encompassing a variety of computational units such as variants of attention,
convolutions, and recurrences. Unlike prior work on memory utilization, which
either relies on raw operator visualizations (e.g. attention maps), or simply
the total \textit{memory capacity} (i.e. cache size) of a model, our metrics
provide highly interpretable and actionable measurements. In particular, we
show how ESS can be leveraged to improve initialization strategies, inform
novel regularizers and advance the performance-efficiency frontier through
model distillation. Furthermore, we demonstrate that the effect of context
delimiters (such as end-of-speech tokens) on ESS highlights cross-architectural
differences in how large language models utilize their available memory to
recall information. Overall, we find that ESS provides valuable insights into
the dynamics that dictate memory utilization, enabling the design of more
efficient and effective sequence models.",http://arxiv.org/pdf/2504.19561v1,,False
An Automated Reinforcement Learning Reward Design Framework with Large Language Model for Cooperative Platoon Coordination,28/04/2025,"Dixiao Wei, Peng Yi, Jinlong Lei, Yiguang Hong, Yuchuan Du","Reinforcement Learning (RL) has demonstrated excellent decision-making
potential in platoon coordination problems. However, due to the variability of
coordination goals, the complexity of the decision problem, and the
time-consumption of trial-and-error in manual design, finding a well
performance reward function to guide RL training to solve complex platoon
coordination problems remains challenging. In this paper, we formally define
the Platoon Coordination Reward Design Problem (PCRDP), extending the RL-based
cooperative platoon coordination problem to incorporate automated reward
function generation. To address PCRDP, we propose a Large Language Model
(LLM)-based Platoon coordination Reward Design (PCRD) framework, which
systematically automates reward function discovery through LLM-driven
initialization and iterative optimization. In this method, LLM first
initializes reward functions based on environment code and task requirements
with an Analysis and Initial Reward (AIR) module, and then iteratively
optimizes them based on training feedback with an evolutionary module. The AIR
module guides LLM to deepen their understanding of code and tasks through a
chain of thought, effectively mitigating hallucination risks in code
generation. The evolutionary module fine-tunes and reconstructs the reward
function, achieving a balance between exploration diversity and convergence
stability for training. To validate our approach, we establish six challenging
coordination scenarios with varying complexity levels within the Yangtze River
Delta transportation network simulation. Comparative experimental results
demonstrate that RL agents utilizing PCRD-generated reward functions
consistently outperform human-engineered reward functions, achieving an average
of 10\% higher performance metrics in all scenarios.",http://arxiv.org/pdf/2504.19480v1,,False
Geometry-Informed Neural Operator Transformer,28/04/2025,"Qibang Liu, Vincient Zhong, Hadi Meidani, Diab Abueidda, Seid Koric, Philippe Geubelle","Machine-learning-based surrogate models offer significant computational
efficiency and faster simulations compared to traditional numerical methods,
especially for problems requiring repeated evaluations of partial differential
equations. This work introduces the Geometry-Informed Neural Operator
Transformer (GINOT), which integrates the transformer architecture with the
neural operator framework to enable forward predictions for arbitrary
geometries. GINOT encodes the surface points cloud of a geometry using a
sampling and grouping mechanism combined with an attention mechanism, ensuring
invariance to point order and padding while maintaining robustness to
variations in point density. The geometry information is seamlessly integrated
with query points in the solution decoder through the attention mechanism. The
performance of GINOT is validated on multiple challenging datasets, showcasing
its high accuracy and strong generalization capabilities for complex and
arbitrary 2D and 3D geometries.",http://arxiv.org/pdf/2504.19452v1,,False
UNet with Axial Transformer : A Neural Weather Model for Precipitation Nowcasting,28/04/2025,"Maitreya Sonawane, Sumit Mamtani","Making accurate weather predictions can be particularly challenging for
localized storms or events that evolve on hourly timescales, such as
thunderstorms. Hence, our goal for the project was to model Weather Nowcasting
for making highly localized and accurate predictions that apply to the
immediate future replacing the current numerical weather models and data
assimilation systems with Deep Learning approaches. A significant advantage of
machine learning is that inference is computationally cheap given an
already-trained model, allowing forecasts that are nearly instantaneous and in
the native high resolution of the input data. In this work we developed a novel
method that employs Transformer-based machine learning models to forecast
precipitation. This approach works by leveraging axial attention mechanisms to
learn complex patterns and dynamics from time series frames. Moreover, it is a
generic framework and can be applied to univariate and multivariate time series
data, as well as time series embeddings data. This paper represents an initial
research on the dataset used in the domain of next frame prediciton, and hence,
we demonstrate state-of-the-art results in terms of metrices (PSNR = 47.67,
SSIM = 0.9943) used for the given dataset using UNet with Axial Transformer.",http://arxiv.org/pdf/2504.19408v1,,False
