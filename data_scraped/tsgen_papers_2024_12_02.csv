Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
SIMS: Simulating Human-Scene Interactions with Real World Script Planning,29/11/2024,"Wenjia Wang, Liang Pan, Zhiyang Dou, Zhouyingcheng Liao, Yuke Lou, Lei Yang, Jingbo Wang, Taku Komura","Simulating long-term human-scene interaction is a challenging yet fascinating
task. Previous works have not effectively addressed the generation of long-term
human scene interactions with detailed narratives for physics-based animation.
This paper introduces a novel framework for the planning and controlling of
long-horizon physical plausible human-scene interaction. On the one hand, films
and shows with stylish human locomotions or interactions with scenes are
abundantly available on the internet, providing a rich source of data for
script planning. On the other hand, Large Language Models (LLMs) can understand
and generate logical storylines.
  This motivates us to marry the two by using an LLM-based pipeline to extract
scripts from videos, and then employ LLMs to imitate and create new scripts,
capturing complex, time-series human behaviors and interactions with
environments. By leveraging this, we utilize a dual-aware policy that achieves
both language comprehension and scene understanding to guide character motions
within contextual and spatial constraints. To facilitate training and
evaluation, we contribute a comprehensive planning dataset containing diverse
motion sequences extracted from real-world videos and expand them with large
language models. We also collect and re-annotate motion clips from existing
kinematic datasets to enable our policy learn diverse skills. Extensive
experiments demonstrate the effectiveness of our framework in versatile task
execution and its generalization ability to various scenarios, showing
remarkably enhanced performance compared with existing methods. Our code and
data will be publicly available soon.",http://arxiv.org/pdf/2411.19921v1,,False
Quantifying the synthetic and real domain gap in aerial scene understanding,29/11/2024,Alina Marcu,"Quantifying the gap between synthetic and real-world imagery is essential for
improving both transformer-based models - that rely on large volumes of data -
and datasets, especially in underexplored domains like aerial scene
understanding where the potential impact is significant. This paper introduces
a novel methodology for scene complexity assessment using Multi-Model Consensus
Metric (MMCM) and depth-based structural metrics, enabling a robust evaluation
of perceptual and structural disparities between domains. Our experimental
analysis, utilizing real-world (Dronescapes) and synthetic (Skyscenes)
datasets, demonstrates that real-world scenes generally exhibit higher
consensus among state-of-the-art vision transformers, while synthetic scenes
show greater variability and challenge model adaptability. The results
underline the inherent complexities and domain gaps, emphasizing the need for
enhanced simulation fidelity and model generalization. This work provides
critical insights into the interplay between domain characteristics and model
performance, offering a pathway for improved domain adaptation strategies in
aerial scene understanding.",http://arxiv.org/pdf/2411.19913v1,,False
Efficient quantum-enhanced classical simulation for patches of quantum landscapes,29/11/2024,"Sacha Lerch, Ricard Puig, Manuel S. Rudolph, Armando Angrisani, Tyson Jones, M. Cerezo, Supanut Thanasilp, Zoë Holmes","Understanding the capabilities of classical simulation methods is key to
identifying where quantum computers are advantageous. Not only does this ensure
that quantum computers are used only where necessary, but also one can
potentially identify subroutines that can be offloaded onto a classical device.
In this work, we show that it is always possible to generate a classical
surrogate of a sub-region (dubbed a ""patch"") of an expectation landscape
produced by a parameterized quantum circuit. That is, we provide a
quantum-enhanced classical algorithm which, after simple measurements on a
quantum device, allows one to classically simulate approximate expectation
values of a subregion of a landscape. We provide time and sample complexity
guarantees for a range of families of circuits of interest, and further
numerically demonstrate our simulation algorithms on an exactly verifiable
simulation of a Hamiltonian variational ansatz and long-time dynamics
simulation on a 127-qubit heavy-hex topology.",http://arxiv.org/pdf/2411.19896v1,,False
MoTe: Learning Motion-Text Diffusion Model for Multiple Generation Tasks,29/11/2024,"Yiming Wu, Wei Ji, Kecheng Zheng, Zicheng Wang, Dong Xu","Recently, human motion analysis has experienced great improvement due to
inspiring generative models such as the denoising diffusion model and large
language model. While the existing approaches mainly focus on generating
motions with textual descriptions and overlook the reciprocal task. In this
paper, we present~\textbf{MoTe}, a unified multi-modal model that could handle
diverse tasks by learning the marginal, conditional, and joint distributions of
motion and text simultaneously. MoTe enables us to handle the paired
text-motion generation, motion captioning, and text-driven motion generation by
simply modifying the input context. Specifically, MoTe is composed of three
components: Motion Encoder-Decoder (MED), Text Encoder-Decoder (TED), and
Moti-on-Text Diffusion Model (MTDM). In particular, MED and TED are trained for
extracting latent embeddings, and subsequently reconstructing the motion
sequences and textual descriptions from the extracted embeddings, respectively.
MTDM, on the other hand, performs an iterative denoising process on the input
context to handle diverse tasks. Experimental results on the benchmark datasets
demonstrate the superior performance of our proposed method on text-to-motion
generation and competitive performance on motion captioning.",http://arxiv.org/pdf/2411.19786v1,,False
Forecasting Foreign Exchange Market Prices Using Technical Indicators with Deep Learning and Attention Mechanism,29/11/2024,"Sahabeh Saadati, Mohammad Manthouri","Accurate prediction of price behavior in the foreign exchange market is
crucial. This paper proposes a novel approach that leverages technical
indicators and deep neural networks. The proposed architecture consists of a
Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN), and
attention mechanism. Initially, trend and oscillation technical indicators are
employed to extract statistical features from Forex currency pair data,
providing insights into price trends, market volatility, relative price
strength, and overbought and oversold conditions. Subsequently, the LSTM and
CNN networks are utilized in parallel to predict future price movements,
leveraging the strengths of both recurrent and convolutional architectures. The
LSTM network captures long-term dependencies and temporal patterns in the data,
while the CNN network extracts local patterns. The outputs of the parallel LSTM
and CNN networks are then fed into an attention mechanism, which learns to
weigh the importance of each feature and temporal dependency, generating a
context-aware representation of the input data. The attention-weighted output
is then used to predict future price movements, enabling the model to focus on
the most relevant features and temporal dependencies. Through a comprehensive
evaluation of the proposed approach on multiple Forex currency pairs, we
demonstrate its effectiveness in predicting price behavior and outperforming
benchmark models.",http://arxiv.org/pdf/2411.19763v1,,False
Improving generalization of robot locomotion policies via Sharpness-Aware Reinforcement Learning,29/11/2024,"Severin Bochem, Eduardo Gonzalez-Sanchez, Yves Bicker, Gabriele Fadini","Reinforcement learning often requires extensive training data.
Simulation-to-real transfer offers a promising approach to address this
challenge in robotics. While differentiable simulators offer improved sample
efficiency through exact gradients, they can be unstable in contact-rich
environments and may lead to poor generalization. This paper introduces a novel
approach integrating sharpness-aware optimization into gradient-based
reinforcement learning algorithms. Our simulation results demonstrate that our
method, tested on contact-rich environments, significantly enhances policy
robustness to environmental variations and action perturbations while
maintaining the sample efficiency of first-order methods. Specifically, our
approach improves action noise tolerance compared to standard first-order
methods and achieves generalization comparable to zeroth-order methods. This
improvement stems from finding flatter minima in the loss landscape, associated
with better generalization. Our work offers a promising solution to balance
efficient learning and robust sim-to-real transfer in robotics, potentially
bridging the gap between simulation and real-world performance.",http://arxiv.org/pdf/2411.19732v1,,False
Real-Time Anomaly Detection in Video Streams,29/11/2024,Fabien Poirier,"This thesis is part of a CIFRE agreement between the company Othello and the
LIASD laboratory. The objective is to develop an artificial intelligence system
that can detect real-time dangers in a video stream. To achieve this, a novel
approach combining temporal and spatial analysis has been proposed. Several
avenues have been explored to improve anomaly detection by integrating object
detection, human pose detection, and motion analysis. For result
interpretability, techniques commonly used for image analysis, such as
activation and saliency maps, have been extended to videos, and an original
method has been proposed. The proposed architecture performs binary or
multiclass classification depending on whether an alert or the cause needs to
be identified. Numerous neural networkmodels have been tested, and three of
them have been selected. You Only Looks Once (YOLO) has been used for spatial
analysis, a Convolutional Recurrent Neuronal Network (CRNN) composed of VGG19
and a Gated Recurrent Unit (GRU) for temporal analysis, and a multi-layer
perceptron for classification. These models handle different types of data and
can be combined in parallel or in series. Although the parallel mode is faster,
the serial mode is generally more reliable. For training these models,
supervised learning was chosen, and two proprietary datasets were created. The
first dataset focuses on objects that may play a potential role in anomalies,
while the second consists of videos containing anomalies or non-anomalies. This
approach allows for the processing of both continuous video streams and finite
videos, providing greater flexibility in detection.",http://arxiv.org/pdf/2411.19731v1,,False
CogACT: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation,29/11/2024,"Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, Xiaofan Wang, Bei Liu, Jianlong Fu, Jianmin Bao, Dong Chen, Yuanchun Shi, Jiaolong Yang, Baining Guo","The advancement of large Vision-Language-Action (VLA) models has
significantly improved robotic manipulation in terms of language-guided task
execution and generalization to unseen scenarios. While existing VLAs adapted
from pretrained large Vision-Language-Models (VLM) have demonstrated promising
generalizability, their task performance is still unsatisfactory as indicated
by the low tasks success rates in different environments. In this paper, we
present a new advanced VLA architecture derived from VLM. Unlike previous works
that directly repurpose VLM for action prediction by simple action
quantization, we propose a omponentized VLA architecture that has a specialized
action module conditioned on VLM output. We systematically study the design of
the action module and demonstrates the strong performance enhancement with
diffusion action transformers for action sequence modeling, as well as their
favorable scaling behaviors. We also conduct comprehensive experiments and
ablation studies to evaluate the efficacy of our models with varied designs.
The evaluation on 5 robot embodiments in simulation and real work shows that
our model not only significantly surpasses existing VLAs in task performance
and but also exhibits remarkable adaptation to new robots and generalization to
unseen objects and backgrounds. It exceeds the average success rates of OpenVLA
which has similar model size (7B) with ours by over 35% in simulated evaluation
and 55% in real robot experiments. It also outperforms the large RT-2-X model
(55B) by 18% absolute success rates in simulation. Code and models can be found
on our project page (https://cogact.github.io/).",http://arxiv.org/pdf/2411.19650v1,,False
SkelMamba: A State Space Model for Efficient Skeleton Action Recognition of Neurological Disorders,29/11/2024,"Niki Martinel, Mariano Serrao, Christian Micheloni","We introduce a novel state-space model (SSM)-based framework for
skeleton-based human action recognition, with an anatomically-guided
architecture that improves state-of-the-art performance in both clinical
diagnostics and general action recognition tasks. Our approach decomposes
skeletal motion analysis into spatial, temporal, and spatio-temporal streams,
using channel partitioning to capture distinct movement characteristics
efficiently. By implementing a structured, multi-directional scanning strategy
within SSMs, our model captures local joint interactions and global motion
patterns across multiple anatomical body parts. This anatomically-aware
decomposition enhances the ability to identify subtle motion patterns critical
in medical diagnosis, such as gait anomalies associated with neurological
conditions. On public action recognition benchmarks, i.e., NTU RGB+D, NTU RGB+D
120, and NW-UCLA, our model outperforms current state-of-the-art methods,
achieving accuracy improvements up to $3.2\%$ with lower computational
complexity than previous leading transformer-based models. We also introduce a
novel medical dataset for motion-based patient neurological disorder analysis
to validate our method's potential in automated disease diagnosis.",http://arxiv.org/pdf/2411.19544v1,,False
Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head Synthesis,29/11/2024,"Tianqi Li, Ruobing Zheng, Minghui Yang, Jingdong Chen, Ming Yang","Recent advances in diffusion models have revolutionized audio-driven talking
head synthesis. Beyond precise lip synchronization, diffusion-based methods
excel in generating subtle expressions and natural head movements that are
well-aligned with the audio signal. However, these methods are confronted by
slow inference speed, insufficient fine-grained control over facial motions,
and occasional visual artifacts largely due to an implicit latent space derived
from Variational Auto-Encoders (VAE), which prevent their adoption in realtime
interaction applications. To address these issues, we introduce Ditto, a
diffusion-based framework that enables controllable realtime talking head
synthesis. Our key innovation lies in bridging motion generation and
photorealistic neural rendering through an explicit identity-agnostic motion
space, replacing conventional VAE representations. This design substantially
reduces the complexity of diffusion learning while enabling precise control
over the synthesized talking heads. We further propose an inference strategy
that jointly optimizes three key components: audio feature extraction, motion
generation, and video synthesis. This optimization enables streaming
processing, realtime inference, and low first-frame delay, which are the
functionalities crucial for interactive applications such as AI assistants.
Extensive experimental results demonstrate that Ditto generates compelling
talking head videos and substantially outperforms existing methods in both
motion control and realtime performance.",http://arxiv.org/pdf/2411.19509v1,,False
COLD: Causal reasOning in cLosed Daily activities,29/11/2024,"Abhinav Joshi, Areeb Ahmad, Ashutosh Modi","Large Language Models (LLMs) have shown state-of-the-art performance in a
variety of tasks, including arithmetic and reasoning; however, to gauge the
intellectual capabilities of LLMs, causal reasoning has become a reliable proxy
for validating a general understanding of the mechanics and intricacies of the
world similar to humans. Previous works in natural language processing (NLP)
have either focused on open-ended causal reasoning via causal commonsense
reasoning (CCR) or framed a symbolic representation-based question answering
for theoretically backed-up analysis via a causal inference engine. The former
adds an advantage of real-world grounding but lacks theoretically backed-up
analysis/validation, whereas the latter is far from real-world grounding. In
this work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed
Daily activities) framework, which is built upon human understanding of daily
real-world activities to reason about the causal nature of events. We show that
the proposed framework facilitates the creation of enormous causal queries (~ 9
million) and comes close to the mini-turing test, simulating causal reasoning
to evaluate the understanding of a daily real-world task. We evaluate multiple
LLMs on the created causal queries and find that causal reasoning is
challenging even for activities trivial to humans. We further explore (the
causal reasoning abilities of LLMs) using the backdoor criterion to determine
the causal strength between events.",http://arxiv.org/pdf/2411.19500v1,,False
Fleximo: Towards Flexible Text-to-Human Motion Video Generation,29/11/2024,"Yuhang Zhang, Yuan Zhou, Zeyu Liu, Yuxuan Cai, Qiuyue Wang, Aidong Men, Huan Yang","Current methods for generating human motion videos rely on extracting pose
sequences from reference videos, which restricts flexibility and control.
Additionally, due to the limitations of pose detection techniques, the
extracted pose sequences can sometimes be inaccurate, leading to low-quality
video outputs. We introduce a novel task aimed at generating human motion
videos solely from reference images and natural language. This approach offers
greater flexibility and ease of use, as text is more accessible than the
desired guidance videos. However, training an end-to-end model for this task
requires millions of high-quality text and human motion video pairs, which are
challenging to obtain. To address this, we propose a new framework called
Fleximo, which leverages large-scale pre-trained text-to-3D motion models. This
approach is not straightforward, as the text-generated skeletons may not
consistently match the scale of the reference image and may lack detailed
information. To overcome these challenges, we introduce an anchor point based
rescale method and design a skeleton adapter to fill in missing details and
bridge the gap between text-to-motion and motion-to-video generation. We also
propose a video refinement process to further enhance video quality. A large
language model (LLM) is employed to decompose natural language into discrete
motion sequences, enabling the generation of motion videos of any desired
length. To assess the performance of Fleximo, we introduce a new benchmark
called MotionBench, which includes 400 videos across 20 identities and 20
motions. We also propose a new metric, MotionScore, to evaluate the accuracy of
motion following. Both qualitative and quantitative results demonstrate that
our method outperforms existing text-conditioned image-to-video generation
methods. All code and model weights will be made publicly available.",http://arxiv.org/pdf/2411.19459v1,,False
Learning Visual Abstract Reasoning through Dual-Stream Networks,29/11/2024,"Kai Zhao, Chang Xu, Bailu Si","Visual abstract reasoning tasks present challenges for deep neural networks,
exposing limitations in their capabilities. In this work, we present a neural
network model that addresses the challenges posed by Raven's Progressive
Matrices (RPM). Inspired by the two-stream hypothesis of visual processing, we
introduce the Dual-stream Reasoning Network (DRNet), which utilizes two
parallel branches to capture image features. On top of the two streams, a
reasoning module first learns to merge the high-level features of the same
image. Then, it employs a rule extractor to handle combinations involving the
eight context images and each candidate image, extracting discrete abstract
rules and utilizing an multilayer perceptron (MLP) to make predictions.
Empirical results demonstrate that the proposed DRNet achieves state-of-the-art
average performance across multiple RPM benchmarks. Furthermore, DRNet
demonstrates robust generalization capabilities, even extending to various
out-of-distribution scenarios. The dual streams within DRNet serve distinct
functions by addressing local or spatial information. They are then integrated
into the reasoning module, leveraging abstract rules to facilitate the
execution of visual reasoning tasks. These findings indicate that the
dual-stream architecture could play a crucial role in visual abstract
reasoning.",http://arxiv.org/pdf/2411.19451v1,10.1609/aaai.v38i15.29641,False
Unsupervised Learning Approach to Anomaly Detection in Gravitational Wave Data,29/11/2024,Ammar Fayad,"Gravitational waves (GW), predicted by Einstein's General Theory of
Relativity, provide a powerful probe of astrophysical phenomena and fundamental
physics. In this work, we propose an unsupervised anomaly detection method
using variational autoencoders (VAEs) to analyze GW time-series data. By
training on noise-only data, the VAE accurately reconstructs noise inputs while
failing to reconstruct anomalies, such as GW signals, which results in
measurable spikes in the reconstruction error. The method was applied to data
from the LIGO H1 and L1 detectors. Evaluation on testing datasets containing
both noise and GW events demonstrated reliable detection, achieving an area
under the ROC curve (AUC) of 0.89. This study introduces VAEs as a robust,
unsupervised approach for identifying anomalies in GW data, which offers a
scalable framework for detecting known and potentially new phenomena in
physics.",http://arxiv.org/pdf/2411.19450v1,,False
Adaptive Interactive Segmentation for Multimodal Medical Imaging via Selection Engine,29/11/2024,"Zhi Li, Kai Zhao, Yaqi Wang, Shuai Wang","In medical image analysis, achieving fast, efficient, and accurate
segmentation is essential for automated diagnosis and treatment. Although
recent advancements in deep learning have significantly improved segmentation
accuracy, current models often face challenges in adaptability and
generalization, particularly when processing multi-modal medical imaging data.
These limitations stem from the substantial variations between imaging
modalities and the inherent complexity of medical data. To address these
challenges, we propose the Strategy-driven Interactive Segmentation Model
(SISeg), built on SAM2, which enhances segmentation performance across various
medical imaging modalities by integrating a selection engine. To mitigate
memory bottlenecks and optimize prompt frame selection during the inference of
2D image sequences, we developed an automated system, the Adaptive Frame
Selection Engine (AFSE). This system dynamically selects the optimal prompt
frames without requiring extensive prior medical knowledge and enhances the
interpretability of the model's inference process through an interactive
feedback mechanism. We conducted extensive experiments on 10 datasets covering
7 representative medical imaging modalities, demonstrating the SISeg model's
robust adaptability and generalization in multi-modal tasks. The project page
and code will be available at: [URL].",http://arxiv.org/pdf/2411.19447v1,,False
AMO Sampler: Enhancing Text Rendering with Overshooting,28/11/2024,"Xixi Hu, Keyang Xu, Bo Liu, Qiang Liu, Hongliang Fei","Achieving precise alignment between textual instructions and generated images
in text-to-image generation is a significant challenge, particularly in
rendering written text within images. Sate-of-the-art models like Stable
Diffusion 3 (SD3), Flux, and AuraFlow still struggle with accurate text
depiction, resulting in misspelled or inconsistent text. We introduce a
training-free method with minimal computational overhead that significantly
enhances text rendering quality. Specifically, we introduce an overshooting
sampler for pretrained rectified flow (RF) models, by alternating between
over-simulating the learned ordinary differential equation (ODE) and
reintroducing noise. Compared to the Euler sampler, the overshooting sampler
effectively introduces an extra Langevin dynamics term that can help correct
the compounding error from successive Euler steps and therefore improve the
text rendering. However, when the overshooting strength is high, we observe
over-smoothing artifacts on the generated images. To address this issue, we
propose an Attention Modulated Overshooting sampler (AMO), which adaptively
controls the strength of overshooting for each image patch according to their
attention score with the text content. AMO demonstrates a 32.3% and 35.9%
improvement in text rendering accuracy on SD3 and Flux without compromising
overall image quality or increasing inference cost.",http://arxiv.org/pdf/2411.19415v1,,False
Integrating Transit Signal Priority into Multi-Agent Reinforcement Learning based Traffic Signal Control,28/11/2024,"Dickness Kakitahi Kwesiga, Suyash Chandra Vishnoi, Angshuman Guin, Michael Hunter","This study integrates Transit Signal Priority (TSP) into multi-agent
reinforcement learning (MARL) based traffic signal control. The first part of
the study develops adaptive signal control based on MARL for a pair of
coordinated intersections in a microscopic simulation environment. The two
agents, one for each intersection, are centrally trained using a value
decomposition network (VDN) architecture. The trained agents show slightly
better performance compared to coordinated actuated signal control based on
overall intersection delay at v/c of 0.95. In the second part of the study the
trained signal control agents are used as background signal controllers while
developing event-based TSP agents. In one variation, independent TSP agents are
formulated and trained under a decentralized training and decentralized
execution (DTDE) framework to implement TSP at each intersection. In the second
variation, the two TSP agents are centrally trained under a centralized
training and decentralized execution (CTDE) framework and VDN architecture to
select and implement coordinated TSP strategies across the two intersections.
In both cases the agents converge to the same bus delay value, but independent
agents show high instability throughout the training process. For the test
runs, the two independent agents reduce bus delay across the two intersections
by 22% compared to the no TSP case while the coordinated TSP agents achieve 27%
delay reduction. In both cases, there is only a slight increase in delay for a
majority of the side street movements.",http://arxiv.org/pdf/2411.19359v1,,False
3D Wasserstein generative adversarial network with dense U-Net based discriminator for preclinical fMRI denoising,28/11/2024,"Sima Soltanpour, Arnold Chang, Dan Madularu, Praveen Kulkarni, Craig Ferris, Chris Joslin","Functional magnetic resonance imaging (fMRI) is extensively used in clinical
and preclinical settings to study brain function, however, fMRI data is
inherently noisy due to physiological processes, hardware, and external noise.
Denoising is one of the main preprocessing steps in any fMRI analysis pipeline.
This process is challenging in preclinical data in comparison to clinical data
due to variations in brain geometry, image resolution, and low signal-to-noise
ratios. In this paper, we propose a structure-preserved algorithm based on a 3D
Wasserstein generative adversarial network with a 3D dense U-net based
discriminator called, 3D U-WGAN. We apply a 4D data configuration to
effectively denoise temporal and spatial information in analyzing preclinical
fMRI data. GAN-based denoising methods often utilize a discriminator to
identify significant differences between denoised and noise-free images,
focusing on global or local features. To refine the fMRI denoising model, our
method employs a 3D dense U-Net discriminator to learn both global and local
distinctions. To tackle potential over-smoothing, we introduce an adversarial
loss and enhance perceptual similarity by measuring feature space distances.
Experiments illustrate that 3D U-WGAN significantly improves image quality in
resting-state and task preclinical fMRI data, enhancing signal-to-noise ratio
without introducing excessive structural changes in existing methods. The
proposed method outperforms state-of-the-art methods when applied to simulated
and real data in a fMRI analysis pipeline.",http://arxiv.org/pdf/2411.19345v1,,False
GRAPE: Generalizing Robot Policy via Preference Alignment,28/11/2024,"Zijian Zhang, Kaiyuan Zheng, Zhaorun Chen, Joel Jang, Yi Li, Chaoqi Wang, Mingyu Ding, Dieter Fox, Huaxiu Yao","Despite the recent advancements of vision-language-action (VLA) models on a
variety of robotics tasks, they suffer from critical issues such as poor
generalizability to unseen tasks, due to their reliance on behavior cloning
exclusively from successful rollouts. Furthermore, they are typically
fine-tuned to replicate demonstrations collected by experts under different
settings, thus introducing distribution bias and limiting their adaptability to
diverse manipulation objectives, such as efficiency, safety, and task
completion. To bridge this gap, we introduce GRAPE: Generalizing Robot Policy
via Preference Alignment. Specifically, GRAPE aligns VLAs on a trajectory level
and implicitly models reward from both successful and failure trials to boost
generalizability to diverse tasks. Moreover, GRAPE breaks down complex
manipulation tasks to independent stages and automatically guides preference
modeling through customized spatiotemporal constraints with keypoints proposed
by a large vision-language model. Notably, these constraints are flexible and
can be customized to align the model with varying objectives, such as safety,
efficiency, or task success. We evaluate GRAPE across a diverse array of tasks
in both real-world and simulated environments. Experimental results demonstrate
that GRAPE enhances the performance of state-of-the-art VLA models, increasing
success rates on in-domain and unseen manipulation tasks by 51.79% and 60.36%,
respectively. Additionally, GRAPE can be aligned with various objectives, such
as safety and efficiency, reducing collision rates by 44.31% and rollout
step-length by 11.15%, respectively. All code, models, and data are available
at https://grape-vla.github.io/",http://arxiv.org/pdf/2411.19309v1,,False
SOWing Information: Cultivating Contextual Coherence with MLLMs in Image Generation,28/11/2024,"Yuhan Pei, Ruoyu Wang, Yongqi Yang, Ye Zhu, Olga Russakovsky, Yu Wu","Originating from the diffusion phenomenon in physics, which describes the
random movement and collisions of particles, diffusion generative models
simulate a random walk in the data space along the denoising trajectory. This
allows information to diffuse across regions, yielding harmonious outcomes.
However, the chaotic and disordered nature of information diffusion in
diffusion models often results in undesired interference between image regions,
causing degraded detail preservation and contextual inconsistency. In this
work, we address these challenges by reframing disordered diffusion as a
powerful tool for text-vision-to-image generation (TV2I) tasks, achieving
pixel-level condition fidelity while maintaining visual and semantic coherence
throughout the image. We first introduce Cyclic One-Way Diffusion (COW), which
provides an efficient unidirectional diffusion framework for precise
information transfer while minimizing disruptive interference. Building on COW,
we further propose Selective One-Way Diffusion (SOW), which utilizes Multimodal
Large Language Models (MLLMs) to clarify the semantic and spatial relationships
within the image. Based on these insights, SOW combines attention mechanisms to
dynamically regulate the direction and intensity of diffusion according to
contextual relationships. Extensive experiments demonstrate the untapped
potential of controlled information diffusion, offering a path to more adaptive
and versatile generative models in a learning-free manner.",http://arxiv.org/pdf/2411.19182v1,,False
TEA: Trajectory Encoding Augmentation for Robust and Transferable Policies in Offline Reinforcement Learning,28/11/2024,"Batıkan Bora Ormancı, Phillip Swazinna, Steffen Udluft, Thomas A. Runkler","In this paper, we investigate offline reinforcement learning (RL) with the
goal of training a single robust policy that generalizes effectively across
environments with unseen dynamics. We propose a novel approach, Trajectory
Encoding Augmentation (TEA), which extends the state space by integrating
latent representations of environmental dynamics obtained from sequence
encoders, such as AutoEncoders. Our findings show that incorporating these
encodings with TEA improves the transferability of a single policy to novel
environments with new dynamics, surpassing methods that rely solely on
unmodified states. These results indicate that TEA captures critical,
environment-specific characteristics, enabling RL agents to generalize
effectively across dynamic conditions.",http://arxiv.org/pdf/2411.19133v1,,False
GRU-PFG: Extract Inter-Stock Correlation from Stock Factors with Graph Neural Network,28/11/2024,"Yonggai Zhuang, Haoran Chen, Kequan Wang, Teng Fei","The complexity of stocks and industries presents challenges for stock
prediction. Currently, stock prediction models can be divided into two
categories. One category, represented by GRU and ALSTM, relies solely on stock
factors for prediction, with limited effectiveness. The other category,
represented by HIST and TRA, incorporates not only stock factors but also
industry information, industry financial reports, public sentiment, and other
inputs for prediction. The second category of models can capture correlations
between stocks by introducing additional information, but the extra data is
difficult to standardize and generalize. Considering the current state and
limitations of these two types of models, this paper proposes the GRU-PFG
(Project Factors into Graph) model. This model only takes stock factors as
input and extracts inter-stock correlations using graph neural networks. It
achieves prediction results that not only outperform the others models relies
solely on stock factors, but also achieve comparable performance to the second
category models. The experimental results show that on the CSI300 dataset, the
IC of GRU-PFG is 0.134, outperforming HIST's 0.131 and significantly surpassing
GRU and Transformer, achieving results better than the second category models.
Moreover as a model that relies solely on stock factors, it has greater
potential for generalization.",http://arxiv.org/pdf/2411.18997v1,,False
VIPaint: Image Inpainting with Pre-Trained Diffusion Models via Variational Inference,28/11/2024,"Sakshi Agarwal, Gabe Hoope, Erik B. Sudderth","Diffusion probabilistic models learn to remove noise that is artificially
added to the data during training. Novel data, like images, may then be
generated from Gaussian noise through a sequence of denoising operations. While
this Markov process implicitly defines a joint distribution over noise-free
data, it is not simple to condition the generative process on masked or partial
images. A number of heuristic sampling procedures have been proposed for
solving inverse problems with diffusion priors, but these approaches do not
directly approximate the true conditional distribution imposed by inference
queries, and are often ineffective for large masked regions. Moreover, many of
these baselines cannot be applied to latent diffusion models which use image
encodings for efficiency. We instead develop a hierarchical variational
inference algorithm that analytically marginalizes missing features, and uses a
rigorous variational bound to optimize a non-Gaussian Markov approximation of
the true diffusion posterior. Through extensive experiments with both
pixel-based and latent diffusion models of images, we show that our VIPaint
method significantly outperforms previous approaches in both the plausibility
and diversity of imputations, and is easily generalized to other inverse
problems like deblurring and superresolution.",http://arxiv.org/pdf/2411.18929v1,,False
EzSQL: An SQL intermediate representation for improving SQL-to-text Generation,28/11/2024,"Meher Bhardwaj, Hrishikesh Ethari, Dennis Singh Moirangthem","The SQL-to-text generation task traditionally uses template base, Seq2Seq,
tree-to-sequence, and graph-to-sequence models. Recent models take advantage of
pre-trained generative language models for this task in the Seq2Seq framework.
However, treating SQL as a sequence of inputs to the pre-trained models is not
optimal. In this work, we put forward a new SQL intermediate representation
called EzSQL to align SQL with the natural language text sequence. EzSQL
simplifies the SQL queries and brings them closer to natural language text by
modifying operators and keywords, which can usually be described in natural
language. EzSQL also removes the need for set operators. Our proposed
SQL-to-text generation model uses EzSQL as the input to a pre-trained
generative language model for generating the text descriptions. We demonstrate
that our model is an effective state-of-the-art method to generate text
narrations from SQL queries on the WikiSQL and Spider datasets. We also show
that by generating pretraining data using our SQL-to-text generation model, we
can enhance the performance of Text-to-SQL parsers.",http://arxiv.org/pdf/2411.18923v1,,False
Improving Accuracy and Generalization for Efficient Visual Tracking,28/11/2024,"Ram Zaveri, Shivang Patel, Yu Gu, Gianfranco Doretto","Efficient visual trackers overfit to their training distributions and lack
generalization abilities, resulting in them performing well on their respective
in-distribution (ID) test sets and not as well on out-of-distribution (OOD)
sequences, imposing limitations to their deployment in-the-wild under
constrained resources. We introduce SiamABC, a highly efficient Siamese tracker
that significantly improves tracking performance, even on OOD sequences.
SiamABC takes advantage of new architectural designs in the way it bridges the
dynamic variability of the target, and of new losses for training. Also, it
directly addresses OOD tracking generalization by including a fast
backward-free dynamic test-time adaptation method that continuously adapts the
model according to the dynamic visual changes of the target. Our extensive
experiments suggest that SiamABC shows remarkable performance gains in OOD sets
while maintaining accurate performance on the ID benchmarks. SiamABC
outperforms MixFormerV2-S by 7.6\% on the OOD AVisT benchmark while being 3x
faster (100 FPS) on a CPU.",http://arxiv.org/pdf/2411.18855v1,,False
