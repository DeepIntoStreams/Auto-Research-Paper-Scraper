Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion,15/07/2024,"Yongyuan Liang, Tingqiang Xu, Kaizhe Hu, Guangqi Jiang, Furong Huang, Huazhe Xu","Can we generate a control policy for an agent using just one demonstration of
desired behaviors as a prompt, as effortlessly as creating an image from a
textual description? In this paper, we present Make-An-Agent, a novel policy
parameter generator that leverages the power of conditional diffusion models
for behavior-to-policy generation. Guided by behavior embeddings that encode
trajectory information, our policy generator synthesizes latent parameter
representations, which can then be decoded into policy networks. Trained on
policy network checkpoints and their corresponding trajectories, our generation
model demonstrates remarkable versatility and scalability on multiple tasks and
has a strong generalization ability on unseen tasks to output well-performed
policies with only few-shot demonstrations as inputs. We showcase its efficacy
and efficiency on various domains and tasks, including varying objectives,
behaviors, and even across different robot manipulators. Beyond simulation, we
directly deploy policies generated by Make-An-Agent onto real-world robots on
locomotion tasks.",http://arxiv.org/pdf/2407.10973v1,,False
Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?,15/07/2024,"Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, Tianbao Xie, Hongshen Xu, Danyang Zhang, Sida Wang, Ruoxi Sun, Pengcheng Yin, Caiming Xiong, Ansong Ni, Qian Liu, Victor Zhong, Lu Chen, Kai Yu, Tao Yu","Data science and engineering workflows often span multiple stages, from
warehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As
vision language models (VLMs) advance in multimodal understanding and code
generation, VLM-based agents could potentially automate these workflows by
generating SQL queries, Python code, and GUI operations. This automation can
improve the productivity of experts while democratizing access to large-scale
data analysis. In this paper, we introduce Spider2-V, the first multimodal
agent benchmark focusing on professional data science and engineering
workflows, featuring 494 real-world tasks in authentic computer environments
and incorporating 20 enterprise-level professional applications. These tasks,
derived from real-world use cases, evaluate the ability of a multimodal agent
to perform data-related tasks by writing code and managing the GUI in
enterprise data software systems. To balance realistic simulation with
evaluation simplicity, we devote significant effort to developing automatic
configurations for task setup and carefully crafting evaluation metrics for
each task. Furthermore, we supplement multimodal agents with comprehensive
documents of these enterprise data software systems. Our empirical evaluation
reveals that existing state-of-the-art LLM/VLM-based agents do not reliably
automate full data workflows (14.0% success). Even with step-by-step guidance,
these agents still underperform in tasks that require fine-grained,
knowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted
workspaces (10.6%). We hope that Spider2-V paves the way for autonomous
multimodal agents to transform the automation of data science and engineering
workflow. Our code and data are available at https://spider2-v.github.io.",http://arxiv.org/pdf/2407.10956v1,,False
Representing Rule-based Chatbots with Transformers,15/07/2024,"Dan Friedman, Abhishek Panigrahi, Danqi Chen","Transformer-based chatbots can conduct fluent, natural-sounding
conversations, but we have limited understanding of the mechanisms underlying
their behavior. Prior work has taken a bottom-up approach to understanding
Transformers by constructing Transformers for various synthetic and formal
language tasks, such as regular expressions and Dyck languages. However, it is
not obvious how to extend this approach to understand more naturalistic
conversational agents. In this work, we take a step in this direction by
constructing a Transformer that implements the ELIZA program, a classic,
rule-based chatbot. ELIZA illustrates some of the distinctive challenges of the
conversational setting, including both local pattern matching and long-term
dialog state tracking. We build on constructions from prior work -- in
particular, for simulating finite-state automata -- showing how simpler
constructions can be composed and extended to give rise to more sophisticated
behavior. Next, we train Transformers on a dataset of synthetically generated
ELIZA conversations and investigate the mechanisms the models learn. Our
analysis illustrates the kinds of mechanisms these models tend to prefer -- for
example, models favor an induction head mechanism over a more precise, position
based copying mechanism; and using intermediate generations to simulate
recurrent data structures, like ELIZA's memory mechanisms. Overall, by drawing
an explicit connection between neural chatbots and interpretable, symbolic
mechanisms, our results offer a new setting for mechanistic analysis of
conversational agents.",http://arxiv.org/pdf/2407.10949v1,,False
Employing Sentence Space Embedding for Classification of Data Stream from Fake News Domain,15/07/2024,"Paweł Zyblewski, Jakub Klikowski, Weronika Borek-Marciniec, Paweł Ksieniewicz","Tabular data is considered the last unconquered castle of deep learning, yet
the task of data stream classification is stated to be an equally important and
demanding research area. Due to the temporal constraints, it is assumed that
deep learning methods are not the optimal solution for application in this
field. However, excluding the entire -- and prevalent -- group of methods seems
rather rash given the progress that has been made in recent years in its
development. For this reason, the following paper is the first to present an
approach to natural language data stream classification using the sentence
space method, which allows for encoding text into the form of a discrete
digital signal. This allows the use of convolutional deep networks dedicated to
image classification to solve the task of recognizing fake news based on text
data. Based on the real-life Fakeddit dataset, the proposed approach was
compared with state-of-the-art algorithms for data stream classification based
on generalization ability and time complexity.",http://arxiv.org/pdf/2407.10807v1,,False
SEMINAR: Search Enhanced Multi-modal Interest Network and Approximate Retrieval for Lifelong Sequential Recommendation,15/07/2024,"Kaiming Shen, Xichen Ding, Zixiang Zheng, Yuqi Gong, Qianqian Li, Zhongyi Liu, Guannan Zhang","The modeling of users' behaviors is crucial in modern recommendation systems.
A lot of research focuses on modeling users' lifelong sequences, which can be
extremely long and sometimes exceed thousands of items. These models use the
target item to search for the most relevant items from the historical sequence.
However, training lifelong sequences in click through rate (CTR) prediction or
personalized search ranking (PSR) is extremely difficult due to the
insufficient learning problem of ID embedding, especially when the IDs in the
lifelong sequence features do not exist in the samples of training dataset.
Additionally, existing target attention mechanisms struggle to learn the
multi-modal representations of items in the sequence well. The distribution of
multi-modal embedding (text, image and attributes) output of user's interacted
items are not properly aligned and there exist divergence across modalities. We
also observe that users' search query sequences and item browsing sequences can
fully depict users' intents and benefit from each other. To address these
challenges, we propose a unified lifelong multi-modal sequence model called
SEMINAR-Search Enhanced Multi-Modal Interest Network and Approximate Retrieval.
Specifically, a network called Pretraining Search Unit (PSU) learns the
lifelong sequences of multi-modal query-item pairs in a pretraining-finetuning
manner with multiple objectives: multi-modal alignment, next query-item pair
prediction, query-item relevance prediction, etc. After pretraining, the
downstream model restores the pretrained embedding as initialization and
finetunes the network. To accelerate the online retrieval speed of multi-modal
embedding, we propose a multi-modal codebook-based product quantization
strategy to approximate the exact attention calculati",http://arxiv.org/pdf/2407.10714v1,,False
Spatio-temporal neural distance fields for conditional generative modeling of the heart,15/07/2024,"Kristine Sørensen, Paula Diez, Jan Margeta, Yasmin El Youssef, Michael Pham, Jonas Jalili Pedersen, Tobias Kühl, Ole de Backer, Klaus Kofoed, Oscar Camara, Rasmus Paulsen","The rhythmic pumping motion of the heart stands as a cornerstone in life, as
it circulates blood to the entire human body through a series of carefully
timed contractions of the individual chambers. Changes in the size, shape and
movement of the chambers can be important markers for cardiac disease and
modeling this in relation to clinical demography or disease is therefore of
interest. Existing methods for spatio-temporal modeling of the human heart
require shape correspondence over time or suffer from large memory
requirements, making it difficult to use for complex anatomies. We introduce a
novel conditional generative model, where the shape and movement is modeled
implicitly in the form of a spatio-temporal neural distance field and
conditioned on clinical demography. The model is based on an auto-decoder
architecture and aims to disentangle the individual variations from that
related to the clinical demography. It is tested on the left atrium (including
the left atrial appendage), where it outperforms current state-of-the-art
methods for anatomical sequence completion and generates synthetic sequences
that realistically mimics the shape and motion of the real left atrium. In
practice, this means we can infer functional measurements from a static image,
generate synthetic populations with specified demography or disease and
investigate how non-imaging clinical data effect the shape and motion of
cardiac anatomies.",http://arxiv.org/pdf/2407.10663v1,,False
SuperPADL: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation,15/07/2024,"Jordan Juravsky, Yunrong Guo, Sanja Fidler, Xue Bin Peng","Physically-simulated models for human motion can generate high-quality
responsive character animations, often in real-time. Natural language serves as
a flexible interface for controlling these models, allowing expert and
non-expert users to quickly create and edit their animations. Many recent
physics-based animation methods, including those that use text interfaces,
train control policies using reinforcement learning (RL). However, scaling
these methods beyond several hundred motions has remained challenging.
Meanwhile, kinematic animation models are able to successfully learn from
thousands of diverse motions by leveraging supervised learning methods.
Inspired by these successes, in this work we introduce SuperPADL, a scalable
framework for physics-based text-to-motion that leverages both RL and
supervised learning to train controllers on thousands of diverse motion clips.
SuperPADL is trained in stages using progressive distillation, starting with a
large number of specialized experts using RL. These experts are then
iteratively distilled into larger, more robust policies using a combination of
reinforcement learning and supervised learning. Our final SuperPADL controller
is trained on a dataset containing over 5000 skills and runs in real time on a
consumer GPU. Moreover, our policy can naturally transition between skills,
allowing for users to interactively craft multi-stage animations. We
experimentally demonstrate that SuperPADL significantly outperforms RL-based
baselines at this large data scale.",http://arxiv.org/pdf/2407.10481v1,10.1145/3641519.3657492,False
LiteFocus: Accelerated Diffusion Inference for Long Audio Synthesis,15/07/2024,"Zhenxiong Tan, Xinyin Ma, Gongfan Fang, Xinchao Wang","Latent diffusion models have shown promising results in audio generation,
making notable advancements over traditional methods. However, their
performance, while impressive with short audio clips, faces challenges when
extended to longer audio sequences. These challenges are due to model's
self-attention mechanism and training predominantly on 10-second clips, which
complicates the extension to longer audio without adaptation. In response to
these issues, we introduce a novel approach, LiteFocus that enhances the
inference of existing audio latent diffusion models in long audio synthesis.
Observed the attention pattern in self-attention, we employ a dual sparse form
for attention calculation, designated as same-frequency focus and
cross-frequency compensation, which curtails the attention computation under
same-frequency constraints, while enhancing audio quality through
cross-frequency refillment. LiteFocus demonstrates substantial reduction on
inference time with diffusion-based TTA model by 1.99x in synthesizing
80-second audio clips while also obtaining improved audio quality.",http://arxiv.org/pdf/2407.10468v1,,False
BandControlNet: Parallel Transformers-based Steerable Popular Music Generation with Fine-Grained Spatiotemporal Features,15/07/2024,"Jing Luo, Xinyu Yang, Dorien Herremans","Controllable music generation promotes the interaction between humans and
composition systems by projecting the users' intent on their desired music. The
challenge of introducing controllability is an increasingly important issue in
the symbolic music generation field. When building controllable generative
popular multi-instrument music systems, two main challenges typically present
themselves, namely weak controllability and poor music quality. To address
these issues, we first propose spatiotemporal features as powerful and
fine-grained controls to enhance the controllability of the generative model.
In addition, an efficient music representation called REMI_Track is designed to
convert multitrack music into multiple parallel music sequences and shorten the
sequence length of each track with Byte Pair Encoding (BPE) techniques.
Subsequently, we release BandControlNet, a conditional model based on parallel
Transformers, to tackle the multiple music sequences and generate high-quality
music samples that are conditioned to the given spatiotemporal control
features. More concretely, the two specially designed modules of
BandControlNet, namely structure-enhanced self-attention (SE-SA) and
Cross-Track Transformer (CTT), are utilized to strengthen the resulting musical
structure and inter-track harmony modeling respectively. Experimental results
tested on two popular music datasets of different lengths demonstrate that the
proposed BandControlNet outperforms other conditional music generation models
on most objective metrics in terms of fidelity and inference speed and shows
great robustness in generating long music samples. The subjective evaluations
show BandControlNet trained on short datasets can generate music with
comparable quality to state-of-the-art models, while outperforming them
significantly using longer datasets.",http://arxiv.org/pdf/2407.10462v1,,False
GraphPrint: Extracting Features from 3D Protein Structure for Drug Target Affinity Prediction,15/07/2024,Amritpal Singh,"Accurate drug target affinity prediction can improve drug candidate
selection, accelerate the drug discovery process, and reduce drug production
costs. Previous work focused on traditional fingerprints or used features
extracted based on the amino acid sequence in the protein, ignoring its 3D
structure which affects its binding affinity. In this work, we propose
GraphPrint: a framework for incorporating 3D protein structure features for
drug target affinity prediction. We generate graph representations for protein
3D structures using amino acid residue location coordinates and combine them
with drug graph representation and traditional features to jointly learn drug
target affinity. Our model achieves a mean square error of 0.1378 and a
concordance index of 0.8929 on the KIBA dataset and improves over using
traditional protein features alone. Our ablation study shows that the 3D
protein structure-based features provide information complementary to
traditional features.",http://arxiv.org/pdf/2407.10452v1,,False
Omni-Dimensional Frequency Learner for General Time Series Analysis,15/07/2024,"Xianing Chen. Hanting Chen, Hailin Hu","Frequency domain representation of time series feature offers a concise
representation for handling real-world time series data with inherent
complexity and dynamic nature. However, current frequency-based methods with
complex operations still fall short of state-of-the-art time domain methods for
general time series analysis. In this work, we present Omni-Dimensional
Frequency Learner (ODFL) model based on a in depth analysis among all the three
aspects of the spectrum feature: channel redundancy property among the
frequency dimension, the sparse and un-salient frequency energy distribution
among the frequency dimension, and the semantic diversity among the variable
dimension. Technically, our method is composed of a semantic-adaptive global
filter with attention to the un-salient frequency bands and partial operation
among the channel dimension. Empirical results show that ODFL achieves
consistent state-of-the-art in five mainstream time series analysis tasks,
including short- and long-term forecasting, imputation, classification, and
anomaly detection, offering a promising foundation for time series analysis.",http://arxiv.org/pdf/2407.10419v1,,False
Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity,15/07/2024,"Santiago Pascual, Chunghsin Yeh, Ioannis Tsiamas, Joan Serrà","Video-to-audio (V2A) generation leverages visual-only video features to
render plausible sounds that match the scene. Importantly, the generated sound
onsets should match the visual actions that are aligned with them, otherwise
unnatural synchronization artifacts arise. Recent works have explored the
progression of conditioning sound generators on still images and then video
features, focusing on quality and semantic matching while ignoring
synchronization, or by sacrificing some amount of quality to focus on improving
synchronization only. In this work, we propose a V2A generative model, named
MaskVAT, that interconnects a full-band high-quality general audio codec with a
sequence-to-sequence masked generative model. This combination allows modeling
both high audio quality, semantic matching, and temporal synchronicity at the
same time. Our results show that, by combining a high-quality codec with the
proper pre-trained audio-visual features and a sequence-to-sequence parallel
structure, we are able to yield highly synchronized results on one hand, whilst
being competitive with the state of the art of non-codec generative audio
models. Sample videos and generated audios are available at
https://maskvat.github.io .",http://arxiv.org/pdf/2407.10387v1,,False
By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting,15/07/2024,"Hyungjun Yoon, Biniyam Aschalew Tolera, Taesik Gong, Kimin Lee, Sung-Ju Lee","Large language models (LLMs) have demonstrated exceptional abilities across
various domains. However, utilizing LLMs for ubiquitous sensing applications
remains challenging as existing text-prompt methods show significant
performance degradation when handling long sensor data sequences. We propose a
visual prompting approach for sensor data using multimodal LLMs (MLLMs). We
design a visual prompt that directs MLLMs to utilize visualized sensor data
alongside the target sensory task descriptions. Additionally, we introduce a
visualization generator that automates the creation of optimal visualizations
tailored to a given sensory task, eliminating the need for prior task-specific
knowledge. We evaluated our approach on nine sensory tasks involving four
sensing modalities, achieving an average of 10% higher accuracy than text-based
prompts and reducing token costs by 15.8x. Our findings highlight the
effectiveness and cost-efficiency of visual prompts with MLLMs for various
sensory tasks.",http://arxiv.org/pdf/2407.10385v1,,False
