Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
"Equi-Euler GraphNet: An Equivariant, Temporal-Dynamics Informed Graph Neural Network for Dual Force and Trajectory Prediction in Multi-Body Systems",18/04/2025,"Vinay Sharma, RÃ©mi Tanguy Oddon, Pietro Tesini, Jens Ravesloot, Cees Taal, Olga Fink","Accurate real-time modeling of multi-body dynamical systems is essential for
enabling digital twin applications across industries. While many data-driven
approaches aim to learn system dynamics, jointly predicting internal loads and
system trajectories remains a key challenge. This dual prediction is especially
important for fault detection and predictive maintenance, where internal
loads-such as contact forces-act as early indicators of faults, reflecting wear
or misalignment before affecting motion. These forces also serve as inputs to
degradation models (e.g., crack growth), enabling damage prediction and
remaining useful life estimation. We propose Equi-Euler GraphNet, a
physics-informed graph neural network (GNN) that simultaneously predicts
internal forces and global trajectories in multi-body systems. In this
mesh-free framework, nodes represent system components and edges encode
interactions. Equi-Euler GraphNet introduces two inductive biases: (1) an
equivariant message-passing scheme, interpreting edge messages as interaction
forces consistent under Euclidean transformations; and (2) a temporal-aware
iterative node update mechanism, based on Euler integration, to capture
influence of distant interactions over time. Tailored for cylindrical roller
bearings, it decouples ring dynamics from constrained motion of rolling
elements. Trained on high-fidelity multiphysics simulations, Equi-Euler
GraphNet generalizes beyond the training distribution, accurately predicting
loads and trajectories under unseen speeds, loads, and configurations. It
outperforms state-of-the-art GNNs focused on trajectory prediction, delivering
stable rollouts over thousands of time steps with minimal error accumulation.
Achieving up to a 200x speedup over conventional solvers while maintaining
comparable accuracy, it serves as an efficient reduced-order model for digital
twins, design, and maintenance.",http://arxiv.org/pdf/2504.13768v1,,False
Learning to Attribute with Attention,18/04/2025,"Benjamin Cohen-Wang, Yung-Sung Chuang, Aleksander Madry","Given a sequence of tokens generated by a language model, we may want to
identify the preceding tokens that influence the model to generate this
sequence. Performing such token attribution is expensive; a common approach is
to ablate preceding tokens and directly measure their effects. To reduce the
cost of token attribution, we revisit attention weights as a heuristic for how
a language model uses previous tokens. Naive approaches to attribute model
behavior with attention (e.g., averaging attention weights across attention
heads to estimate a token's influence) have been found to be unreliable. To
attain faithful attributions, we propose treating the attention weights of
different attention heads as features. This way, we can learn how to
effectively leverage attention weights for attribution (using signal from
ablations). Our resulting method, Attribution with Attention (AT2), reliably
performs on par with approaches that involve many ablations, while being
significantly more efficient. To showcase the utility of AT2, we use it to
prune less important parts of a provided context in a question answering
setting, improving answer quality. We provide code for AT2 at
https://github.com/MadryLab/AT2 .",http://arxiv.org/pdf/2504.13752v1,,False
Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping Occupied Territory from Open Source Intelligence,18/04/2025,"Paul K. Mandal, Cole Leo, Connor Hurley","Open-source intelligence provides a stream of unstructured textual data that
can inform assessments of territorial control. We present CONTACT, a framework
for territorial control prediction using large language models (LLMs) and
minimal supervision. We evaluate two approaches: SetFit, an embedding-based
few-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a
multilingual generative LLM. Our model is trained on a small hand-labeled
dataset of news articles covering ISIS activity in Syria and Iraq, using
prompt-conditioned extraction of control-relevant signals such as military
operations, casualties, and location references. We show that the BLOOMZ-based
model outperforms the SetFit baseline, and that prompt-based supervision
improves generalization in low-resource settings. CONTACT demonstrates that
LLMs fine-tuned using few-shot methods can reduce annotation burdens and
support structured inference from open-ended OSINT streams. Our code is
available at https://github.com/PaulKMandal/CONTACT/.",http://arxiv.org/pdf/2504.13730v1,,False
OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation,18/04/2025,"Yichen Wu, Xudong Pan, Geng Hong, Min Yang","As the general capabilities of large language models (LLMs) improve and agent
applications become more widespread, the underlying deception risks urgently
require systematic evaluation and effective oversight. Unlike existing
evaluation which uses simulated games or presents limited choices, we introduce
OpenDeception, a novel deception evaluation framework with an open-ended
scenario dataset. OpenDeception jointly evaluates both the deception intention
and capabilities of LLM-based agents by inspecting their internal reasoning
process. Specifically, we construct five types of common use cases where LLMs
intensively interact with the user, each consisting of ten diverse, concrete
scenarios from the real world. To avoid ethical concerns and costs of high-risk
deceptive interactions with human testers, we propose to simulate the
multi-turn dialogue via agent simulation. Extensive evaluation of eleven
mainstream LLMs on OpenDeception highlights the urgent need to address
deception risks and security concerns in LLM-based agents: the deception
intention ratio across the models exceeds 80%, while the deception success rate
surpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do
exhibit a higher risk of deception, which calls for more alignment efforts on
inhibiting deceptive behaviors.",http://arxiv.org/pdf/2504.13707v1,,False
Task Assignment and Exploration Optimization for Low Altitude UAV Rescue via Generative AI Enhanced Multi-agent Reinforcement Learning,18/04/2025,"Xin Tang, Qian Chen, Wenjie Weng, Chao Jin, Zhang Liu, Jiacheng Wang, Geng Sun, Xiaohuan Li, Dusit Niyato","Artificial Intelligence (AI)-driven convolutional neural networks enhance
rescue, inspection, and surveillance tasks performed by low-altitude uncrewed
aerial vehicles (UAVs) and ground computing nodes (GCNs) in unknown
environments. However, their high computational demands often exceed a single
UAV's capacity, leading to system instability, further exacerbated by the
limited and dynamic resources of GCNs. To address these challenges, this paper
proposes a novel cooperation framework involving UAVs, ground-embedded robots
(GERs), and high-altitude platforms (HAPs), which enable resource pooling
through UAV-to-GER (U2G) and UAV-to-HAP (U2H) communications to provide
computing services for UAV offloaded tasks. Specifically, we formulate the
multi-objective optimization problem of task assignment and exploration
optimization in UAVs as a dynamic long-term optimization problem. Our objective
is to minimize task completion time and energy consumption while ensuring
system stability over time. To achieve this, we first employ the Lyapunov
optimization technique to transform the original problem, with stability
constraints, into a per-slot deterministic problem. We then propose an
algorithm named HG-MADDPG, which combines the Hungarian algorithm with a
generative diffusion model (GDM)-based multi-agent deep deterministic policy
gradient (MADDPG) approach. We first introduce the Hungarian algorithm as a
method for exploration area selection, enhancing UAV efficiency in interacting
with the environment. We then innovatively integrate the GDM and multi-agent
deep deterministic policy gradient (MADDPG) to optimize task assignment
decisions, such as task offloading and resource allocation. Simulation results
demonstrate the effectiveness of the proposed approach, with significant
improvements in task offloading efficiency, latency reduction, and system
stability compared to baseline methods.",http://arxiv.org/pdf/2504.13554v1,,False
SwitchMT: An Adaptive Context Switching Methodology for Scalable Multi-Task Learning in Intelligent Autonomous Agents,18/04/2025,"Avaneesh Devkota, Rachmad Vidya Wicaksana Putra, Muhammad Shafique","The ability to train intelligent autonomous agents (such as mobile robots) on
multiple tasks is crucial for adapting to dynamic real-world environments.
However, state-of-the-art reinforcement learning (RL) methods only excel in
single-task settings, and still struggle to generalize across multiple tasks
due to task interference. Moreover, real-world environments also demand the
agents to have data stream processing capabilities. Toward this, a
state-of-the-art work employs Spiking Neural Networks (SNNs) to improve
multi-task learning by exploiting temporal information in data stream, while
enabling lowpower/energy event-based operations. However, it relies on fixed
context/task-switching intervals during its training, hence limiting the
scalability and effectiveness of multi-task learning. To address these
limitations, we propose SwitchMT, a novel adaptive task-switching methodology
for RL-based multi-task learning in autonomous agents. Specifically, SwitchMT
employs the following key ideas: (1) a Deep Spiking Q-Network with active
dendrites and dueling structure, that utilizes task-specific context signals to
create specialized sub-networks; and (2) an adaptive task-switching policy that
leverages both rewards and internal dynamics of the network parameters.
Experimental results demonstrate that SwitchMT achieves superior performance in
multi-task learning compared to state-of-the-art methods. It achieves
competitive scores in multiple Atari games (i.e., Pong: -8.8, Breakout: 5.6,
and Enduro: 355.2) compared to the state-of-the-art, showing its better
generalized learning capability. These results highlight the effectiveness of
our SwitchMT methodology in addressing task interference while enabling
multi-task learning automation through adaptive task switching, thereby paving
the way for more efficient generalist agents with scalable multi-task learning
capabilities.",http://arxiv.org/pdf/2504.13541v1,,False
Integrating Locality-Aware Attention with Transformers for General Geometry PDEs,18/04/2025,"Minsu Koh, Beom-Chul Park, Heejo Kong, Seong-Whan Lee","Neural operators have emerged as promising frameworks for learning mappings
governed by partial differential equations (PDEs), serving as data-driven
alternatives to traditional numerical methods. While methods such as the
Fourier neural operator (FNO) have demonstrated notable performance, their
reliance on uniform grids restricts their applicability to complex geometries
and irregular meshes. Recently, Transformer-based neural operators with linear
attention mechanisms have shown potential in overcoming these limitations for
large-scale PDE simulations. However, these approaches predominantly emphasize
global feature aggregation, often overlooking fine-scale dynamics and localized
PDE behaviors essential for accurate solutions. To address these challenges, we
propose the Locality-Aware Attention Transformer (LA2Former), which leverages
K-nearest neighbors for dynamic patchifying and integrates global-local
attention for enhanced PDE modeling. By combining linear attention for
efficient global context encoding with pairwise attention for capturing
intricate local interactions, LA2Former achieves an optimal balance between
computational efficiency and predictive accuracy. Extensive evaluations across
six benchmark datasets demonstrate that LA2Former improves predictive accuracy
by over 50% relative to existing linear attention methods, while also
outperforming full pairwise attention under optimal conditions. This work
underscores the critical importance of localized feature learning in advancing
Transformer-based neural operators for solving PDEs on complex and irregular
domains.",http://arxiv.org/pdf/2504.13480v1,,False
Using Machine Learning and Neural Networks to Analyze and Predict Chaos in Multi-Pendulum and Chaotic Systems,18/04/2025,"Vasista Ramachandruni, Sai Hruday Reddy Nara, Geo Lalu, Sabrina Yang, Mohit Ramesh Kumar, Aarjav Jain, Pratham Mehta, Hankyu Koo, Jason Damonte, Marx Akl","A chaotic system is a highly volatile system characterized by its sensitive
dependence on initial conditions and outside factors. Chaotic systems are
prevalent throughout the world today: in weather patterns, disease outbreaks,
and even financial markets. Chaotic systems are seen in every field of science
and humanities, so being able to predict these systems is greatly beneficial to
society. In this study, we evaluate 10 different machine learning models and
neural networks [1] based on Root Mean Squared Error (RMSE) and R^2 values for
their ability to predict one of these systems, the multi-pendulum. We begin by
generating synthetic data representing the angles of the pendulum over time
using the Runge Kutta Method for solving 4th Order Differential Equations
(ODE-RK4) [2]. At first, we used the single-step sliding window approach,
predicting the 50st step after training for steps 0-49 and so forth. However,
to more accurately cover chaotic motion and behavior in these systems, we
transitioned to a time-step based approach. Here, we trained the model/network
on many initial angles and tested it on a completely new set of initial angles,
or 'in-between' to capture chaotic motion to its fullest extent. We also
evaluated the stability of the system using Lyapunov exponents. We concluded
that for a double pendulum, the best model was the Long Short Term Memory
Network (LSTM)[3] for the sliding window and time step approaches in both
friction and frictionless scenarios. For triple pendulum, the Vanilla Recurrent
Neural Network (VRNN)[4] was the best for the sliding window and Gated
Recurrent Network (GRU) [5] was the best for the time step approach, but for
friction, LSTM was the best.",http://arxiv.org/pdf/2504.13453v1,,False
Adaptive Non-local Observable on Quantum Neural Networks,18/04/2025,"Hsin-Yi Lin, Huan-Hsin Tseng, Samuel Yen-Chi Chen, Shinjae Yoo","Conventional Variational Quantum Circuits (VQCs) for Quantum Machine Learning
typically rely on a fixed Hermitian observable, often built from Pauli
operators. Inspired by the Heisenberg picture, we propose an adaptive non-local
measurement framework that substantially increases the model complexity of the
quantum circuits. Our introduction of dynamical Hermitian observables with
evolving parameters shows that optimizing VQC rotations corresponds to tracing
a trajectory in the observable space. This viewpoint reveals that standard VQCs
are merely a special case of the Heisenberg representation.
  Furthermore, we show that properly incorporating variational rotations with
non-local observables enhances qubit interaction and information mixture,
admitting flexible circuit designs. Two non-local measurement schemes are
introduced, and numerical simulations on classification tasks confirm that our
approach outperforms conventional VQCs, yielding a more powerful and
resource-efficient approach as a Quantum Neural Network.",http://arxiv.org/pdf/2504.13414v1,,False
