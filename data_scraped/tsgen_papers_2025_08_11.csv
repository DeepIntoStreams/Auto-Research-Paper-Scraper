Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls,08/08/2025,Sanket Badhe,"Large Language Models (LLMs) have demonstrated impressive fluency and
reasoning capabilities, but their potential for misuse has raised growing
concern. In this paper, we present ScamAgent, an autonomous multi-turn agent
built on top of LLMs, capable of generating highly realistic scam call scripts
that simulate real-world fraud scenarios. Unlike prior work focused on
single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts
dynamically to simulated user responses, and employs deceptive persuasion
strategies across conversational turns. We show that current LLM safety
guardrails, including refusal mechanisms and content filters, are ineffective
against such agent-based threats. Even models with strong prompt-level
safeguards can be bypassed when prompts are decomposed, disguised, or delivered
incrementally within an agent framework. We further demonstrate the
transformation of scam scripts into lifelike voice calls using modern
text-to-speech systems, completing a fully automated scam pipeline. Our
findings highlight an urgent need for multi-turn safety auditing, agent-level
control frameworks, and new methods to detect and disrupt conversational
deception powered by generative AI.",http://arxiv.org/pdf/2508.06457v1,,False
Echoes of Automation: The Increasing Use of LLMs in Newsmaking,08/08/2025,"Abolfazl Ansari, Delvin Ce Zhang, Nafis Irtiza Tripto, Dongwon Lee","The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns
for journalistic integrity and authorship. This study examines AI-generated
content across over 40,000 news articles from major, local, and college news
media, in various media formats. Using three advanced AI-text detectors (e.g.,
Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of
GenAI use in recent years, especially in local and college news. Sentence-level
analysis reveals LLMs are often used in the introduction of news, while
conclusions usually written manually. Linguistic analysis shows GenAI boosts
word richness and readability but lowers formality, leading to more uniform
writing styles, particularly in local media.",http://arxiv.org/pdf/2508.06445v1,,False
Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation,08/08/2025,"Youguang Xing, Xu Luo, Junlin Xie, Lianli Gao, Hengtao Shen, Jingkuan Song","Generalist robot policies trained on large-scale datasets such as Open
X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.
However, they often struggle to generalize beyond the distribution of their
training data. In this paper, we investigate the underlying cause of this
limited generalization capability. We identify shortcut learning -- the
reliance on task-irrelevant features -- as a key impediment to generalization.
Through comprehensive theoretical and empirical analysis, we uncover two
primary contributors to shortcut learning: (1) limited diversity within
individual sub-datasets, and (2) significant distributional disparities across
sub-datasets, leading to dataset fragmentation. These issues arise from the
inherent structure of large-scale datasets like OXE, which are typically
composed of multiple sub-datasets collected independently across varied
environments and embodiments. Our findings provide critical insights into
dataset collection strategies that can reduce shortcut learning and enhance the
generalization ability of generalist robot policies. Moreover, in scenarios
where acquiring new large-scale data is impractical, we demonstrate that
carefully selected robotic data augmentation strategies can effectively reduce
shortcut learning in existing offline datasets, thereby improving
generalization capabilities of generalist robot policies, e.g., $\pi_0$, in
both simulation and real-world environments. More information at
https://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.",http://arxiv.org/pdf/2508.06426v1,,False
Tree-Based Deep Learning for Ranking Symbolic Integration Algorithms,08/08/2025,"Rashid Barket, Matthew England, Jürgen Gerhard","Symbolic indefinite integration in Computer Algebra Systems such as Maple
involves selecting the most effective algorithm from multiple available
methods. Not all methods will succeed for a given problem, and when several do,
the results, though mathematically equivalent, can differ greatly in
presentation complexity. Traditionally, this choice has been made with minimal
consideration of the problem instance, leading to inefficiencies.
  We present a machine learning (ML) approach using tree-based deep learning
models within a two-stage architecture: first identifying applicable methods
for a given instance, then ranking them by predicted output complexity.
Furthermore, we find representing mathematical expressions as tree structures
significantly improves performance over sequence-based representations, and our
two-stage framework outperforms alternative ML formulations.
  Using a diverse dataset generated by six distinct data generators, our models
achieve nearly 90% accuracy in selecting the optimal method on a 70,000 example
holdout test set. On an independent out-of-distribution benchmark from Maple's
internal test suite, our tree transformer model maintains strong
generalisation, outperforming Maple's built-in selector and prior ML
approaches.
  These results highlight the critical role of data representation and problem
framing in ML for symbolic computation, and we expect our methodology to
generalise effectively to similar optimisation problems in mathematical
software.",http://arxiv.org/pdf/2508.06383v1,,False
DP-SPRT: Differentially Private Sequential Probability Ratio Tests,08/08/2025,"Thomas Michel, Debabrota Basu, Emilie Kaufmann","We revisit Wald's celebrated Sequential Probability Ratio Test for sequential
tests of two simple hypotheses, under privacy constraints. We propose DP-SPRT,
a wrapper that can be calibrated to achieve desired error probabilities and
privacy constraints, addressing a significant gap in previous work. DP-SPRT
relies on a private mechanism that processes a sequence of queries and stops
after privately determining when the query results fall outside a predefined
interval. This OutsideInterval mechanism improves upon naive composition of
existing techniques like AboveThreshold, potentially benefiting other
sequential algorithms. We prove generic upper bounds on the error and sample
complexity of DP-SPRT that can accommodate various noise distributions based on
the practitioner's privacy needs. We exemplify them in two settings: Laplace
noise (pure Differential Privacy) and Gaussian noise (R\'enyi differential
privacy). In the former setting, by providing a lower bound on the sample
complexity of any $\epsilon$-DP test with prescribed type I and type II errors,
we show that DP-SPRT is near optimal when both errors are small and the two
hypotheses are close. Moreover, we conduct an experimental study revealing its
good practical performance.",http://arxiv.org/pdf/2508.06377v1,,False
Structural Equation-VAE: Disentangled Latent Representations for Tabular Data,08/08/2025,"Ruiyu Zhang, Ce Zhao, Xin Zhao, Lin Nie, Wai-Fung Lam","Learning interpretable latent representations from tabular data remains a
challenge in deep generative modeling. We introduce SE-VAE (Structural
Equation-Variational Autoencoder), a novel architecture that embeds measurement
structure directly into the design of a variational autoencoder. Inspired by
structural equation modeling, SE-VAE aligns latent subspaces with known
indicator groupings and introduces a global nuisance latent to isolate
construct-specific confounding variation. This modular architecture enables
disentanglement through design rather than through statistical regularizers
alone. We evaluate SE-VAE on a suite of simulated tabular datasets and
benchmark its performance against a series of leading baselines using standard
disentanglement metrics. SE-VAE consistently outperforms alternatives in factor
recovery, interpretability, and robustness to nuisance variation. Ablation
results reveal that architectural structure, rather than regularization
strength, is the key driver of performance. SE-VAE offers a principled
framework for white-box generative modeling in scientific and social domains
where latent constructs are theory-driven and measurement validity is
essential.",http://arxiv.org/pdf/2508.06347v1,,False
Decorrelated feature importance from local sample weighting,08/08/2025,"Benedikt Fröhlich, Alison Durst, Merle Behr","Feature importance (FI) statistics provide a prominent and valuable method of
insight into the decision process of machine learning (ML) models, but their
effectiveness has well-known limitations when correlation is present among the
features in the training data. In this case, the FI often tends to be
distributed among all features which are in correlation with the
response-generating signal features. Even worse, if multiple signal features
are in strong correlation with a noise feature, while being only modestly
correlated with one another, this can result in a noise feature having a
distinctly larger FI score than any signal feature. Here we propose local
sample weighting (losaw) which can flexibly be integrated into many ML
algorithms to improve FI scores in the presence of feature correlation in the
training data. Our approach is motivated from inverse probability weighting in
causal inference and locally, within the ML model, uses a sample weighting
scheme to decorrelate a target feature from the remaining features. This
reduces model bias locally, whenever the effect of a potential signal feature
is evaluated and compared to others. Moreover, losaw comes with a natural
tuning parameter, the minimum effective sample size of the weighted population,
which corresponds to an interpretation-prediction-tradeoff, analog to a
bias-variance-tradeoff as for classical ML tuning parameters. We demonstrate
how losaw can be integrated within decision tree-based ML methods and within
mini-batch training of neural networks. We investigate losaw for random forest
and convolutional neural networks in a simulation study on settings showing
diverse correlation patterns. We found that losaw improves FI consistently.
Moreover, it often improves prediction accuracy for out-of-distribution, while
maintaining a similar accuracy for in-distribution test data.",http://arxiv.org/pdf/2508.06337v1,,False
LLM Serving Optimization with Variable Prefill and Decode Lengths,08/08/2025,"Meixuan Wang, Yinyu Ye, Zijie Zhou","We study the problem of serving LLM (Large Language Model) requests where
each request has heterogeneous prefill and decode lengths. In LLM serving, the
prefill length corresponds to the input prompt length, which determines the
initial memory usage in the KV cache. The decode length refers to the number of
output tokens generated sequentially, with each additional token increasing the
KV cache memory usage by one unit. Given a set of n requests, our goal is to
schedule and process them to minimize the total completion time. We show that
this problem is NP-hard due to the interplay of batching, placement
constraints, precedence relationships, and linearly increasing memory usage. We
then analyze commonly used scheduling strategies in practice, such as
First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their
competitive ratios scale up sublinearly with the memory limit-a significant
drawback in real-world settings where memory demand is large. To address this,
we propose a novel algorithm based on a new selection metric that efficiently
forms batches over time. We prove that this algorithm achieves a constant
competitive ratio. Finally, we develop and evaluate a few algorithm variants
inspired by this approach, including dynamic programming variants, local search
methods, and an LP-based scheduler, demonstrating through comprehensive
simulations that they outperform standard baselines while maintaining
computational efficiency.",http://arxiv.org/pdf/2508.06133v1,,False
Enhancing the Scalability of Classical Surrogates for Real-World Quantum Machine Learning Applications,08/08/2025,"Philip Anton Hernicht, Alona Sakhnenko, Corey O'Meara, Giorgio Cortiana, Jeanette Miriam Lorenz","Quantum machine learning (QML) presents potential for early industrial
adoption, yet limited access to quantum hardware remains a significant
bottleneck for deployment of QML solutions. This work explores the use of
classical surrogates to bypass this restriction, which is a technique that
allows to build a lightweight classical representation of a (trained) quantum
model, enabling to perform inference on entirely classical devices. We reveal
prohibiting high computational demand associated with previously proposed
methods for generating classical surrogates from quantum models, and propose an
alternative pipeline enabling generation of classical surrogates at a larger
scale than was previously possible. Previous methods required at least a
high-performance computing (HPC) system for quantum models of below industrial
scale (ca. 20 qubits), which raises questions about its practicality. We
greatly minimize the redundancies of the previous approach, utilizing only a
minute fraction of the resources previously needed. We demonstrate the
effectiveness of our method on a real-world energy demand forecasting problem,
conducting rigorous testing of performance and computation demand in both
simulations and on quantum hardware. Our results indicate that our method
achieves high accuracy on the testing dataset while its computational resource
requirements scale linearly rather than exponentially. This work presents a
lightweight approach to transform quantum solutions into classically deployable
versions, facilitating faster integration of quantum technology in industrial
settings. Furthermore, it can serve as a powerful research tool in search
practical quantum advantage in an empirical setup.",http://arxiv.org/pdf/2508.06131v1,,False
Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention,08/08/2025,"Shree Mitra, Ritabrata Chakraborty, Nilkanta Sahu","Recognizing handwritten mathematical expressions (HMER) is a challenging task
due to the inherent two-dimensional structure, varying symbol scales, and
complex spatial relationships among symbols. In this paper, we present a
self-supervised learning (SSL) framework for HMER that eliminates the need for
expensive labeled data. Our approach begins by pretraining an image encoder
using a combination of global and local contrastive loss, enabling the model to
learn both holistic and fine-grained representations. A key contribution of
this work is a novel self-supervised attention network, which is trained using
a progressive spatial masking strategy. This attention mechanism is designed to
learn semantically meaningful focus regions, such as operators, exponents, and
nested mathematical notation, without requiring any supervision. The
progressive masking curriculum encourages the network to become increasingly
robust to missing or occluded visual information, ultimately improving
structural understanding. Our complete pipeline consists of (1) self-supervised
pretraining of the encoder, (2) self-supervised attention learning, and (3)
supervised fine-tuning with a transformer decoder to generate LATEX sequences.
Extensive experiments on CROHME benchmarks demonstrate that our method
outperforms existing SSL and fully supervised baselines, validating the
effectiveness of our progressive attention mechanism in enhancing HMER
performance. Our codebase can be found here.",http://arxiv.org/pdf/2508.06107v1,,False
Bounding Distributional Shifts in World Modeling through Novelty Detection,08/08/2025,"Eric Jing, Abdeslam Boularias","Recent work on visual world models shows significant promise in latent state
dynamics obtained from pre-trained image backbones. However, most of the
current approaches are sensitive to training quality, requiring near-complete
coverage of the action and state space during training to prevent divergence
during inference. To make a model-based planning algorithm more robust to the
quality of the learned world model, we propose in this work to use a
variational autoencoder as a novelty detector to ensure that proposed action
trajectories during planning do not cause the learned model to deviate from the
training data distribution. To evaluate the effectiveness of this approach, a
series of experiments in challenging simulated robot environments was carried
out, with the proposed method incorporated into a model-predictive control
policy loop extending the DINO-WM architecture. The results clearly show that
the proposed method improves over state-of-the-art solutions in terms of data
efficiency.",http://arxiv.org/pdf/2508.06096v1,,False
Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology,08/08/2025,"Barak Gahtan, Alex M. Bronstein","Deep temporal architectures such as Temporal Convolutional Networks (TCNs)
achieve strong predictive performance on sequential data, yet theoretical
understanding of their generalization remains limited. We address this gap by
providing both the first non-vacuous, architecture-aware generalization bounds
for deep temporal models and a principled evaluation methodology.
  For exponentially $\beta$-mixing sequences, we derive bounds scaling as $
O\!\Bigl(R\,\sqrt{\tfrac{D\,p\,n\,\log N}{N}}\Bigr), $ where $D$ is network
depth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our
delayed-feedback blocking mechanism transforms dependent samples into
effectively independent ones while discarding only $O(1/\log N)$ of the data,
yielding $\sqrt{D}$ scaling instead of exponential, implying that doubling
depth requires approximately quadrupling the training data.
  We also introduce a fair-comparison methodology that fixes the effective
sample size to isolate the effect of temporal structure from information
content. Under $N_{\text{eff}}=2{,}000$, strongly dependent sequences
($\rho=0.8$) exhibit $\approx76\%$ smaller generalization gaps than weakly
dependent ones ($\rho=0.2$), challenging the intuition that dependence is
purely detrimental. Yet convergence rates diverge from theory: weak
dependencies follow $N_{\text{eff}}^{-1.21}$ scaling and strong dependencies
follow $N_{\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.
These findings reveal that temporal dependence can enhance learning under fixed
information budgets, while highlighting gaps between theory and practice that
motivate future research.",http://arxiv.org/pdf/2508.06066v1,,False
Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning,08/08/2025,"Daechul Ahn, San Kim, Jonghyun Choi","Large Language Models (LLMs) have recently demonstrated impressive action
sequence prediction capabilities but often struggle with dynamic, long-horizon
tasks such as real-time strategic games. In a game such as StarCraftII (SC2),
agents need to manage resource constraints and adapt to evolving battlefield
situations in a partially observable environment. This often overwhelms
exisiting LLM-based approaches. To address these challenges, we propose a
hierarchical multi-agent framework that employs specialized imitation learning
agents under a meta-controller called Strategic Planner (SP). By expert
demonstrations, each specialized agent learns a distinctive strategy, such as
aerial support or defensive maneuvers, and produces coherent, structured
multistep action sequences. The SP then orchestrates these proposals into a
single, environmentally adaptive plan that ensures local decisions aligning
with long-term strategies. We call this HIMA (Hierarchical Imitation
Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that
encompasses all race match combinations in SC2. Our empirical results show that
HIMA outperforms state of the arts in strategic clarity, adaptability, and
computational efficiency, underscoring the potential of combining specialized
imitation modules with meta-level orchestration to develop more robust,
general-purpose AI agents.",http://arxiv.org/pdf/2508.06042v1,,False
Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning,08/08/2025,"Wei Xiang, Ziyue Lei, Haoyuan Che, Fangyuan Ye, Xueting Wu, Lingyun Sun","Operational skill learning, inherently physical and reliant on hands-on
practice and kinesthetic feedback, has yet to be effectively replicated in
large language model (LLM)-supported training. Current LLM training assistants
primarily generate customized textual feedback, neglecting the crucial
kinesthetic modality. This gap derives from the textual and uncertain nature of
LLMs, compounded by concerns on user acceptance of LLM driven body control. To
bridge this gap and realize the potential of collaborative human-LLM action,
this work explores human experience of LLM driven kinesthetic assistance.
Specifically, we introduced an ""Align-Analyze-Adjust"" strategy and developed
FlightAxis, a tool that integrates LLM with Electrical Muscle Stimulation (EMS)
for flight skill acquisition, a representative operational skill domain.
FlightAxis learns flight skills from manuals and guides forearm movements
during simulated flight tasks. Our results demonstrate high user acceptance of
LLM-mediated body control and significantly reduced task completion times.
Crucially, trainees reported that this kinesthetic assistance enhanced their
awareness of operation flaws and fostered increased engagement in the training
process, rather than relieving perceived load. This work demonstrated the
potential of kinesthetic LLM training in operational skill acquisition.",http://arxiv.org/pdf/2508.06000v1,,False
Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization,08/08/2025,"Fei Xu Yu, Gina Adam, Nathaniel D. Bastian, Tian Lan","Large language models (LLMs) have demonstrated remarkable capabilities in
code generation and structured reasoning; however, their performance often
degrades on complex tasks that require consistent multi-step planning. Recent
work has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet
existing approaches primarily focus on generating heuristic-based code for
optimization or target simpler tasks where correctness alone is sufficient. In
this work, we propose MCTS-OPS, a novel neural-symbolic framework that
formulates prompt selection as a sequential decision process guided by MCTS.
Our method explores and refines multi-step prompt sequences for the goal of
improving code generation quality and enhancing the problem-solving
capabilities of LLMs in general optimization. Experiments on network
optimization show significant improvement over the baselines, both in the
success rate of executing the generated code and in the optimization results
with the specified objective and constraints (2$\sim$4$\times$ higher reward
and 3$\times$ lower standard deviation). Moreover, it improves the chance of
attaining the optimal solution by about 10\% of cases, compared to baseline
methods in hard problems. These results highlight the promise of combining
symbolic planning with LLMs for robust, high-quality code generation in complex
domains.",http://arxiv.org/pdf/2508.05995v1,,False
