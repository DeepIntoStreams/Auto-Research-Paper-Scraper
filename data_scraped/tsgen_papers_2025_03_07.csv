Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning,06/03/2025,"Pranjal Aggarwal, Sean Welleck","Reasoning language models have shown an uncanny ability to improve
performance at test-time by ``thinking longer''-that is, by generating longer
chain-of-thought sequences and hence using more compute. However, the length of
their chain-of-thought reasoning is not controllable, making it impossible to
allocate test-time compute to achieve a desired level of performance. We
introduce Length Controlled Policy Optimization (LCPO), a simple reinforcement
learning method that optimizes for accuracy and adherence to user-specified
length constraints. We use LCPO to train L1, a reasoning language model that
produces outputs satisfying a length constraint given in its prompt. L1's
length control allows for smoothly trading off computational cost and accuracy
on a wide range of tasks, and outperforms the state-of-the-art S1 method for
length control. Furthermore, we uncover an unexpected short chain-of-thought
capability in models trained with LCPO. For instance, our 1.5B L1 model
surpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise
control over reasoning length, allowing for fine-grained allocation of
test-time compute and accuracy. We release code and models at
https://www.cmu-l3.github.io/l1",http://arxiv.org/pdf/2503.04697v1,,False
Coarse graining and reduced order models for plume ejection dynamics,06/03/2025,"Ike Griss Salas, Megan R. Ebers, Jake Stevens-Haas, J. Nathan Kutz","Monitoring the atmospheric dispersion of pollutants is increasingly critical
for environmental impact assessments. High-fidelity computational models are
often employed to simulate plume dynamics, guiding decision-making and
prioritizing resource deployment. However, such models can be prohibitively
expensive to simulate, as they require resolving turbulent flows at fine
spatial and temporal resolutions. Moreover, there are at least two distinct
dynamical regimes of interest in the plume: (i) the initial ejection of the
plume where turbulent mixing is generated by the shear-driven Kelvin-Helmholtz
instability, and (ii) the ensuing turbulent diffusion and advection which is
often modeled by the Gaussian plume model. We address the challenge of modeling
the initial plume generation. Specifically, we propose a data-driven framework
that identifies a reduced-order analytical model for plume dynamics -- directly
from video data. We extract a time series of plume center and edge points from
video snapshots and evaluate different regressions based to their extrapolation
performance to generate a time series of coefficients that characterize the
plume's overall direction and spread. We regress to a sinusoidal model inspired
by the Kelvin-Helmholtz instability for the edge points in order to identify
the plume's dispersion and vorticity. Overall, this reduced-order modeling
framework provides a data-driven and lightweight approach to capture the
dominant features of the initial nonlinear point-source plume dynamics,
agnostic to plume type and starting only from video. The resulting model is a
pre-cursor to standard models such as the Gaussian plume model and has the
potential to enable rapid assessment and evaluation of critical environmental
hazards, such as methane leaks, chemical spills, and pollutant dispersal from
smokestacks.",http://arxiv.org/pdf/2503.04690v1,,False
Multi-Agent Inverse Q-Learning from Demonstrations,06/03/2025,"Nathaniel Haynam, Adam Khoja, Dhruv Kumar, Vivek Myers, Erdem Bıyık","When reward functions are hand-designed, deep reinforcement learning
algorithms often suffer from reward misspecification, causing them to learn
suboptimal policies in terms of the intended task objectives. In the
single-agent case, inverse reinforcement learning (IRL) techniques attempt to
address this issue by inferring the reward function from expert demonstrations.
However, in multi-agent problems, misalignment between the learned and true
objectives is exacerbated due to increased environment non-stationarity and
variance that scales with multiple agents. As such, in multi-agent general-sum
games, multi-agent IRL algorithms have difficulty balancing cooperative and
competitive objectives. To address these issues, we propose Multi-Agent
Marginal Q-Learning from Demonstrations (MAMQL), a novel sample-efficient
framework for multi-agent IRL. For each agent, MAMQL learns a critic
marginalized over the other agents' policies, allowing for a well-motivated use
of Boltzmann policies in the multi-agent context. We identify a connection
between optimal marginalized critics and single-agent soft-Q IRL, allowing us
to apply a direct, simple optimization criterion from the single-agent domain.
Across our experiments on three different simulated domains, MAMQL
significantly outperforms previous multi-agent methods in average reward,
sample efficiency, and reward recovery by often more than 2-5x. We make our
code available at https://sites.google.com/view/mamql .",http://arxiv.org/pdf/2503.04679v1,,False
CLDyB: Towards Dynamic Benchmarking for Continual Learning with Pre-trained Models,06/03/2025,"Shengzhuang Chen, Yikai Liao, Xiaoxiao Sun, Kede Ma, Ying Wei","The advent of the foundation model era has sparked significant research
interest in leveraging pre-trained representations for continual learning (CL),
yielding a series of top-performing CL methods on standard evaluation
benchmarks. Nonetheless, there are growing concerns regarding potential data
contamination during the pre-training stage. Furthermore, standard evaluation
benchmarks, which are typically static, fail to capture the complexities of
real-world CL scenarios, resulting in saturated performance. To address these
issues, we describe CL on dynamic benchmarks (CLDyB), a general computational
framework based on Markov decision processes for evaluating CL methods
reliably. CLDyB dynamically identifies inherently difficult and
algorithm-dependent tasks for the given CL methods, and determines challenging
task orders using Monte Carlo tree search. Leveraging CLDyB, we first conduct a
joint evaluation of multiple state-of-the-art CL methods, leading to a set of
commonly challenging and generalizable task sequences where existing CL methods
tend to perform poorly. We then conduct separate evaluations of individual CL
methods using CLDyB, discovering their respective strengths and weaknesses. The
source code and generated task sequences are publicly accessible at
https://github.com/szc12153/CLDyB.",http://arxiv.org/pdf/2503.04655v1,,False
Simulating the Real World: A Unified Survey of Multimodal Generative Models,06/03/2025,"Yuqi Hu, Longguang Wang, Xian Liu, Ling-Hao Chen, Yuwei Guo, Yukai Shi, Ce Liu, Anyi Rao, Zeyu Wang, Hui Xiong","Understanding and replicating the real world is a critical challenge in
Artificial General Intelligence (AGI) research. To achieve this, many existing
approaches, such as world models, aim to capture the fundamental principles
governing the physical world, enabling more accurate simulations and meaningful
interactions. However, current methods often treat different modalities,
including 2D (images), videos, 3D, and 4D representations, as independent
domains, overlooking their interdependencies. Additionally, these methods
typically focus on isolated dimensions of reality without systematically
integrating their connections. In this survey, we present a unified survey for
multimodal generative models that investigate the progression of data
dimensionality in real-world simulation. Specifically, this survey starts from
2D generation (appearance), then moves to video (appearance+dynamics) and 3D
generation (appearance+geometry), and finally culminates in 4D generation that
integrate all dimensions. To the best of our knowledge, this is the first
attempt to systematically unify the study of 2D, video, 3D and 4D generation
within a single framework. To guide future research, we provide a comprehensive
review of datasets, evaluation metrics and future directions, and fostering
insights for newcomers. This survey serves as a bridge to advance the study of
multimodal generative models and real-world simulation within a unified
framework.",http://arxiv.org/pdf/2503.04641v1,,False
Enhancing SAM with Efficient Prompting and Preference Optimization for Semi-supervised Medical Image Segmentation,06/03/2025,"Aishik Konwer, Zhijian Yang, Erhan Bas, Cao Xiao, Prateek Prasanna, Parminder Bhatia, Taha Kass-Hout","Foundational models such as the Segment Anything Model (SAM) are gaining
traction in medical imaging segmentation, supporting multiple downstream tasks.
However, such models are supervised in nature, still relying on large annotated
datasets or prompts supplied by experts. Conventional techniques such as active
learning to alleviate such limitations are limited in scope and still
necessitate continuous human involvement and complex domain knowledge for label
refinement or establishing reward ground truth. To address these challenges, we
propose an enhanced Segment Anything Model (SAM) framework that utilizes
annotation-efficient prompts generated in a fully unsupervised fashion, while
still capturing essential semantic, location, and shape information through
contrastive language-image pretraining and visual question answering. We adopt
the direct preference optimization technique to design an optimal policy that
enables the model to generate high-fidelity segmentations with simple ratings
or rankings provided by a virtual annotator simulating the human annotation
process. State-of-the-art performance of our framework in tasks such as lung
segmentation, breast tumor segmentation, and organ segmentation across various
modalities, including X-ray, ultrasound, and abdominal CT, justifies its
effectiveness in low-annotation data scenarios.",http://arxiv.org/pdf/2503.04639v1,,False
The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation,06/03/2025,"Aoxiong Yin, Kai Shen, Yichong Leng, Xu Tan, Xinyu Zhou, Juncheng Li, Siliang Tang","Recent advancements in text-to-video (T2V) generation have been driven by two
competing paradigms: autoregressive language models and diffusion models.
However, each paradigm has intrinsic limitations: language models struggle with
visual quality and error accumulation, while diffusion models lack semantic
understanding and causal modeling. In this work, we propose LanDiff, a hybrid
framework that synergizes the strengths of both paradigms through
coarse-to-fine generation. Our architecture introduces three key innovations:
(1) a semantic tokenizer that compresses 3D visual features into compact 1D
discrete representations through efficient semantic compression, achieving a
$\sim$14,000$\times$ compression ratio; (2) a language model that generates
semantic tokens with high-level semantic relationships; (3) a streaming
diffusion model that refines coarse semantics into high-fidelity videos.
Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the
VBench T2V benchmark, surpassing the state-of-the-art open-source models
Hunyuan Video (13B) and other commercial models such as Sora, Keling, and
Hailuo. Furthermore, our model also achieves state-of-the-art performance in
long video generation, surpassing other open-source models in this field. Our
demo can be viewed at https://landiff.github.io/.",http://arxiv.org/pdf/2503.04606v1,,False
Fusion of Various Optimization Based Feature Smoothing Methods for Wearable and Non-invasive Blood Glucose Estimation,06/03/2025,"Yiting Wei, Bingo Wing-Kuen Ling, Danni Chen, Yuheng Dai, Qing Liu","Recently, the wearable and non-invasive blood glucose estimation approach has
been proposed. However, due to the unreliability of the acquisition device, the
presence of the noise and the variations of the acquisition environments, the
obtained features and the reference blood glucose values are highly unreliable.
To address this issue, this paper proposes a polynomial fitting approach to
smooth the obtained features or the reference blood glucose values. First, the
blood glucose values are estimated based on the individual optimization
approaches. Second, the absolute difference values between the estimated blood
glucose values and the actual blood glucose values based on each optimization
approach are computed. Third, these absolute difference values for each
optimization approach are sorted in the ascending order. Fourth, for each
sorted blood glucose value, the optimization method corresponding to the
minimum absolute difference value is selected. Fifth, the accumulate
probability of each selected optimization method is computed. If the accumulate
probability of any selected optimization method at a point is greater than a
threshold value, then the accumulate probabilities of these three selected
optimization methods at that point are reset to zero. A range of the sorted
blood glucose values are defined as that with the corresponding boundaries
points being the previous reset point and this reset point. Hence, after
performing the above procedures for all the sorted reference blood glucose
values in the validation set, the regions of the sorted reference blood glucose
values and the corresponding optimization methods in these regions are
determined. The computer numerical simulation results show that our proposed
method yields the mean absolute relative deviation (MARD) at 0.0930 and the
percentage of the test data falling in the zone A of the Clarke error grid at
94.1176%.",http://arxiv.org/pdf/2503.03770v1,10.1049/syb2.12063,False
Time-varying Factor Augmented Vector Autoregression with Grouped Sparse Autoencoder,06/03/2025,"Yiyong Luo, Brooks Paige, Jim Griffin","Recent economic events, including the global financial crisis and COVID-19
pandemic, have exposed limitations in linear Factor Augmented Vector
Autoregressive (FAVAR) models for forecasting and structural analysis.
Nonlinear dimension techniques, particularly autoencoders, have emerged as
promising alternatives in a FAVAR framework, but challenges remain in
identifiability, interpretability, and integration with traditional nonlinear
time series methods. We address these challenges through two contributions.
First, we introduce a Grouped Sparse autoencoder that employs the
Spike-and-Slab Lasso prior, with parameters under this prior being shared
across variables of the same economic category, thereby achieving
semi-identifiability and enhancing model interpretability. Second, we
incorporate time-varying parameters into the VAR component to better capture
evolving economic dynamics. Our empirical application to the US economy
demonstrates that the Grouped Sparse autoencoder produces more interpretable
factors through its parsimonious structure; and its combination with
time-varying parameter VAR shows superior performance in both point and density
forecasting. Impulse response analysis reveals that monetary policy shocks
during recessions generate more moderate responses with higher uncertainty
compared to expansionary periods.",http://arxiv.org/pdf/2503.04386v1,,False
scDD: Latent Codes Based scRNA-seq Dataset Distillation with Foundation Model Knowledge,06/03/2025,"Zhen Yu, Jianan Han, Yang Liu, Qingchao Chen","Single-cell RNA sequencing (scRNA-seq) technology has profiled hundreds of
millions of human cells across organs, diseases, development and perturbations
to date. However, the high-dimensional sparsity, batch effect noise, category
imbalance, and ever-increasing data scale of the original sequencing data pose
significant challenges for multi-center knowledge transfer, data fusion, and
cross-validation between scRNA-seq datasets. To address these barriers, (1) we
first propose a latent codes-based scRNA-seq dataset distillation framework
named scDD, which transfers and distills foundation model knowledge and
original dataset information into a compact latent space and generates
synthetic scRNA-seq dataset by a generator to replace the original dataset.
Then, (2) we propose a single-step conditional diffusion generator named SCDG,
which perform single-step gradient back-propagation to help scDD optimize
distillation quality and avoid gradient decay caused by multi-step
back-propagation. Meanwhile, SCDG ensures the scRNA-seq data characteristics
and inter-class discriminability of the synthetic dataset through flexible
conditional control and generation quality assurance. Finally, we propose a
comprehensive benchmark to evaluate the performance of scRNA-seq dataset
distillation in different data analysis tasks. It is validated that our
proposed method can achieve 7.61% absolute and 15.70% relative improvement over
previous state-of-the-art methods on average task.",http://arxiv.org/pdf/2503.04357v1,,False
Towards Autonomous Reinforcement Learning for Real-World Robotic Manipulation with Large Language Models,06/03/2025,"Niccolò Turcato, Matteo Iovino, Aris Synodinos, Alberto Dalla Libera, Ruggero Carli, Pietro Falco","Recent advancements in Large Language Models (LLMs) and Visual Language
Models (VLMs) have significantly impacted robotics, enabling high-level
semantic motion planning applications. Reinforcement Learning (RL), a
complementary paradigm, enables agents to autonomously optimize complex
behaviors through interaction and reward signals. However, designing effective
reward functions for RL remains challenging, especially in real-world tasks
where sparse rewards are insufficient and dense rewards require elaborate
design. In this work, we propose Autonomous Reinforcement learning for Complex
HumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,
a pre-trained LLM, to generate reward functions directly from natural language
task descriptions. The rewards are used to train RL agents in simulated
environments, where we formalize the reward generation process to enhance
feasibility. Additionally, GPT-4 automates the coding of task success criteria,
creating a fully automated, one-shot procedure for translating human-readable
text into deployable robot skills. Our approach is validated through extensive
simulated experiments on single-arm and bi-manual manipulation tasks using an
ABB YuMi collaborative robot, highlighting its practicality and effectiveness.
Tasks are demonstrated on the real robot setup.",http://arxiv.org/pdf/2503.04280v1,,False
Knowledge Retention for Continual Model-Based Reinforcement Learning,06/03/2025,"Yixiang Sun, Haotian Fu, Michael Littman, George Konidaris","We propose DRAGO, a novel approach for continual model-based reinforcement
learning aimed at improving the incremental development of world models across
a sequence of tasks that differ in their reward functions but not the state
space or dynamics. DRAGO comprises two key components: Synthetic Experience
Rehearsal, which leverages generative models to create synthetic experiences
from past tasks, allowing the agent to reinforce previously learned dynamics
without storing data, and Regaining Memories Through Exploration, which
introduces an intrinsic reward mechanism to guide the agent toward revisiting
relevant states from prior tasks. Together, these components enable the agent
to maintain a comprehensive and continually developing world model,
facilitating more effective learning and adaptation across diverse
environments. Empirical evaluations demonstrate that DRAGO is able to preserve
knowledge across tasks, achieving superior performance in various continual
learning scenarios.",http://arxiv.org/pdf/2503.04256v1,,False
Hedging with Sparse Reward Reinforcement Learning,06/03/2025,"Yiheng Ding, Gangnan Yuan, Dewei Zuo, Ting Gao","Derivatives, as a critical class of financial instruments, isolate and trade
the price attributes of risk assets such as stocks, commodities, and indices,
aiding risk management and enhancing market efficiency. However, traditional
hedging models, constrained by assumptions such as continuous trading and zero
transaction costs, fail to satisfy risk control requirements in complex and
uncertain real-world markets.
  With advances in computing technology and deep learning, data-driven trading
strategies are becoming increasingly prevalent. This thesis proposes a
derivatives hedging framework integrating deep learning and reinforcement
learning. The framework comprises a probabilistic forecasting model and a
hedging agent, enabling market probability prediction, derivative pricing, and
hedging.
  Specifically, we design a spatiotemporal attention-based probabilistic
financial time series forecasting Transformer to address the scarcity of
derivatives hedging data. A low-rank attention mechanism compresses
high-dimensional assets into a low-dimensional latent space, capturing
nonlinear asset relationships. The Transformer models sequential dependencies
within this latent space, improving market probability forecasts and
constructing an online training environment for downstream hedging tasks.
  Additionally, we incorporate generalized geometric Brownian motion to develop
a risk-neutral pricing approach for derivatives. We model derivatives hedging
as a reinforcement learning problem with sparse rewards and propose a behavior
cloning-based recurrent proximal policy optimization (BC-RPPO) algorithm. This
pretraining-finetuning framework significantly enhances the hedging agent's
performance. Numerical experiments in the U.S. and Chinese financial markets
demonstrate our method's superiority over traditional approaches.",http://arxiv.org/pdf/2503.04218v1,,False
Unsupervised anomaly detection on cybersecurity data streams: a case with BETH dataset,06/03/2025,Evgeniy Eremin,"In modern world the importance of cybersecurity of various systems is
increasing from year to year. The number of information security events
generated by information security tools grows up with the development of the IT
infrastructure. At the same time, the cyber threat landscape does not remain
constant, and monitoring should take into account both already known attack
indicators and those for which there are no signature rules in information
security products of various classes yet. Detecting anomalies in large
cybersecurity data streams is a complex task that, if properly addressed, can
allow for timely response to atypical and previously unknown cyber threats. The
possibilities of using of offline algorithms may be limited for a number of
reasons related to the time of training and the frequency of retraining. Using
stream learning algorithms for solving this task is capable of providing
near-real-time data processing. This article examines the results of ten
algorithms from three Python stream machine-learning libraries on BETH dataset
with cybersecurity events, which contains information about the creation,
cloning, and destruction of operating system processes collected using extended
eBPF. ROC-AUC metric and total processing time of processing with these
algorithms are presented. Several combinations of features and the order of
events are considered. In conclusion, some mentions are given about the most
promising algorithms and possible directions for further research are outlined.",http://arxiv.org/pdf/2503.04178v1,,False
CoFinDiff: Controllable Financial Diffusion Model for Time Series Generation,06/03/2025,"Yuki Tanaka, Ryuji Hashimoto, Takehiro Takayanagi, Zhe Piao, Yuri Murayama, Kiyoshi Izumi","The generation of synthetic financial data is a critical technology in the
financial domain, addressing challenges posed by limited data availability.
Traditionally, statistical models have been employed to generate synthetic
data. However, these models fail to capture the stylized facts commonly
observed in financial data, limiting their practical applicability. Recently,
machine learning models have been introduced to address the limitations of
statistical models; however, controlling synthetic data generation remains
challenging. We propose CoFinDiff (Controllable Financial Diffusion model), a
synthetic financial data generation model based on conditional diffusion models
that accept conditions about the synthetic time series. By incorporating
conditions derived from price data into the conditional diffusion model via
cross-attention, CoFinDiff learns the relationships between the conditions and
the data, generating synthetic data that align with arbitrary conditions.
Experimental results demonstrate that: (i) synthetic data generated by
CoFinDiff capture stylized facts; (ii) the generated data accurately meet
specified conditions for trends and volatility; (iii) the diversity of the
generated data surpasses that of the baseline models; and (iv) models trained
on CoFinDiff-generated data achieve improved performance in deep hedging task.",http://arxiv.org/pdf/2503.04164v1,,False
Semantic Retrieval Augmented Contrastive Learning for Sequential Recommendation,06/03/2025,"Ziqiang Cui, Yunpeng Weng, Xing Tang, Xiaokun Zhang, Dugang Liu, Shiwei Li, Peiyang Liu, Bowei He, Weihong Luo, Xiuqiang He, Chen Ma","Sequential recommendation aims to model user preferences based on historical
behavior sequences, which is crucial for various online platforms. Data
sparsity remains a significant challenge in this area as most users have
limited interactions and many items receive little attention. To mitigate this
issue, contrastive learning has been widely adopted. By constructing positive
sample pairs from the data itself and maximizing their agreement in the
embedding space,it can leverage available data more effectively. Constructing
reasonable positive sample pairs is crucial for the success of contrastive
learning. However, current approaches struggle to generate reliable positive
pairs as they either rely on representations learned from inherently sparse
collaborative signals or use random perturbations which introduce significant
uncertainty. To address these limitations, we propose a novel approach named
Semantic Retrieval Augmented Contrastive Learning (SRA-CL), which leverages
semantic information to improve the reliability of contrastive samples. SRA-CL
comprises two main components: (1) Cross-Sequence Contrastive Learning via User
Semantic Retrieval, which utilizes large language models (LLMs) to understand
diverse user preferences and retrieve semantically similar users to form
reliable positive samples through a learnable sample synthesis method; and (2)
Intra-Sequence Contrastive Learning via Item Semantic Retrieval, which employs
LLMs to comprehend items and retrieve similar items to perform semantic-based
item substitution, thereby creating semantically consistent augmented views for
contrastive learning. SRA-CL is plug-and-play and can be integrated into
standard sequential recommendation models. Extensive experiments on four public
datasets demonstrate the effectiveness and generalizability of the proposed
approach.",http://arxiv.org/pdf/2503.04162v1,,False
Robust Multi-View Learning via Representation Fusion of Sample-Level Attention and Alignment of Simulated Perturbation,06/03/2025,"Jie Xu, Na Zhao, Gang Niu, Masashi Sugiyama, Xiaofeng Zhu","Recently, multi-view learning (MVL) has garnered significant attention due to
its ability to fuse discriminative information from multiple views. However,
real-world multi-view datasets are often heterogeneous and imperfect, which
usually makes MVL methods designed for specific combinations of views lack
application potential and limits their effectiveness. To address this issue, we
propose a novel robust MVL method (namely RML) with simultaneous representation
fusion and alignment. Specifically, we introduce a simple yet effective
multi-view transformer fusion network where we transform heterogeneous
multi-view data into homogeneous word embeddings, and then integrate multiple
views by the sample-level attention mechanism to obtain a fused representation.
Furthermore, we propose a simulated perturbation based multi-view contrastive
learning framework that dynamically generates the noise and unusable
perturbations for simulating imperfect data conditions. The simulated noisy and
unusable data obtain two distinct fused representations, and we utilize
contrastive learning to align them for learning discriminative and robust
representations. Our RML is self-supervised and can also be applied for
downstream tasks as a regularization. In experiments, we employ it in
unsupervised multi-view clustering, noise-label classification, and as a
plug-and-play module for cross-modal hashing retrieval. Extensive comparison
experiments and ablation studies validate the effectiveness of RML.",http://arxiv.org/pdf/2503.04151v1,,False
Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge,06/03/2025,"Xinyue Cui, Johnny Tian-Zheng Wei, Swabha Swayamdipta, Robin Jia","Data watermarking in language models injects traceable signals, such as
specific token sequences or stylistic patterns, into copyrighted text, allowing
copyright holders to track and verify training data ownership. Previous data
watermarking techniques primarily focus on effective memorization after
pretraining, while overlooking challenges that arise in other stages of the LLM
pipeline, such as the risk of watermark filtering during data preprocessing, or
potential forgetting through post-training, or verification difficulties due to
API-only access. We propose a novel data watermarking approach that injects
coherent and plausible yet fictitious knowledge into training data using
generated passages describing a fictitious entity and its associated
attributes. Our watermarks are designed to be memorized by the LLM through
seamlessly integrating in its training data, making them harder to detect
lexically during preprocessing.We demonstrate that our watermarks can be
effectively memorized by LLMs, and that increasing our watermarks' density,
length, and diversity of attributes strengthens their memorization. We further
show that our watermarks remain robust throughout LLM development, maintaining
their effectiveness after continual pretraining and supervised finetuning.
Finally, we show that our data watermarks can be evaluated even under API-only
access via question answering.",http://arxiv.org/pdf/2503.04036v1,,False
Integrating Protein Dynamics into Structure-Based Drug Design via Full-Atom Stochastic Flows,06/03/2025,"Xiangxin Zhou, Yi Xiao, Haowei Lin, Xinheng He, Jiaqi Guan, Yang Wang, Qiang Liu, Feng Zhou, Liang Wang, Jianzhu Ma","The dynamic nature of proteins, influenced by ligand interactions, is
essential for comprehending protein function and progressing drug discovery.
Traditional structure-based drug design (SBDD) approaches typically target
binding sites with rigid structures, limiting their practical application in
drug development. While molecular dynamics simulation can theoretically capture
all the biologically relevant conformations, the transition rate is dictated by
the intrinsic energy barrier between them, making the sampling process
computationally expensive. To overcome the aforementioned challenges, we
propose to use generative modeling for SBDD considering conformational changes
of protein pockets. We curate a dataset of apo and multiple holo states of
protein-ligand complexes, simulated by molecular dynamics, and propose a
full-atom flow model (and a stochastic version), named DynamicFlow, that learns
to transform apo pockets and noisy ligands into holo pockets and corresponding
3D ligand molecules. Our method uncovers promising ligand molecules and
corresponding holo conformations of pockets. Additionally, the resultant
holo-like states provide superior inputs for traditional SBDD approaches,
playing a significant role in practical drug discovery.",http://arxiv.org/pdf/2503.03989v1,,False
Image Data Augmentation for the TAIGA-IACT Experiment with Conditional Generative Adversarial Networks,06/03/2025,"Yu. Yu. Dubenskaya, A. P. Kryukov, E. O. Gres, S. P. Polyakov, E. B. Postnikov, P. A. Volchugov, A. A. Vlaskina, D. P. Zhurov","Modern Imaging Atmospheric Cherenkov Telescopes (IACTs) generate a huge
amount of data that must be classified automatically, ideally in real time.
Currently, machine learning-based solutions are increasingly being used to
solve classification problems. However, these classifiers require proper
training data sets to work correctly. The problem with training neural networks
on real IACT data is that these data need to be pre-labeled, whereas such
labeling is difficult and its results are estimates. In addition, the
distribution of incoming events is highly imbalanced. Firstly, there is an
imbalance in the types of events, since the number of detected gamma quanta is
significantly less than the number of protons. Secondly, the energy
distribution of particles of the same type is also imbalanced, since
high-energy particles are extremely rare. This imbalance results in poorly
trained classifiers that, once trained, do not handle rare events correctly.
Using only conventional Monte Carlo event simulation methods to solve this
problem is possible, but extremely resource-intensive and time-consuming. To
address this issue, we propose to perform data augmentation with artificially
generated events of the desired type and energy using conditional generative
adversarial networks (cGANs), distinguishing classes by energy values. In the
paper, we describe a simple algorithm for generating balanced data sets using
cGANs. Thus, the proposed neural network model produces both imbalanced data
sets for physical analysis as well as balanced data sets suitable for training
other neural networks.",http://arxiv.org/pdf/2503.03982v1,,False
