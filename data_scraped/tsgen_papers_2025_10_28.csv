Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
RobotArena $\infty$: Scalable Robot Benchmarking via Real-to-Sim Translation,27/10/2025,"Yash Jangir, Yidi Zhang, Kashu Yamazaki, Chenyu Zhang, Kuan-Hsun Tu, Tsung-Wei Ke, Lei Ke, Yonatan Bisk, Katerina Fragkiadaki","The pursuit of robot generalists - instructable agents capable of performing
diverse tasks across diverse environments - demands rigorous and scalable
evaluation. Yet real-world testing of robot policies remains fundamentally
constrained: it is labor-intensive, slow, unsafe at scale, and difficult to
reproduce. Existing simulation benchmarks are similarly limited, as they train
and test policies within the same synthetic domains and cannot assess models
trained from real-world demonstrations or alternative simulation environments.
As policies expand in scope and complexity, these barriers only intensify,
since defining ""success"" in robotics often hinges on nuanced human judgments of
execution quality. In this paper, we introduce a new benchmarking framework
that overcomes these challenges by shifting VLA evaluation into large-scale
simulated environments augmented with online human feedback. Leveraging
advances in vision-language models, 2D-to-3D generative modeling, and
differentiable rendering, our approach automatically converts video
demonstrations from widely used robot datasets into simulated counterparts.
Within these digital twins, we assess VLA policies using both automated
VLM-guided scoring and scalable human preference judgments collected from
crowdworkers, transforming human involvement from tedious scene setup,
resetting, and safety supervision into lightweight preference comparisons. To
measure robustness, we systematically perturb simulated environments along
multiple axes, such as textures and object placements, stress-testing policy
generalization under controlled variation. The result is a continuously
evolving, reproducible, and scalable benchmark for real-world trained robot
manipulation policies, addressing a critical missing capability in today's
robotics landscape.",http://arxiv.org/pdf/2510.23571v1,,False
AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines,27/10/2025,"Abolfazl Younesi, Zahra Najafabadi Samani, Thomas Fahringer","Data pipelines are essential in stream processing as they enable the
efficient collection, processing, and delivery of real-time data, supporting
rapid data analysis. In this paper, we present AutoStreamPipe, a novel
framework that employs Large Language Models (LLMs) to automate the design,
generation, and deployment of stream processing pipelines. AutoStreamPipe
bridges the semantic gap between high-level user intent and platform-specific
implementations across distributed stream processing systems for structured
multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an
extended version of GoT. AutoStreamPipe combines resilient execution
strategies, advanced query analysis, and HGoT to deliver pipelines with good
accuracy. Experimental evaluations on diverse pipelines demonstrate that
AutoStreamPipe significantly reduces development time (x6.3) and error rates
(x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM
code-generation methods.",http://arxiv.org/pdf/2510.23408v1,,False
Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps,27/10/2025,"Anwesha Das, John Duff, JÃ¶rg Hoffmann, Vera Demberg","Adaptive agent design offers a way to improve human-AI collaboration on
time-sensitive tasks in rapidly changing environments. In such cases, to ensure
the human maintains an accurate understanding of critical task elements, an
assistive agent must not only identify the highest priority information but
also estimate how and when this information can be communicated most
effectively, given that human attention represents a zero-sum cognitive
resource where focus on one message diminishes awareness of other or upcoming
information. We introduce a theoretical framework for adaptive signalling which
meets these challenges by using principles of rational communication,
formalised as Bayesian reference resolution using the Rational Speech Act (RSA)
modelling framework, to plan a sequence of messages which optimise timely
alignment between user belief and a dynamic environment. The agent adapts
message specificity and timing to the particulars of a user and scenario based
on projections of how prior-guided interpretation of messages will influence
attention to the interface and subsequent belief update, across several
timesteps out to a fixed horizon. In a comparison to baseline methods, we show
that this effectiveness depends crucially on combining multi-step planning with
a realistic model of user awareness. As the first application of RSA for
communication in a dynamic environment, and for human-AI interaction in
general, we establish theoretical foundations for pragmatic communication in
human-agent teams, highlighting how insights from cognitive science can be
capitalised to inform the design of assistive agents.",http://arxiv.org/pdf/2510.23340v1,,False
A Novel Framework for Multi-Modal Protein Representation Learning,27/10/2025,"Runjie Zheng, Zhen Wang, Anjie Qiao, Jiancong Xie, Jiahua Rao, Yuedong Yang","Accurate protein function prediction requires integrating heterogeneous
intrinsic signals (e.g., sequence and structure) with noisy extrinsic contexts
(e.g., protein-protein interactions and GO term annotations). However, two key
challenges hinder effective fusion: (i) cross-modal distributional mismatch
among embeddings produced by pre-trained intrinsic encoders, and (ii) noisy
relational graphs of extrinsic data that degrade GNN-based information
aggregation. We propose Diffused and Aligned Multi-modal Protein Embedding
(DAMPE), a unified framework that addresses these through two core mechanisms.
First, we propose Optimal Transport (OT)-based representation alignment that
establishes correspondence between intrinsic embedding spaces of different
modalities, effectively mitigating cross-modal heterogeneity. Second, we
develop a Conditional Graph Generation (CGG)-based information fusion method,
where a condition encoder fuses the aligned intrinsic embeddings to provide
informative cues for graph reconstruction. Meanwhile, our theoretical analysis
implies that the CGG objective drives this condition encoder to absorb
graph-aware knowledge into its produced protein representations. Empirically,
DAMPE outperforms or matches state-of-the-art methods such as DPFunc on
standard GO benchmarks, achieving AUPR gains of 0.002-0.013 pp and Fmax gains
0.004-0.007 pp. Ablation studies further show that OT-based alignment
contributes 0.043-0.064 pp AUPR, while CGG-based fusion adds 0.005-0.111 pp
Fmax. Overall, DAMPE offers a scalable and theoretically grounded approach for
robust multi-modal protein representation learning, substantially enhancing
protein function prediction.",http://arxiv.org/pdf/2510.23273v1,,False
Accelerating IC Thermal Simulation Data Generation via Block Krylov and Operator Action,27/10/2025,"Hong Wang, Wenkai Yang, Jie Wang, Huanshuo Dong, Zijie Geng, Zhen Huang, Depeng Xie, Zhezheng Hao, Hande Dong","Recent advances in data-driven approaches, such as neural operators (NOs),
have shown substantial efficacy in reducing the solution time for integrated
circuit (IC) thermal simulations. However, a limitation of these approaches is
requiring a large amount of high-fidelity training data, such as chip
parameters and temperature distributions, thereby incurring significant
computational costs. To address this challenge, we propose a novel algorithm
for the generation of IC thermal simulation data, named block Krylov and
operator action (BlocKOA), which simultaneously accelerates the data generation
process and enhances the precision of generated data. BlocKOA is specifically
designed for IC applications. Initially, we use the block Krylov algorithm
based on the structure of the heat equation to quickly obtain a few basic
solutions. Then we combine them to get numerous temperature distributions that
satisfy the physical constraints. Finally, we apply heat operators on these
functions to determine the heat source distributions, efficiently generating
precise data points. Theoretical analysis shows that the time complexity of
BlocKOA is one order lower than the existing method. Experimental results
further validate its efficiency, showing that BlocKOA achieves a 420-fold
speedup in generating thermal simulation data for 5000 chips with varying
physical parameters and IC structures. Even with just 4% of the generation
time, data-driven approaches trained on the data generated by BlocKOA exhibits
comparable performance to that using the existing method.",http://arxiv.org/pdf/2510.23221v1,,False
"Benchmarking VQE Configurations: Architectures, Initializations, and Optimizers for Silicon Ground State Energy",27/10/2025,"Zakaria Boutakka, Nouhaila Innan, Muhammed Shafique, Mohamed Bennai, Z. Sakhi","Quantum computing presents a promising path toward precise quantum chemical
simulations, particularly for systems that challenge classical methods. This
work investigates the performance of the Variational Quantum Eigensolver (VQE)
in estimating the ground-state energy of the silicon atom, a relatively heavy
element that poses significant computational complexity. Within a hybrid
quantum-classical optimization framework, we implement VQE using a range of
ansatz, including Double Excitation Gates, ParticleConservingU2, UCCSD, and
k-UpCCGSD, combined with various optimizers such as gradient descent, SPSA, and
ADAM. The main contribution of this work lies in a systematic methodological
exploration of how these configuration choices interact to influence VQE
performance, establishing a structured benchmark for selecting optimal settings
in quantum chemical simulations. Key findings show that parameter
initialization plays a decisive role in the algorithm's stability, and that the
combination of a chemically inspired ansatz with adaptive optimization yields
superior convergence and precision compared to conventional approaches.",http://arxiv.org/pdf/2510.23171v1,,False
"Treble10: A high-quality dataset for far-field speech recognition, dereverberation, and enhancement",27/10/2025,"Sarabeth S. Mullins, Georg GÃ¶tz, Eric Bezzam, Steven Zheng, Daniel Gert Nielsen","Accurate far-field speech datasets are critical for tasks such as automatic
speech recognition (ASR), dereverberation, speech enhancement, and source
separation. However, current datasets are limited by the trade-off between
acoustic realism and scalability. Measured corpora provide faithful physics but
are expensive, low-coverage, and rarely include paired clean and reverberant
data. In contrast, most simulation-based datasets rely on simplified
geometrical acoustics, thus failing to reproduce key physical phenomena like
diffraction, scattering, and interference that govern sound propagation in
complex environments. We introduce Treble10, a large-scale, physically accurate
room-acoustic dataset. Treble10 contains over 3000 broadband room impulse
responses (RIRs) simulated in 10 fully furnished real-world rooms, using a
hybrid simulation paradigm implemented in the Treble SDK that combines a
wave-based and geometrical acoustics solver. The dataset provides six
complementary subsets, spanning mono, 8th-order Ambisonics, and 6-channel
device RIRs, as well as pre-convolved reverberant speech scenes paired with
LibriSpeech utterances. All signals are simulated at 32 kHz, accurately
modelling low-frequency wave effects and high-frequency reflections. Treble10
bridges the realism gap between measurement and simulation, enabling
reproducible, physically grounded evaluation and large-scale data augmentation
for far-field speech tasks. The dataset is openly available via the Hugging
Face Hub, and is intended as both a benchmark and a template for
next-generation simulation-driven audio research.",http://arxiv.org/pdf/2510.23141v1,,False
Leveraging Hierarchical Organization for Medical Multi-document Summarization,27/10/2025,"Yi-Li Hsu, Katelyn X. Mei, Lucy Lu Wang","Medical multi-document summarization (MDS) is a complex task that requires
effectively managing cross-document relationships. This paper investigates
whether incorporating hierarchical structures in the inputs of MDS can improve
a model's ability to organize and contextualize information across documents
compared to traditional flat summarization methods. We investigate two ways of
incorporating hierarchical organization across three large language models
(LLMs), and conduct comprehensive evaluations of the resulting summaries using
automated metrics, model-based metrics, and domain expert evaluation of
preference, understandability, clarity, complexity, relevance, coverage,
factuality, and coherence. Our results show that human experts prefer
model-generated summaries over human-written summaries. Hierarchical approaches
generally preserve factuality, coverage, and coherence of information, while
also increasing human preference for summaries. Additionally, we examine
whether simulated judgments from GPT-4 align with human judgments, finding
higher agreement along more objective evaluation facets. Our findings
demonstrate that hierarchical structures can improve the clarity of medical
summaries generated by models while maintaining content coverage, providing a
practical way to improve human preference for generated summaries.",http://arxiv.org/pdf/2510.23104v1,,False
SwiftTS: A Swift Selection Framework for Time Series Pre-trained Models via Multi-task Meta-Learning,27/10/2025,"Tengxue Zhang, Biao Ouyang, Yang Shu, Xinyang Chen, Chenjuan Guo, Bin Yang","Pre-trained models exhibit strong generalization to various downstream tasks.
However, given the numerous models available in the model hub, identifying the
most suitable one by individually fine-tuning is time-consuming. In this paper,
we propose \textbf{SwiftTS}, a swift selection framework for time series
pre-trained models. To avoid expensive forward propagation through all
candidates, SwiftTS adopts a learning-guided approach that leverages historical
dataset-model performance pairs across diverse horizons to predict model
performance on unseen datasets. It employs a lightweight dual-encoder
architecture that embeds time series and candidate models with rich
characteristics, computing patchwise compatibility scores between data and
model embeddings for efficient selection. To further enhance the generalization
across datasets and horizons, we introduce a horizon-adaptive expert
composition module that dynamically adjusts expert weights, and the
transferable cross-task learning with cross-dataset and cross-horizon task
sampling to enhance out-of-distribution (OOD) robustness. Extensive experiments
on 14 downstream datasets and 8 pre-trained models demonstrate that SwiftTS
achieves state-of-the-art performance in time series pre-trained model
selection.",http://arxiv.org/pdf/2510.23051v1,,False
Sublinear Sketches for Approximate Nearest Neighbor and Kernel Density Estimation,27/10/2025,"Ved Danait, Srijan Das, Sujoy Bhore","Approximate Nearest Neighbor (ANN) search and Approximate Kernel Density
Estimation (A-KDE) are fundamental problems at the core of modern machine
learning, with broad applications in data analysis, information systems, and
large-scale decision making. In massive and dynamic data streams, a central
challenge is to design compact sketches that preserve essential structural
properties of the data while enabling efficient queries.
  In this work, we develop new sketching algorithms that achieve sublinear
space and query time guarantees for both ANN and A-KDE for a dynamic stream of
data. For ANN in the streaming model, under natural assumptions, we design a
sublinear sketch that requires only $\mathcal{O}(n^{1+\rho-\eta})$ memory by
storing only a sublinear ($n^{-\eta}$) fraction of the total inputs, where
$\rho$ is a parameter of the LSH family, and $0<\eta<1$. Our method supports
sublinear query time, batch queries, and extends to the more general Turnstile
model. While earlier works have focused on Exact NN, this is the first result
on ANN that achieves near-optimal trade-offs between memory size and
approximation error.
  Next, for A-KDE in the Sliding-Window model, we propose a sketch of size
$\mathcal{O}\left(RW \cdot \frac{1}{\sqrt{1+\epsilon} - 1} \log^2 N\right)$,
where $R$ is the number of sketch rows, $W$ is the LSH range, $N$ is the window
size, and $\epsilon$ is the approximation error. This, to the best of our
knowledge, is the first theoretical sublinear sketch guarantee for A-KDE in the
Sliding-Window model.
  We complement our theoretical results with experiments on various real-world
datasets, which show that the proposed sketches are lightweight and achieve
consistently low error in practice.",http://arxiv.org/pdf/2510.23039v1,,False
Diffuse to Detect: A Generalizable Framework for Anomaly Detection with Diffusion Models Applications to UAVs and Beyond,27/10/2025,"Mingze Gong, Juan Du, Jianbang You","Anomaly detection in complex, high-dimensional data, such as UAV sensor
readings, is essential for operational safety but challenging for existing
methods due to their limited sensitivity, scalability, and inability to capture
intricate dependencies. We propose the Diffuse to Detect (DTD) framework, a
novel approach that innovatively adapts diffusion models for anomaly detection,
diverging from their conventional use in generative tasks with high inference
time. By comparison, DTD employs a single-step diffusion process to predict
noise patterns, enabling rapid and precise identification of anomalies without
reconstruction errors. This approach is grounded in robust theoretical
foundations that link noise prediction to the data distribution's score
function, ensuring reliable deviation detection. By integrating Graph Neural
Networks to model sensor relationships as dynamic graphs, DTD effectively
captures spatial (inter-sensor) and temporal anomalies. Its two-branch
architecture, with parametric neural network-based energy scoring for
scalability and nonparametric statistical methods for interpretability,
provides flexible trade-offs between computational efficiency and transparency.
Extensive evaluations on UAV sensor data, multivariate time series, and images
demonstrate DTD's superior performance over existing methods, underscoring its
generality across diverse data modalities. This versatility, combined with its
adaptability, positions DTD as a transformative solution for safety-critical
applications, including industrial monitoring and beyond.",http://arxiv.org/pdf/2510.22928v1,,False
Limits of Generative Pre-Training in Structured EMR Trajectories with Irregular Sampling,27/10/2025,"Nicholas I-Hsien Kuo, Blanca Gallego, Louisa Jorm","Foundation models refer to architectures trained on vast datasets using
autoregressive pre-training from natural language processing to capture
intricate patterns and motifs. They were originally developed to transfer such
learned knowledge to downstream predictive tasks. Recently, however, some
studies repurpose these learned representations for phenotype discovery without
rigorous validation, risking superficially realistic but clinically incoherent
embeddings. To test this mismatch, we trained two autoregressive models -- a
sequence-to-sequence LSTM and a reduced Transformer -- on longitudinal ART for
HIV and Acute Hypotension datasets. Controlled irregularity was added during
training via random inter-visit gaps, while test sequences stayed complete.
Patient-trajectory synthesis evaluated distributional and correlational
fidelity. Both reproduced feature distributions but failed to preserve
cross-feature structure -- showing that generative pre-training yields local
realism but limited clinical coherence. These results highlight the need for
domain-specific evaluation and support trajectory synthesis as a practical
probe before fine-tuning or deployment.",http://arxiv.org/pdf/2510.22878v1,,False
