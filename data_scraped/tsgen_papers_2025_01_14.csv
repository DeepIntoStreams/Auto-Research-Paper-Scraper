Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
Exploring and Mitigating Adversarial Manipulation of Voting-Based Leaderboards,13/01/2025,"Yangsibo Huang, Milad Nasr, Anastasios Angelopoulos, Nicholas Carlini, Wei-Lin Chiang, Christopher A. Choquette-Choo, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Ken Ziyu Liu, Ion Stoica, Florian Tramer, Chiyuan Zhang","It is now common to evaluate Large Language Models (LLMs) by having humans
manually vote to evaluate model outputs, in contrast to typical benchmarks that
evaluate knowledge or skill at some particular task. Chatbot Arena, the most
popular benchmark of this type, ranks models by asking users to select the
better response between two randomly selected models (without revealing which
model was responsible for the generations). These platforms are widely trusted
as a fair and accurate measure of LLM capabilities. In this paper, we show that
if bot protection and other defenses are not implemented, these voting-based
benchmarks are potentially vulnerable to adversarial manipulation.
Specifically, we show that an attacker can alter the leaderboard (to promote
their favorite model or demote competitors) at the cost of roughly a thousand
votes (verified in a simulated, offline version of Chatbot Arena). Our attack
consists of two steps: first, we show how an attacker can determine which model
was used to generate a given reply with more than $95\%$ accuracy; and then,
the attacker can use this information to consistently vote for (or against) a
target model. Working with the Chatbot Arena developers, we identify, propose,
and implement mitigations to improve the robustness of Chatbot Arena against
adversarial manipulation, which, based on our analysis, substantially increases
the cost of such attacks. Some of these defenses were present before our
collaboration, such as bot protection with Cloudflare, malicious user
detection, and rate limiting. Others, including reCAPTCHA and login are being
integrated to strengthen the security in Chatbot Arena.",http://arxiv.org/pdf/2501.07493v1,,False
"Pairwise Comparisons without Stochastic Transitivity: Model, Theory and Applications",13/01/2025,"Sze Ming Lee, Yunxiao Chen","Most statistical models for pairwise comparisons, including the Bradley-Terry
(BT) and Thurstone models and many extensions, make a relatively strong
assumption of stochastic transitivity. This assumption imposes the existence of
an unobserved global ranking among all the players/teams/items and monotone
constraints on the comparison probabilities implied by the global ranking.
However, the stochastic transitivity assumption does not hold in many
real-world scenarios of pairwise comparisons, especially games involving
multiple skills or strategies. As a result, models relying on this assumption
can have suboptimal predictive performance. In this paper, we propose a general
family of statistical models for pairwise comparison data without a stochastic
transitivity assumption, substantially extending the BT and Thurstone models.
In this model, the pairwise probabilities are determined by a (approximately)
low-dimensional skew-symmetric matrix. Likelihood-based estimation methods and
computational algorithms are developed, which allow for sparse data with only a
small proportion of observed pairs. Theoretical analysis shows that the
proposed estimator achieves minimax-rate optimality, which adapts effectively
to the sparsity level of the data. The spectral theory for skew-symmetric
matrices plays a crucial role in the implementation and theoretical analysis.
The proposed method's superiority against the BT model, along with its broad
applicability across diverse scenarios, is further supported by simulations and
real data analysis.",http://arxiv.org/pdf/2501.07437v1,,False
An Investigation into Seasonal Variations in Energy Forecasting for Student Residences,13/01/2025,"Muhammad Umair Danish, Mathumitha Sureshkumar, Thanuri Fonseka, Umeshika Uthayakumar, Vinura Galwaduge","This research provides an in-depth evaluation of various machine learning
models for energy forecasting, focusing on the unique challenges of seasonal
variations in student residential settings. The study assesses the performance
of baseline models, such as LSTM and GRU, alongside state-of-the-art
forecasting methods, including Autoregressive Feedforward Neural Networks,
Transformers, and hybrid approaches. Special attention is given to predicting
energy consumption amidst challenges like seasonal patterns, vacations,
meteorological changes, and irregular human activities that cause sudden
fluctuations in usage. The findings reveal that no single model consistently
outperforms others across all seasons, emphasizing the need for season-specific
model selection or tailored designs. Notably, the proposed Hyper Network based
LSTM and MiniAutoEncXGBoost models exhibit strong adaptability to seasonal
variations, effectively capturing abrupt changes in energy consumption during
summer months. This study advances the energy forecasting field by emphasizing
the critical role of seasonal dynamics and model-specific behavior in achieving
accurate predictions.",http://arxiv.org/pdf/2501.07423v1,,False
Initial Findings on Sensor based Open Vocabulary Activity Recognition via Text Embedding Inversion,13/01/2025,"Lala Shakti Swarup Ray, Bo Zhou, Sungho Suh, Paul Lukowicz","Conventional human activity recognition (HAR) relies on classifiers trained
to predict discrete activity classes, inherently limiting recognition to
activities explicitly present in the training set. Such classifiers would
invariably fail, putting zero likelihood, when encountering unseen activities.
We propose Open Vocabulary HAR (OV-HAR), a framework that overcomes this
limitation by first converting each activity into natural language and breaking
it into a sequence of elementary motions. This descriptive text is then encoded
into a fixed-size embedding. The model is trained to regress this embedding,
which is subsequently decoded back into natural language using a pre-trained
embedding inversion model. Unlike other works that rely on auto-regressive
large language models (LLMs) at their core, OV-HAR achieves open vocabulary
recognition without the computational overhead of such models. The generated
text can be transformed into a single activity class using LLM prompt
engineering. We have evaluated our approach on different modalities, including
vision (pose), IMU, and pressure sensors, demonstrating robust generalization
across unseen activities and modalities, offering a fundamentally different
paradigm from contemporary classifiers.",http://arxiv.org/pdf/2501.07408v1,,False
Simulating the Hubbard Model with Equivariant Normalizing Flows,13/01/2025,"Dominic Schuh, Janik Kreit, Evan Berkowitz, Lena Funcke, Thomas Luu, Kim A. Nicoli, Marcel Rodekamp","Generative models, particularly normalizing flows, have shown exceptional
performance in learning probability distributions across various domains of
physics, including statistical mechanics, collider physics, and lattice field
theory. In the context of lattice field theory, normalizing flows have been
successfully applied to accurately learn the Boltzmann distribution, enabling a
range of tasks such as direct estimation of thermodynamic observables and
sampling independent and identically distributed (i.i.d.) configurations.
  In this work, we present a proof-of-concept demonstration that normalizing
flows can be used to learn the Boltzmann distribution for the Hubbard model.
This model is widely employed to study the electronic structure of graphene and
other carbon nanomaterials. State-of-the-art numerical simulations of the
Hubbard model, such as those based on Hybrid Monte Carlo (HMC) methods, often
suffer from ergodicity issues, potentially leading to biased estimates of
physical observables. Our numerical experiments demonstrate that leveraging
i.i.d.\ sampling from the normalizing flow effectively addresses these issues.",http://arxiv.org/pdf/2501.07371v1,,False
Digital Operating Mode Classification of Real-World Amateur Radio Transmissions,13/01/2025,"Maximilian Bundscherer, Thomas H. Schmitt, Ilja Baumann, Tobias Bocklet","This study presents an ML approach for classifying digital radio operating
modes evaluated on real-world transmissions. We generated 98 different
parameterized radio signals from 17 digital operating modes, transmitted each
of them on the 70 cm (UHF) amateur radio band, and recorded our transmissions
with two different architectures of SDR receivers. Three lightweight ML models
were trained exclusively on spectrograms of limited non-transmitted signals
with random characters as payloads. This training involved an online data
augmentation pipeline to simulate various radio channel impairments. Our best
model, EfficientNetB0, achieved an accuracy of 93.80% across the 17 operating
modes and 85.47% across all 98 parameterized radio signals, evaluated on our
real-world transmissions with Wikipedia articles as payloads. Furthermore, we
analyzed the impact of varying signal durations & the number of FFT bins on
classification, assessed the effectiveness of our simulated channel
impairments, and tested our models across multiple simulated SNRs.",http://arxiv.org/pdf/2501.07337v1,,False
Estimating quantum relative entropies on quantum computers,13/01/2025,"Yuchen Lu, Kun Fang","Quantum relative entropy, a quantum generalization of the well-known
Kullback-Leibler divergence, serves as a fundamental measure of the
distinguishability between quantum states and plays a pivotal role in quantum
information science. Despite its importance, efficiently estimating quantum
relative entropy between two quantum states on quantum computers remains a
significant challenge. In this work, we propose the first quantum algorithm for
estimating quantum relative entropy and Petz R\'{e}nyi divergence from two
unknown quantum states on quantum computers, addressing open problems
highlighted in [Phys. Rev. A 109, 032431 (2024)] and [IEEE Trans. Inf. Theory
70, 5653-5680 (2024)]. This is achieved by combining quadrature approximations
of relative entropies, the variational representation of quantum f-divergences,
and a new technique for parameterizing Hermitian polynomial operators to
estimate their traces with quantum states. Notably, the circuit size of our
algorithm is at most 2n+1 with n being the number of qubits in the quantum
states and it is directly applicable to distributed scenarios, where quantum
states to be compared are hosted on cross-platform quantum computers. We
validate our algorithm through numerical simulations, laying the groundwork for
its future deployment on quantum hardware devices.",http://arxiv.org/pdf/2501.07292v1,,False
LLM-Net: Democratizing LLMs-as-a-Service through Blockchain-based Expert Networks,13/01/2025,"Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng","The centralization of Large Language Models (LLMs) development has created
significant barriers to AI advancement, limiting the democratization of these
powerful technologies. This centralization, coupled with the scarcity of
high-quality training data and mounting complexity of maintaining comprehensive
expertise across rapidly expanding knowledge domains, poses critical challenges
to the continued growth of LLMs. While solutions like Retrieval-Augmented
Generation (RAG) offer potential remedies, maintaining up-to-date expert
knowledge across diverse domains remains a significant challenge, particularly
given the exponential growth of specialized information. This paper introduces
LLMs Networks (LLM-Net), a blockchain-based framework that democratizes
LLMs-as-a-Service through a decentralized network of specialized LLM providers.
By leveraging collective computational resources and distributed domain
expertise, LLM-Net incorporates fine-tuned expert models for various specific
domains, ensuring sustained knowledge growth while maintaining service quality
through collaborative prompting mechanisms. The framework's robust design
includes blockchain technology for transparent transaction and performance
validation, establishing an immutable record of service delivery. Our
simulation, built on top of state-of-the-art LLMs such as Claude 3.5 Sonnet,
Llama 3.1, Grok-2, and GPT-4o, validates the effectiveness of the
reputation-based mechanism in maintaining service quality by selecting
high-performing respondents (LLM providers). Thereby it demonstrates the
potential of LLM-Net to sustain AI advancement through the integration of
decentralized expertise and blockchain-based accountability.",http://arxiv.org/pdf/2501.07288v1,,False
"Bridging Smart Meter Gaps: A Benchmark of Statistical, Machine Learning and Time Series Foundation Models for Data Imputation",13/01/2025,"Amir Sartipi, Joaquin Delgado Fernandez, Sergio Potenciano Menci, Alessio Magitteri","The integrity of time series data in smart grids is often compromised by
missing values due to sensor failures, transmission errors, or disruptions.
Gaps in smart meter data can bias consumption analyses and hinder reliable
predictions, causing technical and economic inefficiencies. As smart meter data
grows in volume and complexity, conventional techniques struggle with its
nonlinear and nonstationary patterns. In this context, Generative Artificial
Intelligence offers promising solutions that may outperform traditional
statistical methods. In this paper, we evaluate two general-purpose Large
Language Models and five Time Series Foundation Models for smart meter data
imputation, comparing them with conventional Machine Learning and statistical
models. We introduce artificial gaps (30 minutes to one day) into an anonymized
public dataset to test inference capabilities. Results show that Time Series
Foundation Models, with their contextual understanding and pattern recognition,
could significantly enhance imputation accuracy in certain cases. However, the
trade-off between computational cost and performance gains remains a critical
consideration.",http://arxiv.org/pdf/2501.07276v1,,False
Skip Mamba Diffusion for Monocular 3D Semantic Scene Completion,13/01/2025,"Li Liang, Naveed Akhtar, Jordan Vice, Xiangrui Kong, Ajmal Saeed Mian","3D semantic scene completion is critical for multiple downstream tasks in
autonomous systems. It estimates missing geometric and semantic information in
the acquired scene data. Due to the challenging real-world conditions, this
task usually demands complex models that process multi-modal data to achieve
acceptable performance. We propose a unique neural model, leveraging advances
from the state space and diffusion generative modeling to achieve remarkable 3D
semantic scene completion performance with monocular image input. Our technique
processes the data in the conditioned latent space of a variational autoencoder
where diffusion modeling is carried out with an innovative state space
technique. A key component of our neural network is the proposed Skimba (Skip
Mamba) denoiser, which is adept at efficiently processing long-sequence data.
The Skimba diffusion model is integral to our 3D scene completion network,
incorporating a triple Mamba structure, dimensional decomposition residuals and
varying dilations along three directions. We also adopt a variant of this
network for the subsequent semantic segmentation stage of our method. Extensive
evaluation on the standard SemanticKITTI and SSCBench-KITTI360 datasets show
that our approach not only outperforms other monocular techniques by a large
margin, it also achieves competitive performance against stereo methods. The
code is available at https://github.com/xrkong/skimba",http://arxiv.org/pdf/2501.07260v1,,False
Unveiling the Potential of Text in High-Dimensional Time Series Forecasting,13/01/2025,"Xin Zhou, Weiqing Wang, Shilin Qu, Zhiqiang Zhang, Christoph Bergmeir","Time series forecasting has traditionally focused on univariate and
multivariate numerical data, often overlooking the benefits of incorporating
multimodal information, particularly textual data. In this paper, we propose a
novel framework that integrates time series models with Large Language Models
to improve high-dimensional time series forecasting. Inspired by multimodal
models, our method combines time series and textual data in the dual-tower
structure. This fusion of information creates a comprehensive representation,
which is then processed through a linear layer to generate the final forecast.
Extensive experiments demonstrate that incorporating text enhances
high-dimensional time series forecasting performance. This work paves the way
for further research in multimodal time series forecasting.",http://arxiv.org/pdf/2501.07048v1,,False
Explore the Use of Time Series Foundation Model for Car-Following Behavior Analysis,13/01/2025,"Luwei Zeng, Runze Yan","Modeling car-following behavior is essential for traffic simulation,
analyzing driving patterns, and understanding complex traffic flows with
varying levels of autonomous vehicles. Traditional models like the Safe
Distance Model and Intelligent Driver Model (IDM) require precise parameter
calibration and often lack generality due to simplified assumptions about
driver behavior. While machine learning and deep learning methods capture
complex patterns, they require large labeled datasets. Foundation models
provide a more efficient alternative. Pre-trained on vast, diverse time series
datasets, they can be applied directly to various tasks without the need for
extensive re-training. These models generalize well across domains, and with
minimal fine-tuning, they can be adapted to specific tasks like car-following
behavior prediction. In this paper, we apply Chronos, a state-of-the-art public
time series foundation model, to analyze car-following behavior using the Open
ACC dataset. Without fine-tuning, Chronos outperforms traditional models like
IDM and Exponential smoothing with trend and seasonality (ETS), and achieves
similar results to deep learning models such as DeepAR and TFT, with an RMSE of
0.60. After fine-tuning, Chronos reduces the error to an RMSE of 0.53,
representing a 33.75% improvement over IDM and a 12-37% reduction compared to
machine learning models like ETS and deep learning models including DeepAR,
WaveNet, and TFT. This demonstrates the potential of foundation models to
significantly advance transportation research, offering a scalable, adaptable,
and highly accurate approach to predicting and simulating car-following
behaviors.",http://arxiv.org/pdf/2501.07034v1,,False
Combining LLM decision and RL action selection to improve RL policy for adaptive interventions,13/01/2025,"Karine Karine, Benjamin M. Marlin","Reinforcement learning (RL) is increasingly being used in the healthcare
domain, particularly for the development of personalized health adaptive
interventions. Inspired by the success of Large Language Models (LLMs), we are
interested in using LLMs to update the RL policy in real time, with the goal of
accelerating personalization. We use the text-based user preference to
influence the action selection on the fly, in order to immediately incorporate
the user preference. We use the term ""user preference"" as a broad term to refer
to a user personal preference, constraint, health status, or a statement
expressing like or dislike, etc. Our novel approach is a hybrid method that
combines the LLM response and the RL action selection to improve the RL policy.
Given an LLM prompt that incorporates the user preference, the LLM acts as a
filter in the typical RL action selection. We investigate different prompting
strategies and action selection strategies. To evaluate our approach, we
implement a simulation environment that generates the text-based user
preferences and models the constraints that impact behavioral dynamics. We show
that our approach is able to take into account the text-based user preferences,
while improving the RL policy, thus improving personalization in adaptive
intervention.",http://arxiv.org/pdf/2501.06980v1,,False
