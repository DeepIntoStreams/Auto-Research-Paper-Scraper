Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
VeriGraph: Scene Graphs for Execution Verifiable Robot Planning,15/11/2024,"Daniel Ekpo, Mara Levy, Saksham Suri, Chuong Huynh, Abhinav Shrivastava","Recent advancements in vision-language models (VLMs) offer potential for
robot task planning, but challenges remain due to VLMs' tendency to generate
incorrect action sequences. To address these limitations, we propose VeriGraph,
a novel framework that integrates VLMs for robotic planning while verifying
action feasibility. VeriGraph employs scene graphs as an intermediate
representation, capturing key objects and spatial relationships to improve plan
verification and refinement. The system generates a scene graph from input
images and uses it to iteratively check and correct action sequences generated
by an LLM-based task planner, ensuring constraints are respected and actions
are executable. Our approach significantly enhances task completion rates
across diverse manipulation scenarios, outperforming baseline methods by 58%
for language-based tasks and 30% for image-based tasks.",http://arxiv.org/pdf/2411.10446v1,,False
Mitigating Parameter Degeneracy using Joint Conditional Diffusion Model for WECC Composite Load Model in Power Systems,15/11/2024,"Feiqin Zhu, Dmitrii Torbunov, Yihui Ren, Zhongjing Jiang, Tianqiao Zhao, Amirthagunaraj Yogarathnam, Meng Yue","Data-driven modeling for dynamic systems has gained widespread attention in
recent years. Its inverse formulation, parameter estimation, aims to infer the
inherent model parameters from observations. However, parameter degeneracy,
where different combinations of parameters yield the same observable output,
poses a critical barrier to accurately and uniquely identifying model
parameters. In the context of WECC composite load model (CLM) in power systems,
utility practitioners have observed that CLM parameters carefully selected for
one fault event may not perform satisfactorily in another fault. Here, we
innovate a joint conditional diffusion model-based inverse problem solver
(JCDI), that incorporates a joint conditioning architecture with simultaneous
inputs of multi-event observations to improve parameter generalizability.
Simulation studies on the WECC CLM show that the proposed JCDI effectively
reduces uncertainties of degenerate parameters, thus the parameter estimation
error is decreased by 42.1% compared to a single-event learning scheme. This
enables the model to achieve high accuracy in predicting power trajectories
under different fault events, including electronic load tripping and motor
stalling, outperforming standard deep reinforcement learning and supervised
learning approaches. We anticipate this work will contribute to mitigating
parameter degeneracy in system dynamics, providing a general parameter
estimation framework across various scientific domains.",http://arxiv.org/pdf/2411.10431v1,,False
Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash,15/11/2024,"Parsa Hejabi, Elnaz Rahmati, Alireza S. Ziabari, Preni Golazizian, Jesse Thomason, Morteza Dehghani","Large Language Models (LLMs) have shown impressive capabilities in complex
tasks and interactive environments, yet their creativity remains underexplored.
This paper introduces a simulation framework utilizing the game Balderdash to
evaluate both the creativity and logical reasoning of LLMs. In Balderdash,
players generate fictitious definitions for obscure terms to deceive others
while identifying correct definitions. Our framework enables multiple LLM
agents to participate in this game, assessing their ability to produce
plausible definitions and strategize based on game rules and history. We
implemented a centralized game engine featuring various LLMs as participants
and a judge LLM to evaluate semantic equivalence. Through a series of
experiments, we analyzed the performance of different LLMs, examining metrics
such as True Definition Ratio, Deception Ratio, and Correct Guess Ratio. The
results provide insights into the creative and deceptive capabilities of LLMs,
highlighting their strengths and areas for improvement. Specifically, the study
reveals that infrequent vocabulary in LLMs' input leads to poor reasoning on
game rules and historical context
(https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash).",http://arxiv.org/pdf/2411.10422v1,,False
Multiscale Dubuc: A New Similarity Measure for Time Series,15/11/2024,"Mahsa Khazaei, Azim Ahmadzadeh, Krishna Rukmini Puthucode","Quantifying similarities between time series in a meaningful way remains a
challenge in time series analysis, despite many advances in the field. Most
real-world solutions still rely on a few popular measures, such as Euclidean
Distance (EuD), Longest Common Subsequence (LCSS), and Dynamic Time Warping
(DTW). The strengths and weaknesses of these measures have been studied
extensively, and incremental improvements have been proposed. In this study,
however, we present a different similarity measure that fuses the notion of
Dubuc's variation from fractal analysis with the Intersection-over-Union (IoU)
measure which is widely used in object recognition (also known as the Jaccard
Index). In this proof-of-concept paper, we introduce the Multiscale Dubuc
Distance (MDD) measure and prove that it is a metric, possessing desirable
properties such as the triangle inequality. We use 95 datasets from the UCR
Time Series Classification Archive to compare MDD's performance with EuD, LCSS,
and DTW. Our experiments show that MDD's overall success, without any
case-specific customization, is comparable to DTW with optimized window sizes
per dataset. We also highlight several datasets where MDD's performance
improves significantly when its single parameter is customized. This
customization serves as a powerful tool for gauging MDD's sensitivity to noise.
Lastly, we show that MDD's running time is linear in the length of the time
series, which is crucial for real-world applications involving very large
datasets.",http://arxiv.org/pdf/2411.10418v1,,False
On the Cost of Model-Serving Frameworks: An Experimental Evaluation,15/11/2024,"Pasquale De Rosa, YÃ©rom-David Bromberg, Pascal Felber, Djob Mvondo, Valerio Schiavoni","In machine learning (ML), the inference phase is the process of applying
pre-trained models to new, unseen data with the objective of making
predictions. During the inference phase, end-users interact with ML services to
gain insights, recommendations, or actions based on the input data. For this
reason, serving strategies are nowadays crucial for deploying and managing
models in production environments effectively. These strategies ensure that
models are available, scalable, reliable, and performant for real-world
applications, such as time series forecasting, image classification, natural
language processing, and so on. In this paper, we evaluate the performances of
five widely-used model serving frameworks (TensorFlow Serving, TorchServe,
MLServer, MLflow, and BentoML) under four different scenarios (malware
detection, cryptocoin prices forecasting, image classification, and sentiment
analysis). We demonstrate that TensorFlow Serving is able to outperform all the
other frameworks in serving deep learning (DL) models. Moreover, we show that
DL-specific frameworks (TensorFlow Serving and TorchServe) display
significantly lower latencies than the three general-purpose ML frameworks
(BentoML, MLFlow, and MLServer).",http://arxiv.org/pdf/2411.10337v1,10.1109/IC2E61754.2024.00032,False
A Realistic Collimated X-Ray Image Simulation Pipeline,15/11/2024,"Benjamin El-Zein, Dominik Eckert, Thomas Weber, Maximilian Rohleder, Ludwig Ritschl, Steffen Kappler, Andreas Maier","Collimator detection remains a challenging task in X-ray systems with
unreliable or non-available information about the detectors position relative
to the source. This paper presents a physically motivated image processing
pipeline for simulating the characteristics of collimator shadows in X-ray
images. By generating randomized labels for collimator shapes and locations,
incorporating scattered radiation simulation, and including Poisson noise, the
pipeline enables the expansion of limited datasets for training deep neural
networks. We validate the proposed pipeline by a qualitative and quantitative
comparison against real collimator shadows. Furthermore, it is demonstrated
that utilizing simulated data within our deep learning framework not only
serves as a suitable substitute for actual collimators but also enhances the
generalization performance when applied to real-world data.",http://arxiv.org/pdf/2411.10308v1,10.1007/978-3-031-58171-7_14,False
Multidimensional Byte Pair Encoding: Shortened Sequences for Improved Visual Data Generation,15/11/2024,"Tim Elsner, Paula Usinger, Julius Nehring-Wirxel, Gregor Kobsik, Victor Czech, Yanjiang He, Isaak Lim, Leif Kobbelt","In language processing, transformers benefit greatly from text being
condensed. This is achieved through a larger vocabulary that captures word
fragments instead of plain characters. This is often done with Byte Pair
Encoding. In the context of images, tokenisation of visual data is usually
limited to regular grids obtained from quantisation methods, without global
content awareness. Our work improves tokenisation of visual data by bringing
Byte Pair Encoding from 1D to multiple dimensions, as a complementary add-on to
existing compression. We achieve this through counting constellations of token
pairs and replacing the most frequent token pair with a newly introduced token.
The multidimensionality only increases the computation time by a factor of 2
for images, making it applicable even to large datasets like ImageNet within
minutes on consumer hardware. This is a lossless preprocessing step. Our
evaluation shows improved training and inference performance of transformers on
visual data achieved by compressing frequent constellations of tokens: The
resulting sequences are shorter, with more uniformly distributed information
content, e.g. condensing empty regions in an image into single tokens. As our
experiments show, these condensed sequences are easier to process. We
additionally introduce a strategy to amplify this compression further by
clustering the vocabulary.",http://arxiv.org/pdf/2411.10281v1,,False
Measuring Non-Adversarial Reproduction of Training Data in Large Language Models,15/11/2024,"Michael Aerni, Javier Rando, Edoardo Debenedetti, Nicholas Carlini, Daphne Ippolito, Florian TramÃ¨r","Large language models memorize parts of their training data. Memorizing short
snippets and facts is required to answer questions about the world and to be
fluent in any language. But models have also been shown to reproduce long
verbatim sequences of memorized text when prompted by a motivated adversary. In
this work, we investigate an intermediate regime of memorization that we call
non-adversarial reproduction, where we quantify the overlap between model
responses and pretraining data when responding to natural and benign prompts.
For a variety of innocuous prompt categories (e.g., writing a letter or a
tutorial), we show that up to 15% of the text output by popular conversational
language models overlaps with snippets from the Internet. In worst cases, we
find generations where 100% of the content can be found exactly online. For the
same tasks, we find that human-written text has far less overlap with Internet
data. We further study whether prompting strategies can close this reproduction
gap between models and humans. While appropriate prompting can reduce
non-adversarial reproduction on average, we find that mitigating worst-case
reproduction of training data requires stronger defenses -- even for benign
interactions.",http://arxiv.org/pdf/2411.10242v1,,False
Causal Time-Series Synchronization for Multi-Dimensional Forecasting,15/11/2024,"Michael Mayr, Georgios C. Chasparis, Josef KÃ¼ng","The process industry's high expectations for Digital Twins require modeling
approaches that can generalize across tasks and diverse domains with
potentially different data dimensions and distributional shifts i.e.,
Foundational Models. Despite success in natural language processing and
computer vision, transfer learning with (self-) supervised signals for
pre-training general-purpose models is largely unexplored in the context of
Digital Twins in the process industry due to challenges posed by
multi-dimensional time-series data, lagged cause-effect dependencies, complex
causal structures, and varying number of (exogenous) variables. We propose a
novel channel-dependent pre-training strategy that leverages synchronized
cause-effect pairs to overcome these challenges by breaking down the
multi-dimensional time-series data into pairs of cause-effect variables. Our
approach focuses on: (i) identifying highly lagged causal relationships using
data-driven methods, (ii) synchronizing cause-effect pairs to generate training
samples for channel-dependent pre-training, and (iii) evaluating the
effectiveness of this approach in channel-dependent forecasting. Our
experimental results demonstrate significant improvements in forecasting
accuracy and generalization capability compared to traditional training
methods.",http://arxiv.org/pdf/2411.10152v1,,False
"Generative Agent Simulations of 1,000 People",15/11/2024,"Joon Sung Park, Carolyn Q. Zou, Aaron Shaw, Benjamin Mako Hill, Carrie Cai, Meredith Ringel Morris, Robb Willer, Percy Liang, Michael S. Bernstein","The promise of human behavioral simulation--general-purpose computational
agents that replicate human behavior across domains--could enable broad
applications in policymaking and social science. We present a novel agent
architecture that simulates the attitudes and behaviors of 1,052 real
individuals--applying large language models to qualitative interviews about
their lives, then measuring how well these agents replicate the attitudes and
behaviors of the individuals that they represent. The generative agents
replicate participants' responses on the General Social Survey 85% as
accurately as participants replicate their own answers two weeks later, and
perform comparably in predicting personality traits and outcomes in
experimental replications. Our architecture reduces accuracy biases across
racial and ideological groups compared to agents given demographic
descriptions. This work provides a foundation for new tools that can help
investigate individual and collective behavior.",http://arxiv.org/pdf/2411.10109v1,,False
Physics-informed neural networks need a physicist to be accurate: the case of mass and heat transport in Fischer-Tropsch catalyst particles,15/11/2024,"Tymofii Nikolaienko, Harshil Patel, Aniruddha Panda, Subodh Madhav Joshi, Stanislav Jaso, Kaushic Kalyanaraman","Physics-Informed Neural Networks (PINNs) have emerged as an influential
technology, merging the swift and automated capabilities of machine learning
with the precision and dependability of simulations grounded in theoretical
physics. PINNs are often employed to solve algebraic or differential equations
to replace some or even all steps of multi-stage computational workflows,
leading to their significant speed-up. However, wide adoption of PINNs is still
hindered by reliability issues, particularly at extreme ends of the input
parameter ranges. In this study, we demonstrate this in the context of a system
of coupled non-linear differential reaction-diffusion and heat transfer
equations related to Fischer-Tropsch synthesis, which are solved by a
finite-difference method with a PINN used in evaluating their source terms. It
is shown that the testing strategies traditionally used to assess the accuracy
of neural networks as function approximators can overlook the peculiarities
which ultimately cause instabilities of the finite-difference solver. We
propose a domain knowledge-based modifications to the PINN architecture
ensuring its correct asymptotic behavior. When combined with an improved
numerical scheme employed as an initial guess generator, the proposed
modifications are shown to recover the overall stability of the simulations,
while preserving the speed-up brought by PINN as the workflow component. We
discuss the possible applications of the proposed hybrid transport equation
solver in context of chemical reactors simulations.",http://arxiv.org/pdf/2411.10048v1,,False
Orca: Enhancing Role-Playing Abilities of Large Language Models by Integrating Personality Traits,15/11/2024,Yuxuan Huang,"Large language models has catalyzed the development of personalized dialogue
systems, numerous role-playing conversational agents have emerged. While
previous research predominantly focused on enhancing the model's capability to
follow instructions by designing character profiles, neglecting the
psychological factors that drive human conversations. In this paper, we propose
Orca, a framework for data processing and training LLMs of custom characters by
integrating personality traits. Orca comprises four stages: (1) Personality
traits inferring, leverage LLMs to infer user's BigFive personality trait
reports and scores. (2) Data Augment, simulate user's profile, background
story, and psychological activities. (3) Dataset construction,
personality-conditioned instruction prompting (PCIP) to stimulate LLMs. (4)
Modeling and Training, personality-conditioned instruction tuning (PTIT and
PSIT), using the generated data to enhance existing open-source LLMs. We
introduce OrcaBench, the first benchmark for evaluating the quality of content
generated by LLMs on social platforms across multiple scales. Our experiments
demonstrate that our proposed model achieves superior performance on this
benchmark, demonstrating its excellence and effectiveness in perceiving
personality traits that significantly improve role-playing abilities. Our Code
is available at https://github.com/Aipura/Orca.",http://arxiv.org/pdf/2411.10006v1,,False
Unlocking Transfer Learning for Open-World Few-Shot Recognition,15/11/2024,"Byeonggeun Kim, Juntae Lee, Kyuhong Shim, Simyung Chang","Few-Shot Open-Set Recognition (FSOSR) targets a critical real-world
challenge, aiming to categorize inputs into known categories, termed closed-set
classes, while identifying open-set inputs that fall outside these classes.
Although transfer learning where a model is tuned to a given few-shot task has
become a prominent paradigm in closed-world, we observe that it fails to expand
to open-world. To unlock this challenge, we propose a two-stage method which
consists of open-set aware meta-learning with open-set free transfer learning.
In the open-set aware meta-learning stage, a model is trained to establish a
metric space that serves as a beneficial starting point for the subsequent
stage. During the open-set free transfer learning stage, the model is further
adapted to a specific target task through transfer learning. Additionally, we
introduce a strategy to simulate open-set examples by modifying the training
dataset or generating pseudo open-set examples. The proposed method achieves
state-of-the-art performance on two widely recognized benchmarks, miniImageNet
and tieredImageNet, with only a 1.5\% increase in training effort. Our work
demonstrates the effectiveness of transfer learning in FSOSR.",http://arxiv.org/pdf/2411.09986v1,,False
Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs,15/11/2024,"Xiaofeng Zhang, Yihao Quan, Chaochen Gu, Chen Shen, Xiaosong Yuan, Shaotian Yan, Hao Cheng, Kaijie Wu, Jieping Ye","The hallucination problem in multimodal large language models (MLLMs) remains
a common issue. Although image tokens occupy a majority of the input sequence
of MLLMs, there is limited research to explore the relationship between image
tokens and hallucinations. In this paper, we analyze the distribution of
attention scores for image tokens across each layer and head of the model,
revealing an intriguing and common phenomenon: most hallucinations are closely
linked to the pattern of attention sinks in the self-attention matrix of image
tokens, where shallow layers exhibit dense attention sinks and deeper layers
show sparse attention sinks. We further analyze the attention heads of
different layers and find that heads with high-density attention sink in the
image part play a positive role in alleviating hallucinations. In this paper,
we propose a training-free method named \textcolor{red}{\textbf{E}}nhancing
\textcolor{red}{\textbf{A}}ttention \textcolor{red}{\textbf{H}}eads (EAH), an
approach designed to enhance the convergence of image tokens attention sinks in
the shallow layers. EAH identifies the attention head that shows the vision
sink in a shallow layer and extracts its attention matrix. This attention map
is then broadcast to other heads in the layer, thereby strengthening the layer
to pay more attention to the image itself. With extensive experiments, EAH
shows significant hallucination-mitigating performance on different MLLMs and
metrics, proving its effectiveness and generality.",http://arxiv.org/pdf/2411.09968v1,,False
Dense ReLU Neural Networks for Temporal-spatial Model,15/11/2024,"Zhi Zhang, Carlos Misael Madrid Padilla, Xiaokai Luo, Oscar Hernan Madrid Padilla, Daren Wang","In this paper, we focus on fully connected deep neural networks utilizing the
Rectified Linear Unit (ReLU) activation function for nonparametric estimation.
We derive non-asymptotic bounds that lead to convergence rates, addressing both
temporal and spatial dependence in the observed measurements. By accounting for
dependencies across time and space, our models better reflect the complexities
of real-world data, enhancing both predictive performance and theoretical
robustness. We also tackle the curse of dimensionality by modeling the data on
a manifold, exploring the intrinsic dimensionality of high-dimensional data. We
broaden existing theoretical findings of temporal-spatial analysis by applying
them to neural networks in more general contexts and demonstrate that our proof
techniques are effective for models with short-range dependence. Our empirical
simulations across various synthetic response functions underscore the superior
performance of our method, outperforming established approaches in the existing
literature. These findings provide valuable insights into the strong
capabilities of dense neural networks for temporal-spatial modeling across a
broad range of function classes.",http://arxiv.org/pdf/2411.09961v1,,False
