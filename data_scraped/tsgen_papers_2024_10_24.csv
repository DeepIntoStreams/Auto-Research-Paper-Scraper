Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration,23/10/2024,"Xin Li, Qizhi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zekai Yu, Weize Chen, Chen Qian, Chuan Shi, Cheng Yang","Graphs are widely used for modeling relational data in real-world scenarios,
such as social networks and urban computing. Existing LLM-based graph analysis
approaches either integrate graph neural networks (GNNs) for specific machine
learning tasks, limiting their transferability, or rely solely on LLMs'
internal reasoning ability, resulting in suboptimal performance. To address
these limitations, we take advantage of recent advances in LLM-based agents,
which have shown capabilities of utilizing external knowledge or tools for
problem solving. By simulating human problem-solving strategies such as analogy
and collaboration, we propose a multi-agent system based on LLMs named
GraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from
three modules, and the agents with different specialities can collaborate with
each other to address complex problems. Specifically, (1) input-output
normalization module: the question agent extracts and refines four key
arguments from the original question, facilitating the problem understanding,
and the answer agent organizes the results to meet the output requirement; (2)
external knowledge retrieval module: we first build a knowledge base consisting
of relevant documentation and experience information, and then the search agent
retrieves the most relevant entries for each question. (3) problem-solving
module: given the retrieved information from search agent, the coding agent
uses established algorithms via programming to generate solutions, and in case
the coding agent does not work, the reasoning agent will directly compute the
results without programming. Extensive experiments on six graph analysis
benchmarks demonstrate that GraphTeam achieves state-of-the-art performance
with an average 25.85% improvement over the best baseline in terms of accuracy.
The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.",http://arxiv.org/pdf/2410.18032v1,,False
Dynamic Spectrum Access for Ambient Backscatter Communication-assisted D2D Systems with Quantum Reinforcement Learning,23/10/2024,"Nguyen Van Huynh, Bolun Zhang, Dinh-Hieu Tran, Dinh Thai Hoang, Diep N. Nguyen, Gan Zheng, Dusit Niyato, Quoc-Viet Pham","Spectrum access is an essential problem in device-to-device (D2D)
communications. However, with the recent growth in the number of mobile
devices, the wireless spectrum is becoming scarce, resulting in low spectral
efficiency for D2D communications. To address this problem, this paper aims to
integrate the ambient backscatter communication technology into D2D devices to
allow them to backscatter ambient RF signals to transmit their data when the
shared spectrum is occupied by mobile users. To obtain the optimal spectrum
access policy, i.e., stay idle or access the shared spectrum and perform active
transmissions or backscattering ambient RF signals for transmissions, to
maximize the average throughput for D2D users, deep reinforcement learning
(DRL) can be adopted. However, DRL-based solutions may require long training
time due to the curse of dimensionality issue as well as complex deep neural
network architectures. For that, we develop a novel quantum reinforcement
learning (RL) algorithm that can achieve a faster convergence rate with fewer
training parameters compared to DRL thanks to the quantum superposition and
quantum entanglement principles. Specifically, instead of using conventional
deep neural networks, the proposed quantum RL algorithm uses a parametrized
quantum circuit to approximate an optimal policy. Extensive simulations then
demonstrate that the proposed solution not only can significantly improve the
average throughput of D2D devices when the shared spectrum is busy but also can
achieve much better performance in terms of convergence rate and learning
complexity compared to existing DRL-based methods.",http://arxiv.org/pdf/2410.17971v1,,False
POMDP-Driven Cognitive Massive MIMO Radar: Joint Target Detection-Tracking In Unknown Disturbances,23/10/2024,"Imad Bouhou, Stefano Fortunati, Leila Gharsalli, Alexandre Renaux","The joint detection and tracking of a moving target embedded in an unknown
disturbance represents a key feature that motivates the development of the
cognitive radar paradigm. Building upon recent advancements in robust target
detection with multiple-input multiple-output (MIMO) radars, this work explores
the application of a Partially Observable Markov Decision Process (POMDP)
framework to enhance the tracking and detection tasks in a statistically
unknown environment. In the POMDP setup, the radar system is considered as an
intelligent agent that continuously senses the surrounding environment,
optimizing its actions to maximize the probability of detection $(P_D)$ and
improve the target position and velocity estimation, all this while keeping a
constant probability of false alarm $(P_{FA})$. The proposed approach employs
an online algorithm that does not require any apriori knowledge of the noise
statistics, and it relies on a much more general observation model than the
traditional range-azimuth-elevation model employed by conventional tracking
algorithms. Simulation results clearly show substantial performance improvement
of the POMDP-based algorithm compared to the State-Action-Reward-State-Action
(SARSA)-based one that has been recently investigated in the context of massive
MIMO (MMIMO) radar systems.",http://arxiv.org/pdf/2410.17967v1,,False
Semi-Implicit Functional Gradient Flow,23/10/2024,"Shiyue Zhang, Ziheng Cheng, Cheng Zhang","Particle-based variational inference methods (ParVIs) use non-parametric
variational families represented by particles to approximate the target
distribution according to the kernelized Wasserstein gradient flow for the
Kullback-Leibler (KL) divergence. Recent works introduce functional gradient
flows to substitute the kernel for better flexibility. However, the
deterministic updating mechanism may suffer from limited exploration and
require expensive repetitive runs for new samples. In this paper, we propose
Semi-Implicit Functional Gradient flow (SIFG), a functional gradient ParVI
method that uses perturbed particles as the approximation family. The
corresponding functional gradient flow, which can be estimated via denoising
score matching, exhibits strong theoretical convergence guarantee. We also
present an adaptive version of our method to automatically choose the suitable
noise magnitude. Extensive experiments demonstrate the effectiveness and
efficiency of the proposed framework on both simulated and real data problems.",http://arxiv.org/pdf/2410.17935v1,,False
Addressing Asynchronicity in Clinical Multimodal Fusion via Individualized Chest X-ray Generation,23/10/2024,"Wenfang Yao, Chen Liu, Kejing Yin, William K. Cheung, Jing Qin","Integrating multi-modal clinical data, such as electronic health records
(EHR) and chest X-ray images (CXR), is particularly beneficial for clinical
prediction tasks. However, in a temporal setting, multi-modal data are often
inherently asynchronous. EHR can be continuously collected but CXR is generally
taken with a much longer interval due to its high cost and radiation dose. When
clinical prediction is needed, the last available CXR image might have been
outdated, leading to suboptimal predictions. To address this challenge, we
propose DDL-CXR, a method that dynamically generates an up-to-date latent
representation of the individualized CXR images. Our approach leverages latent
diffusion models for patient-specific generation strategically conditioned on a
previous CXR image and EHR time series, providing information regarding
anatomical structures and disease progressions, respectively. In this way, the
interaction across modalities could be better captured by the latent CXR
generation process, ultimately improving the prediction performance.
Experiments using MIMIC datasets show that the proposed model could effectively
address asynchronicity in multimodal fusion and consistently outperform
existing methods.",http://arxiv.org/pdf/2410.17918v1,,False
Lightweight Neural App Control,23/10/2024,"Filippos Christianos, Georgios Papoudakis, Thomas Coste, Jianye Hao, Jun Wang, Kun Shao","This paper introduces a novel mobile phone control architecture, termed ``app
agents"", for efficient interactions and controls across various Android apps.
The proposed Lightweight Multi-modal App Control (LiMAC) takes as input a
textual goal and a sequence of past mobile observations, such as screenshots
and corresponding UI trees, to generate precise actions. To address the
computational constraints inherent to smartphones, within LiMAC, we introduce a
small Action Transformer (AcT) integrated with a fine-tuned vision-language
model (VLM) for real-time decision-making and task execution. We evaluate LiMAC
on two open-source mobile control datasets, demonstrating the superior
performance of our small-form-factor approach against fine-tuned versions of
open-source VLMs, such as Florence2 and Qwen2-VL. It also significantly
outperforms prompt engineering baselines utilising closed-source foundation
models like GPT-4o. More specifically, LiMAC increases the overall action
accuracy by up to 19% compared to fine-tuned VLMs, and up to 42% compared to
prompt-engineering baselines.",http://arxiv.org/pdf/2410.17883v1,,False
The Probabilistic Tsetlin Machine: A Novel Approach to Uncertainty Quantification,23/10/2024,"K. Darshana Abeyrathna, Sara El Mekkaoui, Andreas Hafver, Christian Agrell","Tsetlin Machines (TMs) have emerged as a compelling alternative to
conventional deep learning methods, offering notable advantages such as smaller
memory footprint, faster inference, fault-tolerant properties, and
interpretability. Although various adaptations of TMs have expanded their
applicability across diverse domains, a fundamental gap remains in
understanding how TMs quantify uncertainty in their predictions. In response,
this paper introduces the Probabilistic Tsetlin Machine (PTM) framework, aimed
at providing a robust, reliable, and interpretable approach for uncertainty
quantification. Unlike the original TM, the PTM learns the probability of
staying on each state of each Tsetlin Automaton (TA) across all clauses. These
probabilities are updated using the feedback tables that are part of the TM
framework: Type I and Type II feedback. During inference, TAs decide their
actions by sampling states based on learned probability distributions, akin to
Bayesian neural networks when generating weight values. In our experimental
analysis, we first illustrate the spread of the probabilities across TA states
for the noisy-XOR dataset. Then we evaluate the PTM alongside benchmark models
using both simulated and real-world datasets. The experiments on the simulated
dataset reveal the PTM's effectiveness in uncertainty quantification,
particularly in delineating decision boundaries and identifying regions of high
uncertainty. Moreover, when applied to multiclass classification tasks using
the Iris dataset, the PTM demonstrates competitive performance in terms of
predictive entropy and expected calibration error, showcasing its potential as
a reliable tool for uncertainty estimation. Our findings underscore the
importance of selecting appropriate models for accurate uncertainty
quantification in predictive tasks, with the PTM offering a particularly
interpretable and effective solution.",http://arxiv.org/pdf/2410.17851v1,,False
Optimal Streaming Algorithms for Multi-Armed Bandits,23/10/2024,"Tianyuan Jin, Keke Huang, Jing Tang, Xiaokui Xiao","This paper studies two variants of the best arm identification (BAI) problem
under the streaming model, where we have a stream of $n$ arms with reward
distributions supported on $[0,1]$ with unknown means. The arms in the stream
are arriving one by one, and the algorithm cannot access an arm unless it is
stored in a limited size memory.
  We first study the streaming \eps-$top$-$k$ arms identification problem,
which asks for $k$ arms whose reward means are lower than that of the $k$-th
best arm by at most $\eps$ with probability at least $1-\delta$. For general
$\eps \in (0,1)$, the existing solution for this problem assumes $k = 1$ and
achieves the optimal sample complexity $O(\frac{n}{\eps^2} \log
\frac{1}{\delta})$ using $O(\log^*(n))$ ($\log^*(n)$ equals the number of times
that we need to apply the logarithm function on $n$ before the results is no
more than 1.) memory and a single pass of the stream. We propose an algorithm
that works for any $k$ and achieves the optimal sample complexity
$O(\frac{n}{\eps^2} \log\frac{k}{\delta})$ using a single-arm memory and a
single pass of the stream.
  Second, we study the streaming BAI problem, where the objective is to
identify the arm with the maximum reward mean with at least $1-\delta$
probability, using a single-arm memory and as few passes of the input stream as
possible. We present a single-arm-memory algorithm that achieves a near
instance-dependent optimal sample complexity within $O(\log \Delta_2^{-1})$
passes, where $\Delta_2$ is the gap between the mean of the best arm and that
of the second best arm.",http://arxiv.org/pdf/2410.17835v1,,False
PGDiffSeg: Prior-Guided Denoising Diffusion Model with Parameter-Shared Attention for Breast Cancer Segmentation,23/10/2024,"Feiyan Feng, Tianyu Liu, Hong Wang, Jun Zhao, Wei Li, Yanshen Sun","Early detection through imaging and accurate diagnosis is crucial in
mitigating the high mortality rate associated with breast cancer. However,
locating tumors from low-resolution and high-noise medical images is extremely
challenging. Therefore, this paper proposes a novel PGDiffSeg (Prior-Guided
Diffusion Denoising Model with Parameter-Shared Attention) that applies
diffusion denoising methods to breast cancer medical image segmentation,
accurately recovering the affected areas from Gaussian noise. Firstly, we
design a parallel pipeline for noise processing and semantic information
processing and propose a parameter-shared attention module (PSA) in multi-layer
that seamlessly integrates these two pipelines. This integration empowers
PGDiffSeg to incorporate semantic details at multiple levels during the
denoising process, producing highly accurate segmentation maps. Secondly, we
introduce a guided strategy that leverages prior knowledge to simulate the
decision-making process of medical professionals, thereby enhancing the model's
ability to locate tumor positions precisely. Finally, we provide the first-ever
discussion on the interpretability of the generative diffusion model in the
context of breast cancer segmentation. Extensive experiments have demonstrated
the superiority of our model over the current state-of-the-art approaches,
confirming its effectiveness as a flexible diffusion denoising method suitable
for medical image research. Our code will be publicly available later.",http://arxiv.org/pdf/2410.17812v1,,False
VISAGE: Video Synthesis using Action Graphs for Surgery,23/10/2024,"Yousef Yeganeh, Rachmadio Lazuardi, Amir Shamseddin, Emine Dari, Yash Thirani, Nassir Navab Azade Farshad","Surgical data science (SDS) is a field that analyzes patient data before,
during, and after surgery to improve surgical outcomes and skills. However,
surgical data is scarce, heterogeneous, and complex, which limits the
applicability of existing machine learning methods. In this work, we introduce
the novel task of future video generation in laparoscopic surgery. This task
can augment and enrich the existing surgical data and enable various
applications, such as simulation, analysis, and robot-aided surgery.
Ultimately, it involves not only understanding the current state of the
operation but also accurately predicting the dynamic and often unpredictable
nature of surgical procedures. Our proposed method, VISAGE (VIdeo Synthesis
using Action Graphs for Surgery), leverages the power of action scene graphs to
capture the sequential nature of laparoscopic procedures and utilizes diffusion
models to synthesize temporally coherent video sequences. VISAGE predicts the
future frames given only a single initial frame, and the action graph triplets.
By incorporating domain-specific knowledge through the action graph, VISAGE
ensures the generated videos adhere to the expected visual and motion patterns
observed in real laparoscopic procedures. The results of our experiments
demonstrate high-fidelity video generation for laparoscopy procedures, which
enables various applications in SDS.",http://arxiv.org/pdf/2410.17751v1,,False
FuzzWiz -- Fuzzing Framework for Efficient Hardware Coverage,23/10/2024,"Deepak Narayan Gadde, Aman Kumar, Djones Lettnin, Sebastian Simon","Ever-increasing design complexity of System-on-Chips (SoCs) led to
significant verification challenges. Unlike software, bugs in hardware design
are vigorous and eternal i.e., once the hardware is fabricated, it cannot be
repaired with any patch. Despite being one of the powerful techniques used in
verification, the dynamic random approach cannot give confidence to complex
Register Transfer Leve (RTL) designs during the pre-silicon design phase. In
particular, achieving coverage targets and exposing bugs is a complicated task
with random simulations. In this paper, we leverage an existing testing
solution available in the software world known as fuzzing and apply it to
hardware verification in order to achieve coverage targets in quick time. We
created an automated hardware fuzzing framework FuzzWiz using metamodeling and
Python to achieve coverage goals faster. It includes parsing the RTL design
module, converting it into C/C++ models, creating generic testbench with
assertions, fuzzer-specific compilation, linking, and fuzzing. Furthermore, it
is configurable and provides the debug flow if any crash is detected during the
fuzzing process. The proposed framework is applied on four IP blocks from
Google's OpenTitan chip with various fuzzing engines to show its scalability
and compatibility. Our benchmarking results show that we could achieve around
90% of the coverage 10 times faster than traditional simulation regression
based approach.",http://arxiv.org/pdf/2410.17732v1,,False
Optimizing Load Scheduling in Power Grids Using Reinforcement Learning and Markov Decision Processes,23/10/2024,Dongwen Luo,"Power grid load scheduling is a critical task that ensures the balance
between electricity generation and consumption while minimizing operational
costs and maintaining grid stability. Traditional optimization methods often
struggle with the dynamic and stochastic nature of power systems, especially
when faced with renewable energy sources and fluctuating demand. This paper
proposes a reinforcement learning (RL) approach using a Markov Decision Process
(MDP) framework to address the challenges of dynamic load scheduling. The MDP
is defined by a state space representing grid conditions, an action space
covering control operations like generator adjustments and storage management,
and a reward function balancing economic efficiency and system reliability. We
investigate the application of various RL algorithms, from basic Q-Learning to
more advanced Deep Q-Networks (DQN) and Actor-Critic methods, to determine
optimal scheduling policies. The proposed approach is evaluated through a
simulated power grid environment, demonstrating its potential to improve
scheduling efficiency and adapt to variable demand patterns. Our results show
that the RL-based method provides a robust and scalable solution for real-time
load scheduling, contributing to the efficient management of modern power
grids.",http://arxiv.org/pdf/2410.17696v1,,False
Entity-based Reinforcement Learning for Autonomous Cyber Defence,23/10/2024,"Isaac Symes Thompson, Alberto Caron, Chris Hicks, Vasilios Mavroudis","A significant challenge for autonomous cyber defence is ensuring a defensive
agent's ability to generalise across diverse network topologies and
configurations.
  This capability is necessary for agents to remain effective when deployed in
dynamically changing environments, such as an enterprise network where devices
may frequently join and leave.
  Standard approaches to deep reinforcement learning, where policies are
parameterised using a fixed-input multi-layer perceptron (MLP) expect
fixed-size observation and action spaces. In autonomous cyber defence, this
makes it hard to develop agents that generalise to environments with network
topologies different from those trained on, as the number of nodes affects the
natural size of the observation and action spaces. To overcome this limitation,
we reframe the problem of autonomous network defence using entity-based
reinforcement learning, where the observation and action space of an agent are
decomposed into a collection of discrete entities. This framework enables the
use of policy parameterisations specialised in compositional generalisation.
Namely, we train a Transformer-based policy on the Yawning Titan cyber-security
simulation environment and test its generalisation capabilities across various
network topologies. We demonstrate that this approach significantly outperforms
an MLP-based policy on fixed networks, and has the ability for zero-shot
generalisation to networks of a different size to those seen in training.
  These findings highlight the potential for entity-based reinforcement
learning to advance the field of autonomous cyber defence by providing more
generalisable policies capable of handling variations in real-world network
environments.",http://arxiv.org/pdf/2410.17647v1,10.1145/3689933.3690835,False
ImDy: Human Inverse Dynamics from Imitated Observations,23/10/2024,"Xinpeng Liu, Junxuan Liang, Zili Lin, Haowen Hou, Yong-Lu Li, Cewu Lu","Inverse dynamics (ID), which aims at reproducing the driven torques from
human kinematic observations, has been a critical tool for gait analysis.
However, it is hindered from wider application to general motion due to its
limited scalability. Conventional optimization-based ID requires expensive
laboratory setups, restricting its availability. To alleviate this problem, we
propose to exploit the recently progressive human motion imitation algorithms
to learn human inverse dynamics in a data-driven manner. The key insight is
that the human ID knowledge is implicitly possessed by motion imitators, though
not directly applicable. In light of this, we devise an efficient data
collection pipeline with state-of-the-art motion imitation algorithms and
physics simulators, resulting in a large-scale human inverse dynamics benchmark
as Imitated Dynamics (ImDy). ImDy contains over 150 hours of motion with joint
torque and full-body ground reaction force data. With ImDy, we train a
data-driven human inverse dynamics solver ImDyS(olver) in a fully supervised
manner, which conducts ID and ground reaction force estimation simultaneously.
Experiments on ImDy and real-world data demonstrate the impressive competency
of ImDyS in human inverse dynamics and ground reaction force estimation.
Moreover, the potential of ImDy(-S) as a fundamental motion analysis tool is
exhibited with downstream applications. The project page is
https://foruck.github.io/ImDy/.",http://arxiv.org/pdf/2410.17610v1,,False
Predicting Company Growth by Econophysics informed Machine Learning,23/10/2024,"Ruyi Tao, Kaiwei Liu, Xu Jing, Jiang Zhang","Predicting company growth is crucial for strategic adjustment, operational
decision-making, risk assessment, and loan eligibility reviews. Traditional
models for company growth often focus too much on theory, overlooking practical
forecasting, or they rely solely on time series forecasting techniques,
ignoring interpretability and the inherent mechanisms of company growth. In
this paper, we propose a machine learning-based prediction framework that
incorporates an econophysics model for company growth. Our model captures both
the intrinsic growth mechanisms of companies led by scaling laws and the
fluctuations influenced by random factors and individual decisions,
demonstrating superior predictive performance compared with methods that use
time series techniques alone. Its advantages are more pronounced in long-range
prediction tasks. By explicitly modeling the baseline growth and volatility
components, our model is more interpretable.",http://arxiv.org/pdf/2410.17587v1,,False
Exploring Tokenization Methods for Multitrack Sheet Music Generation,23/10/2024,"Yashan Wang, Shangda Wu, Xingjian Du, Maosong Sun","This study explores the tokenization of multitrack sheet music in ABC
notation, introducing two methods--bar-stream and line-stream patching. We
compare these methods against existing techniques, including bar patching, byte
patching, and Byte Pair Encoding (BPE). In terms of both computational
efficiency and the musicality of the generated compositions, experimental
results show that bar-stream patching performs best overall compared to the
others, which makes it a promising tokenization strategy for sheet music
generation.",http://arxiv.org/pdf/2410.17584v1,,False
Time and Frequency Synergy for Source-Free Time-Series Domain Adaptations,23/10/2024,"Muhammad Tanzil Furqon, Mahardhika Pratama, Ary Mazharuddin Shiddiqi, Lin Liu, Habibullah Habibullah, Kutluyil Dogancay","The issue of source-free time-series domain adaptations still gains scarce
research attentions. On the other hand, existing approaches rely solely on
time-domain features ignoring frequency components providing complementary
information. This paper proposes Time Frequency Domain Adaptation (TFDA), a
method to cope with the source-free time-series domain adaptation problems.
TFDA is developed with a dual branch network structure fully utilizing both
time and frequency features in delivering final predictions. It induces
pseudo-labels based on a neighborhood concept where predictions of a sample
group are aggregated to generate reliable pseudo labels. The concept of
contrastive learning is carried out in both time and frequency domains with
pseudo label information and a negative pair exclusion strategy to make valid
neighborhood assumptions. In addition, the time-frequency consistency technique
is proposed using the self-distillation strategy while the uncertainty
reduction strategy is implemented to alleviate uncertainties due to the domain
shift problem. Last but not least, the curriculum learning strategy is
integrated to combat noisy pseudo labels. Our experiments demonstrate the
advantage of our approach over prior arts with noticeable margins in benchmark
problems.",http://arxiv.org/pdf/2410.17511v1,,False
