Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version),04/09/2024,"Yao Mu, Tianxing Chen, Shijia Peng, Zanxin Chen, Zeyu Gao, Yude Zou, Lunkai Lin, Zhiqiang Xie, Ping Luo","Effective collaboration of dual-arm robots and their tool use capabilities
are increasingly important areas in the advancement of robotics. These skills
play a significant role in expanding robots' ability to operate in diverse
real-world environments. However, progress is impeded by the scarcity of
specialized training data. This paper introduces RoboTwin, a novel benchmark
dataset combining real-world teleoperated data with synthetic data from digital
twins, designed for dual-arm robotic scenarios. Using the COBOT Magic platform,
we have collected diverse data on tool usage and human-robot interaction. We
present a innovative approach to creating digital twins using AI-generated
content, transforming 2D images into detailed 3D models. Furthermore, we
utilize large language models to generate expert-level training data and
task-specific pose sequences oriented toward functionality. Our key
contributions are: 1) the RoboTwin benchmark dataset, 2) an efficient
real-to-simulation pipeline, and 3) the use of language models for automatic
expert-level data generation. These advancements are designed to address the
shortage of robotic training data, potentially accelerating the development of
more capable and versatile robotic systems for a wide range of real-world
applications. The project page is available at
https://robotwin-benchmark.github.io/early-version/",http://arxiv.org/pdf/2409.02920v1,,False
Hybrid Imitation-Learning Motion Planner for Urban Driving,04/09/2024,"Cristian Gariboldi, Matteo Corno, Beng Jin","With the release of open source datasets such as nuPlan and Argoverse, the
research around learning-based planners has spread a lot in the last years.
Existing systems have shown excellent capabilities in imitating the human
driver behaviour, but they struggle to guarantee safe closed-loop driving.
Conversely, optimization-based planners offer greater security in short-term
planning scenarios. To confront this challenge, in this paper we propose a
novel hybrid motion planner that integrates both learning-based and
optimization-based techniques. Initially, a multilayer perceptron (MLP)
generates a human-like trajectory, which is then refined by an
optimization-based component. This component not only minimizes tracking errors
but also computes a trajectory that is both kinematically feasible and
collision-free with obstacles and road boundaries. Our model effectively
balances safety and human-likeness, mitigating the trade-off inherent in these
objectives. We validate our approach through simulation experiments and further
demonstrate its efficacy by deploying it in real-world self-driving vehicles.",http://arxiv.org/pdf/2409.02871v1,,False
"Building a Scalable, Effective, and Steerable Search and Ranking Platform",04/09/2024,"Marjan Celikik, Jacek Wasilewski, Ana Peleteiro Ramallo, Alexey Kurennoy, Evgeny Labzin, Danilo Ascione, Tural Gurbanov, GÃ©raud Le Falher, Andrii Dzhoha, Ian Harris","Modern e-commerce platforms offer vast product selections, making it
difficult for customers to find items that they like and that are relevant to
their current session intent. This is why it is key for e-commerce platforms to
have near real-time scalable and adaptable personalized ranking and search
systems. While numerous methods exist in the scientific literature for building
such systems, many are unsuitable for large-scale industrial use due to
complexity and performance limitations. Consequently, industrial ranking
systems often resort to computationally efficient yet simplistic retrieval or
candidate generation approaches, which overlook near real-time and
heterogeneous customer signals, which results in a less personalized and
relevant experience. Moreover, related customer experiences are served by
completely different systems, which increases complexity, maintenance, and
inconsistent experiences.
  In this paper, we present a personalized, adaptable near real-time ranking
platform that is reusable across various use cases, such as browsing and
search, and that is able to cater to millions of items and customers under
heavy load (thousands of requests per second). We employ transformer-based
models through different ranking layers which can learn complex behavior
patterns directly from customer action sequences while being able to
incorporate temporal (e.g. in-session) and contextual information. We validate
our system through a series of comprehensive offline and online real-world
experiments at a large online e-commerce platform, and we demonstrate its
superiority when compared to existing systems, both in terms of customer
experience as well as in net revenue. Finally, we share the lessons learned
from building a comprehensive, modern ranking platform for use in a large-scale
e-commerce environment.",http://arxiv.org/pdf/2409.02856v1,,False
Regularized Multi-output Gaussian Convolution Process with Domain Adaptation,04/09/2024,"Wang Xinming, Wang Chao, Song Xuan, Kirby Levi, Wu Jianguo","Multi-output Gaussian process (MGP) has been attracting increasing attention
as a transfer learning method to model multiple outputs. Despite its high
flexibility and generality, MGP still faces two critical challenges when
applied to transfer learning. The first one is negative transfer, which occurs
when there exists no shared information among the outputs. The second challenge
is the input domain inconsistency, which is commonly studied in transfer
learning yet not explored in MGP. In this paper, we propose a regularized MGP
modeling framework with domain adaptation to overcome these challenges. More
specifically, a sparse covariance matrix of MGP is proposed by using
convolution process, where penalization terms are added to adaptively select
the most informative outputs for knowledge transfer. To deal with the domain
inconsistency, a domain adaptation method is proposed by marginalizing
inconsistent features and expanding missing features to align the input domains
among different outputs. Statistical properties of the proposed method are
provided to guarantee the performance practically and asymptotically. The
proposed framework outperforms state-of-the-art benchmarks in comprehensive
simulation studies and one real case study of a ceramic manufacturing process.
The results demonstrate the effectiveness of our method in dealing with both
the negative transfer and the domain inconsistency.",http://arxiv.org/pdf/2409.02778v1,10.1109/TPAMI.2022.3205036,False
Task-Oriented Communication for Graph Data: A Graph Information Bottleneck Approach,04/09/2024,"Shujing Li, Yanhu Wang, Shuaishuai Guo, Chenyuan Feng","Graph data, essential in fields like knowledge representation and social
networks, often involves large networks with many nodes and edges. Transmitting
these graphs can be highly inefficient due to their size and redundancy for
specific tasks. This paper introduces a method to extract a smaller,
task-focused subgraph that maintains key information while reducing
communication overhead. Our approach utilizes graph neural networks (GNNs) and
the graph information bottleneck (GIB) principle to create a compact,
informative, and robust graph representation suitable for transmission. The
challenge lies in the irregular structure of graph data, making GIB
optimization complex. We address this by deriving a tractable variational upper
bound for the objective function. Additionally, we propose the VQ-GIB
mechanism, integrating vector quantization (VQ) to convert subgraph
representations into a discrete codebook sequence, compatible with existing
digital communication systems. Our experiments show that this GIB-based method
significantly lowers communication costs while preserving essential
task-related information. The approach demonstrates robust performance across
various communication channels, suitable for both continuous and discrete
systems.",http://arxiv.org/pdf/2409.02728v1,,False
Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon,04/09/2024,Ramon Tavares,"This study presents a comprehensive methodology for modeling and forecasting
the historical time series of fire spots detected by the AQUA_M-T satellite in
the Amazon, Brazil. The approach utilizes a mixed Recurrent Neural Network
(RNN) model, combining Long Short-Term Memory (LSTM) and Gated Recurrent Unit
(GRU) architectures to predict monthly accumulations of daily detected fire
spots. A summary of the data revealed a consistent seasonality over time, with
annual maximum and minimum fire spot values tending to repeat at the same
periods each year. The primary objective is to verify whether the forecasts
capture this inherent seasonality through rigorous statistical analysis. The
methodology involved careful data preparation, model configuration, and
training using cross-validation with two seeds, ensuring that the data
generalizes well to the test and validation sets, and confirming the
convergence of the model parameters. The results indicate that the mixed LSTM
and GRU model offers improved accuracy in forecasting 12 months ahead,
demonstrating its effectiveness in capturing complex temporal patterns and
modeling the observed time series. This research significantly contributes to
the application of deep learning techniques in environmental monitoring,
specifically in fire spot forecasting. In addition to improving forecast
accuracy, the proposed approach highlights the potential for adaptation to
other time series forecasting challenges, opening new avenues for research and
development in machine learning and natural phenomenon prediction. Keywords:
Time Series Forecasting, Recurrent Neural Networks, Deep Learning.",http://arxiv.org/pdf/2409.02681v1,,False
Advancing Cyber Incident Timeline Analysis Through Rule Based AI and Large Language Models,04/09/2024,"Fatma Yasmine Loumachi, Mohamed Chahine Ghanem","Timeline Analysis (TA) is a key part of Timeline Forensics (TF) in Digital
Forensics (DF), focusing primarily on examining and analysing temporal digital
artefacts such as timestamps, derived from event logs, file metadata, and other
related data to correlate events resulting from cyber incidents and reconstruct
their chronological timeline. Traditional tools often struggle to efficiently
process the vast volume and variety of data acquired during DF investigations
and Incident Response (IR) processes. This paper presents a novel framework,
GenDFIR, that combines Rule-Based Artificial Intelligence (R-BAI) algorithms
with Large Language Models (LLMs) to advance and automate the TA process. Our
approach consists of two main stages (1) We use R-BAI to identify and select
anomalous digital artefacts based on predefined rules. (2) The selected
artefacts are then converted into embeddings for processing by an LLM with the
help of a Retrieval-Augmented Generation (RAG) agent. The LLM consequently
leverages its capabilities to perform automated TA on the artefacts and predict
potential incident scenarios. To validate our framework, we evaluate GenDFIR
performance, efficiency, and reliability using various metrics across synthetic
cyber incident simulation scenarios. This paper presents a proof of concept,
where the findings demonstrate the significant potential of integrating R-BAI
and LLMs for TA. This novel approach highlights the power of Generative AI
(GenAI), specifically LLMs, and opens new avenues for advanced threat detection
and incident reconstruction, representing a significant step forward in the
field.",http://arxiv.org/pdf/2409.02572v1,,False
Cog-GA: A Large Language Models-based Generative Agent for Vision-Language Navigation in Continuous Environments,04/09/2024,"Zhiyuan Li, Yanfeng Lu, Yao Mu, Hong Qiao","Vision Language Navigation in Continuous Environments (VLN-CE) represents a
frontier in embodied AI, demanding agents to navigate freely in unbounded 3D
spaces solely guided by natural language instructions. This task introduces
distinct challenges in multimodal comprehension, spatial reasoning, and
decision-making. To address these challenges, we introduce Cog-GA, a generative
agent founded on large language models (LLMs) tailored for VLN-CE tasks. Cog-GA
employs a dual-pronged strategy to emulate human-like cognitive processes.
Firstly, it constructs a cognitive map, integrating temporal, spatial, and
semantic elements, thereby facilitating the development of spatial memory
within LLMs. Secondly, Cog-GA employs a predictive mechanism for waypoints,
strategically optimizing the exploration trajectory to maximize navigational
efficiency. Each waypoint is accompanied by a dual-channel scene description,
categorizing environmental cues into 'what' and 'where' streams as the brain.
This segregation enhances the agent's attentional focus, enabling it to discern
pertinent spatial information for navigation. A reflective mechanism
complements these strategies by capturing feedback from prior navigation
experiences, facilitating continual learning and adaptive replanning. Extensive
evaluations conducted on VLN-CE benchmarks validate Cog-GA's state-of-the-art
performance and ability to simulate human-like navigation behaviors. This
research significantly contributes to the development of strategic and
interpretable VLN-CE agents.",http://arxiv.org/pdf/2409.02522v1,,False
Reliable Deep Diffusion Tensor Estimation: Rethinking the Power of Data-Driven Optimization Routine,04/09/2024,"Jialong Li, Zhicheng Zhang, Yunwei Chen, Qiqi Lu, Ye Wu, Xiaoming Liu, QianJin Feng, Yanqiu Feng, Xinyuan Zhang","Diffusion tensor imaging (DTI) holds significant importance in clinical
diagnosis and neuroscience research. However, conventional model-based fitting
methods often suffer from sensitivity to noise, leading to decreased accuracy
in estimating DTI parameters. While traditional data-driven deep learning
methods have shown potential in terms of accuracy and efficiency, their limited
generalization to out-of-training-distribution data impedes their broader
application due to the diverse scan protocols used across centers, scanners,
and studies. This work aims to tackle these challenges and promote the use of
DTI by introducing a data-driven optimization-based method termed DoDTI. DoDTI
combines the weighted linear least squares fitting algorithm and regularization
by denoising technique. The former fits DW images from diverse acquisition
settings into diffusion tensor field, while the latter applies a deep
learning-based denoiser to regularize the diffusion tensor field instead of the
DW images, which is free from the limitation of fixed-channel assignment of the
network. The optimization object is solved using the alternating direction
method of multipliers and then unrolled to construct a deep neural network,
leveraging a data-driven strategy to learn network parameters. Extensive
validation experiments are conducted utilizing both internally simulated
datasets and externally obtained in-vivo datasets. The results, encompassing
both qualitative and quantitative analyses, showcase that the proposed method
attains state-of-the-art performance in DTI parameter estimation. Notably, it
demonstrates superior generalization, accuracy, and efficiency, rendering it
highly reliable for widespread application in the field.",http://arxiv.org/pdf/2409.02492v1,,False
Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering,04/09/2024,"Peng Wang, Huijie Zhang, Zekai Zhang, Siyi Chen, Yi Ma, Qing Qu","Recent empirical studies have demonstrated that diffusion models can
effectively learn the image distribution and generate new samples. Remarkably,
these models can achieve this even with a small number of training samples
despite a large image dimension, circumventing the curse of dimensionality. In
this work, we provide theoretical insights into this phenomenon by leveraging
key empirical observations: (i) the low intrinsic dimensionality of image data,
(ii) a union of manifold structure of image data, and (iii) the low-rank
property of the denoising autoencoder in trained diffusion models. These
observations motivate us to assume the underlying data distribution of image
data as a mixture of low-rank Gaussians and to parameterize the denoising
autoencoder as a low-rank model according to the score function of the assumed
distribution. With these setups, we rigorously show that optimizing the
training loss of diffusion models is equivalent to solving the canonical
subspace clustering problem over the training samples. Based on this
equivalence, we further show that the minimal number of samples required to
learn the underlying distribution scales linearly with the intrinsic dimensions
under the above data and model assumptions. This insight sheds light on why
diffusion models can break the curse of dimensionality and exhibit the phase
transition in learning distributions. Moreover, we empirically establish a
correspondence between the subspaces and the semantic representations of image
data, facilitating image editing. We validate these results with corroborated
experimental results on both simulated distributions and image datasets.",http://arxiv.org/pdf/2409.02426v1,,False
"Abstractive Text Summarization: State of the Art, Challenges, and Improvements",04/09/2024,"Hassan Shakil, Ahmad Farooq, Jugal Kalita","Specifically focusing on the landscape of abstractive text summarization, as
opposed to extractive techniques, this survey presents a comprehensive
overview, delving into state-of-the-art techniques, prevailing challenges, and
prospective research directions. We categorize the techniques into traditional
sequence-to-sequence models, pre-trained large language models, reinforcement
learning, hierarchical methods, and multi-modal summarization. Unlike prior
works that did not examine complexities, scalability and comparisons of
techniques in detail, this review takes a comprehensive approach encompassing
state-of-the-art methods, challenges, solutions, comparisons, limitations and
charts out future improvements - providing researchers an extensive overview to
advance abstractive summarization research. We provide vital comparison tables
across techniques categorized - offering insights into model complexity,
scalability and appropriate applications. The paper highlights challenges such
as inadequate meaning representation, factual consistency, controllable text
summarization, cross-lingual summarization, and evaluation metrics, among
others. Solutions leveraging knowledge incorporation and other innovative
strategies are proposed to address these challenges. The paper concludes by
highlighting emerging research areas like factual inconsistency,
domain-specific, cross-lingual, multilingual, and long-document summarization,
as well as handling noisy data. Our objective is to provide researchers and
practitioners with a structured overview of the domain, enabling them to better
understand the current landscape and identify potential areas for further
research and improvement.",http://arxiv.org/pdf/2409.02413v1,10.1016/j.neucom.2024.128255,False
Learning Privacy-Preserving Student Networks via Discriminative-Generative Distillation,04/09/2024,"Shiming Ge, Bochao Liu, Pengju Wang, Yong Li, Dan Zeng","While deep models have proved successful in learning rich knowledge from
massive well-annotated data, they may pose a privacy leakage risk in practical
deployment. It is necessary to find an effective trade-off between high utility
and strong privacy. In this work, we propose a discriminative-generative
distillation approach to learn privacy-preserving deep models. Our key idea is
taking models as bridge to distill knowledge from private data and then
transfer it to learn a student network via two streams. First, discriminative
stream trains a baseline classifier on private data and an ensemble of teachers
on multiple disjoint private subsets, respectively. Then, generative stream
takes the classifier as a fixed discriminator and trains a generator in a
data-free manner. After that, the generator is used to generate massive
synthetic data which are further applied to train a variational autoencoder
(VAE). Among these synthetic data, a few of them are fed into the teacher
ensemble to query labels via differentially private aggregation, while most of
them are embedded to the trained VAE for reconstructing synthetic data.
Finally, a semi-supervised student learning is performed to simultaneously
handle two tasks: knowledge transfer from the teachers with distillation on few
privately labeled synthetic data, and knowledge enhancement with tangent-normal
adversarial regularization on many triples of reconstructed synthetic data. In
this way, our approach can control query cost over private data and mitigate
accuracy degradation in a unified manner, leading to a privacy-preserving
student model. Extensive experiments and analysis clearly show the
effectiveness of the proposed approach.",http://arxiv.org/pdf/2409.02404v1,10.1109/TIP.2022.3226416,False
Neural Dynamics Model of Visual Decision-Making: Learning from Human Experts,04/09/2024,"Jie Su, Fang Cai, Shu-Kuo Zhao, Xin-Yi Wang, Tian-Yi Qian, Da-Hui Wang, Bo Hong","Uncovering the fundamental neural correlates of biological intelligence,
developing mathematical models, and conducting computational simulations are
critical for advancing new paradigms in artificial intelligence (AI). In this
study, we implemented a comprehensive visual decision-making model that spans
from visual input to behavioral output, using a neural dynamics modeling
approach. Drawing inspiration from the key components of the dorsal visual
pathway in primates, our model not only aligns closely with human behavior but
also reflects neural activities in primates, and achieving accuracy comparable
to convolutional neural networks (CNNs). Moreover, magnetic resonance imaging
(MRI) identified key neuroimaging features such as structural connections and
functional connectivity that are associated with performance in perceptual
decision-making tasks. A neuroimaging-informed fine-tuning approach was
introduced and applied to the model, leading to performance improvements that
paralleled the behavioral variations observed among subjects. Compared to
classical deep learning models, our model more accurately replicates the
behavioral performance of biological intelligence, relying on the structural
characteristics of biological neural networks rather than extensive training
data, and demonstrating enhanced resilience to perturbation.",http://arxiv.org/pdf/2409.02390v1,,False
Machine Learning Applications to Computational Plasma Physics and Reduced-Order Plasma Modeling: A Perspective,04/09/2024,"Farbod Faraji, Maryam Reza","Machine learning (ML) provides a broad spectrum of tools and architectures
that enable the transformation of data from simulations and experiments into
useful and explainable science, thereby augmenting domain knowledge.
Furthermore, ML-enhanced numerical modelling can revamp scientific computing
for real-world complex engineering systems, creating unique opportunities to
examine the operation of the technologies in detail and automate their
optimization and control. In recent years, ML applications have seen
significant growth across various scientific domains, particularly in fluid
mechanics, where ML has shown great promise in enhancing computational modeling
of fluid flows. In contrast, ML applications in numerical plasma physics
research remain relatively limited in scope and extent. Despite this, the close
relationship between fluid mechanics and plasma physics presents a valuable
opportunity to create a roadmap for transferring ML advances in fluid flow
modeling to computational plasma physics. This Perspective aims to outline such
a roadmap. We begin by discussing some general fundamental aspects of ML,
including the various categories of ML algorithms and the different types of
problems that can be solved with the help of ML. With regard to each problem
type, we then present specific examples from the use of ML in computational
fluid dynamics, reviewing several insightful prior efforts. We also review
recent ML applications in plasma physics for each problem type. The paper
discusses promising future directions and development pathways for ML in plasma
modelling within the different application areas. Additionally, we point out
prominent challenges that must be addressed to realize ML's full potential in
computational plasma physics, including the need for cost-effective
high-fidelity simulation tools for extensive data generation.",http://arxiv.org/pdf/2409.02349v1,,False
Generative Principal Component Regression via Variational Inference,03/09/2024,"Austin Talbot, Corey J Keller, David E Carlson, Alex V Kotlar","The ability to manipulate complex systems, such as the brain, to modify
specific outcomes has far-reaching implications, particularly in the treatment
of psychiatric disorders. One approach to designing appropriate manipulations
is to target key features of predictive models. While generative latent
variable models, such as probabilistic principal component analysis (PPCA), is
a powerful tool for identifying targets, they struggle incorporating
information relevant to low-variance outcomes into the latent space. When
stimulation targets are designed on the latent space in such a scenario, the
intervention can be suboptimal with minimal efficacy. To address this problem,
we develop a novel objective based on supervised variational autoencoders
(SVAEs) that enforces such information is represented in the latent space. The
novel objective can be used with linear models, such as PPCA, which we refer to
as generative principal component regression (gPCR). We show in simulations
that gPCR dramatically improves target selection in manipulation as compared to
standard PCR and SVAEs. As part of these simulations, we develop a metric for
detecting when relevant information is not properly incorporated into the
loadings. We then show in two neural datasets related to stress and social
behavior in which gPCR dramatically outperforms PCR in predictive performance
and that SVAEs exhibit low incorporation of relevant information into the
loadings. Overall, this work suggests that our method significantly improves
target selection for manipulation using latent variable models over competitor
inference schemes.",http://arxiv.org/pdf/2409.02327v1,,False
TimeDiT: General-purpose Diffusion Transformers for Time Series Foundation Model,03/09/2024,"Defu Cao, Wen Ye, Yizhou Zhang, Yan Liu","With recent advances in building foundation models for texts and video data,
there is a surge of interest in foundation models for time series. A family of
models have been developed, utilizing a temporal auto-regressive generative
Transformer architecture, whose effectiveness has been proven in Large Language
Models. While the empirical results are promising, almost all existing time
series foundation models have only been tested on well-curated ``benchmark''
datasets very similar to texts. However, real-world time series exhibit unique
challenges, such as variable channel sizes across domains, missing values, and
varying signal sampling intervals due to the multi-resolution nature of
real-world data. Additionally, the uni-directional nature of temporally
auto-regressive decoding limits the incorporation of domain knowledge, such as
physical laws expressed as partial differential equations (PDEs). To address
these challenges, we introduce the Time Diffusion Transformer (TimeDiT), a
general foundation model for time series that employs a denoising diffusion
paradigm instead of temporal auto-regressive generation. TimeDiT leverages the
Transformer architecture to capture temporal dependencies and employs diffusion
processes to generate high-quality candidate samples without imposing stringent
assumptions on the target distribution via novel masking schemes and a channel
alignment strategy. Furthermore, we propose a finetuning-free model editing
strategy that allows the seamless integration of external knowledge during the
sampling process without updating any model parameters. Extensive experiments
conducted on a varity of tasks such as forecasting, imputation, and anomaly
detection, demonstrate the effectiveness of TimeDiT.",http://arxiv.org/pdf/2409.02322v1,,False
"Attention-Based Reading, Highlighting, and Forecasting of the Limit Order Book",03/09/2024,"Jiwon Jung, Kiseop Lee","Managing high-frequency data in a limit order book (LOB) is a complex task
that often exceeds the capabilities of conventional time-series forecasting
models. Accurately predicting the entire multi-level LOB, beyond just the
mid-price, is essential for understanding high-frequency market dynamics.
However, this task is challenging due to the complex interdependencies among
compound attributes within each dimension, such as order types, features, and
levels. In this study, we explore advanced multidimensional
sequence-to-sequence models to forecast the entire multi-level LOB, including
order prices and volumes. Our main contribution is the development of a
compound multivariate embedding method designed to capture the complex
relationships between spatiotemporal features. Empirical results show that our
method outperforms other multivariate forecasting methods, achieving the lowest
forecasting error while preserving the ordinal structure of the LOB.",http://arxiv.org/pdf/2409.02277v1,,False
Optimal L-Systems for Stochastic L-system Inference Problems,03/09/2024,"Ali Lotfi, Ian McQuillan","This paper presents two novel theorems that address two open problems in
stochastic Lindenmayer-system (L-system) inference, specifically focusing on
the construction of an optimal stochastic L-system capable of generating a
given sequence of strings. The first theorem delineates a method for crafting a
stochastic L-system that maximizes the likelihood of producing a given sequence
of words through a singular derivation. Furthermore, the second theorem
determines the stochastic L-systems with the highest probability of producing a
given sequence of words with multiple possible derivations. From these, we
introduce an algorithm to infer an optimal stochastic L-system from a given
sequence. This algorithm incorporates sophisticated optimization techniques,
such as interior point methods, ensuring production of a stochastically optimal
stochastic L-system suitable for generating the given sequence. This allows for
the use of using stochastic L-systems as model for machine learning using only
positive data for training.",http://arxiv.org/pdf/2409.02259v1,,False
DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos,03/09/2024,"Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, Ying Shan","Despite significant advancements in monocular depth estimation for static
images, estimating video depth in the open world remains challenging, since
open-world videos are extremely diverse in content, motion, camera movement,
and length. We present DepthCrafter, an innovative method for generating
temporally consistent long depth sequences with intricate details for
open-world videos, without requiring any supplementary information such as
camera poses or optical flow. DepthCrafter achieves generalization ability to
open-world videos by training a video-to-depth model from a pre-trained
image-to-video diffusion model, through our meticulously designed three-stage
training strategy with the compiled paired video-depth datasets. Our training
approach enables the model to generate depth sequences with variable lengths at
one time, up to 110 frames, and harvest both precise depth details and rich
content diversity from realistic and synthetic datasets. We also propose an
inference strategy that processes extremely long videos through segment-wise
estimation and seamless stitching. Comprehensive evaluations on multiple
datasets reveal that DepthCrafter achieves state-of-the-art performance in
open-world video depth estimation under zero-shot settings. Furthermore,
DepthCrafter facilitates various downstream applications, including depth-based
visual effects and conditional video generation.",http://arxiv.org/pdf/2409.02095v1,,False
TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation,03/09/2024,"Bobby Azad, Pourya Adibfar, Kaiqun Fu","In healthcare, medical image segmentation is crucial for accurate disease
diagnosis and the development of effective treatment strategies. Early
detection can significantly aid in managing diseases and potentially prevent
their progression. Machine learning, particularly deep convolutional neural
networks, has emerged as a promising approach to addressing segmentation
challenges. Traditional methods like U-Net use encoding blocks for local
representation modeling and decoding blocks to uncover semantic relationships.
However, these models often struggle with multi-scale objects exhibiting
significant variations in texture and shape, and they frequently fail to
capture long-range dependencies in the input data. Transformers designed for
sequence-to-sequence predictions have been proposed as alternatives, utilizing
global self-attention mechanisms. Yet, they can sometimes lack precise
localization due to insufficient granular details. To overcome these
limitations, we introduce TransDAE: a novel approach that reimagines the
self-attention mechanism to include both spatial and channel-wise associations
across the entire feature space, while maintaining computational efficiency.
Additionally, TransDAE enhances the skip connection pathway with an inter-scale
interaction module, promoting feature reuse and improving localization
accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on
the Synaps multi-organ dataset, even without relying on pre-trained weights.",http://arxiv.org/pdf/2409.02018v1,,False
On the design space between molecular mechanics and machine learning force fields,03/09/2024,"Yuanqing Wang, Kenichiro Takaba, Michael S. Chen, Marcus Wieder, Yuzhi Xu, John Z. H. Zhang, Kuang Yu, Xinyan Wang, Linfeng Zhang, Daniel J. Cole, Joshua A. Rackers, Joe G. Greener, Peter Eastman, Stefano Martiniani, Mark E. Tuckerman","A force field as accurate as quantum mechanics (QM) and as fast as molecular
mechanics (MM), with which one can simulate a biomolecular system efficiently
enough and meaningfully enough to get quantitative insights, is among the most
ardent dreams of biophysicists -- a dream, nevertheless, not to be fulfilled
any time soon. Machine learning force fields (MLFFs) represent a meaningful
endeavor towards this direction, where differentiable neural functions are
parametrized to fit ab initio energies, and furthermore forces through
automatic differentiation. We argue that, as of now, the utility of the MLFF
models is no longer bottlenecked by accuracy but primarily by their speed (as
well as stability and generalizability), as many recent variants, on limited
chemical spaces, have long surpassed the chemical accuracy of $1$ kcal/mol --
the empirical threshold beyond which realistic chemical predictions are
possible -- though still magnitudes slower than MM. Hoping to kindle
explorations and designs of faster, albeit perhaps slightly less accurate
MLFFs, in this review, we focus our attention on the design space (the
speed-accuracy tradeoff) between MM and ML force fields. After a brief review
of the building blocks of force fields of either kind, we discuss the desired
properties and challenges now faced by the force field development community,
survey the efforts to make MM force fields more accurate and ML force fields
faster, envision what the next generation of MLFF might look like.",http://arxiv.org/pdf/2409.01931v1,,False
Bayesian CART models for aggregate claim modeling,03/09/2024,"Yaojun Zhang, Lanpeng Ji, Georgios Aivaliotis, Charles C. Taylor","This paper proposes three types of Bayesian CART (or BCART) models for
aggregate claim amount, namely, frequency-severity models, sequential models
and joint models. We propose a general framework for the BCART models
applicable to data with multivariate responses, which is particularly useful
for the joint BCART models with a bivariate response: the number of claims and
aggregate claim amount. To facilitate frequency-severity modeling, we
investigate BCART models for the right-skewed and heavy-tailed claim severity
data by using various distributions. We discover that the Weibull distribution
is superior to gamma and lognormal distributions, due to its ability to capture
different tail characteristics in tree models. Additionally, we find that
sequential BCART models and joint BCART models, which incorporate dependence
between the number of claims and average severity, are beneficial and thus
preferable to the frequency-severity BCART models in which independence is
assumed. The effectiveness of these models' performance is illustrated by
carefully designed simulations and real insurance data.",http://arxiv.org/pdf/2409.01908v1,,False
LASP: Surveying the State-of-the-Art in Large Language Model-Assisted AI Planning,03/09/2024,"Haoming Li, Zhaoliang Chen, Jonathan Zhang, Fei Liu","Effective planning is essential for the success of any task, from organizing
a vacation to routing autonomous vehicles and developing corporate strategies.
It involves setting goals, formulating plans, and allocating resources to
achieve them. LLMs are particularly well-suited for automated planning due to
their strong capabilities in commonsense reasoning. They can deduce a sequence
of actions needed to achieve a goal from a given state and identify an
effective course of action. However, it is frequently observed that plans
generated through direct prompting often fail upon execution. Our survey aims
to highlight the existing challenges in planning with language models, focusing
on key areas such as embodied environments, optimal scheduling, competitive and
cooperative games, task decomposition, reasoning, and planning. Through this
study, we explore how LLMs transform AI planning and provide unique insights
into the future of LM-assisted planning.",http://arxiv.org/pdf/2409.01806v1,,False
Uncertainty Quantification Using Ensemble Learning and Monte Carlo Sampling for Performance Prediction and Monitoring in Cell Culture Processes,03/09/2024,"Thanh Tung Khuat, Robert Bassett, Ellen Otte, Bogdan Gabrys","Biopharmaceutical products, particularly monoclonal antibodies (mAbs), have
gained prominence in the pharmaceutical market due to their high specificity
and efficacy. As these products are projected to constitute a substantial
portion of global pharmaceutical sales, the application of machine learning
models in mAb development and manufacturing is gaining momentum. This paper
addresses the critical need for uncertainty quantification in machine learning
predictions, particularly in scenarios with limited training data. Leveraging
ensemble learning and Monte Carlo simulations, our proposed method generates
additional input samples to enhance the robustness of the model in small
training datasets. We evaluate the efficacy of our approach through two case
studies: predicting antibody concentrations in advance and real-time monitoring
of glucose concentrations during bioreactor runs using Raman spectra data. Our
findings demonstrate the effectiveness of the proposed method in estimating the
uncertainty levels associated with process performance predictions and
facilitating real-time decision-making in biopharmaceutical manufacturing. This
contribution not only introduces a novel approach for uncertainty
quantification but also provides insights into overcoming challenges posed by
small training datasets in bioprocess development. The evaluation demonstrates
the effectiveness of our method in addressing key challenges related to
uncertainty estimation within upstream cell cultivation, illustrating its
potential impact on enhancing process control and product quality in the
dynamic field of biopharmaceuticals.",http://arxiv.org/pdf/2409.02149v1,,False
Optimal Power Grid Operations with Foundation Models,03/09/2024,"Alban Puech, Jonas Weiss, Thomas Brunschwiler, Hendrik F. Hamann","The energy transition, crucial for tackling the climate crisis, demands
integrating numerous distributed, renewable energy sources into existing grids.
Along with climate change and consumer behavioral changes, this leads to
changes and variability in generation and load patterns, introducing
significant complexity and uncertainty into grid planning and operations. While
the industry has already started to exploit AI to overcome computational
challenges of established grid simulation tools, we propose the use of AI
Foundation Models (FMs) and advances in Graph Neural Networks to efficiently
exploit poorly available grid data for different downstream tasks, enhancing
grid operations. For capturing the grid's underlying physics, we believe that
building a self-supervised model learning the power flow dynamics is a critical
first step towards developing an FM for the power grid. We show how this
approach may close the gap between the industry needs and current grid analysis
capabilities, to bring the industry closer to optimal grid operation and
planning.",http://arxiv.org/pdf/2409.02148v1,,False
Graphons of Line Graphs,03/09/2024,"Sevvandi Kandanaarachchi, Cheng Soon Ong","A graphon is the limit of a converging graph sequence. Graphons of dense
graphs are useful as they can act as a blueprint and generate graphs of
arbitrary size with similar properties. But for sparse graphs this is not the
case. Sparse graphs converge to the zero graphon, making the generated graphs
empty or edgeless. Thus, the classical graphon definition fails for sparse
graphs. Several methods have been proposed to overcome this limitation and to
understand sparse graphs more deeply. However, the fragile nature of sparse
graphs makes these methods mathematically complex. In this paper we show a
simple method that can shed light on a certain subset of sparse graphs. The
method involves mapping the original graphs to their line graphs. Line graphs
map edges to vertices and connects edges when edges in the original graph share
a vertex. We show that graphs satisfying a particular property, which we call
the square-degree property are sparse, but give rise to dense line graphs. In
particular, star graphs satisfy the square-degree property resulting in dense
line graphs and non-zero graphons of line graphs. Similarly, superlinear
preferential attachment graphs give rise to dense line graphs almost surely. In
contrast, dense graphs, including Erdos-Renyi graphs make the line graphs
sparse, resulting in the zero graphon.",http://arxiv.org/pdf/2409.01656v1,,False
A Time-Intensity Aware Pipeline for Generating Late-Stage Breast DCE-MRI using Generative Adversarial Models,03/09/2024,"Ruben D. Fonnegra, Maria Liliana HernÃ¡ndez, Juan C. Caicedo, Gloria M. DÃ­az","Contrast-enhancement pattern analysis is critical in breast magnetic
resonance imaging (MRI) to distinguish benign from probably malignant tumors.
However, contrast-enhanced image acquisitions are time-consuming and very
expensive. As an alternative to physical acquisition, this paper proposes a
comprehensive pipeline for the generation of accurate long-term (late)
contrast-enhanced breast MRI from the early counterpart. The proposed strategy
focuses on preserving the contrast agent pattern in the enhanced regions while
maintaining visual properties in the entire synthesized images. To that end, a
novel loss function that leverages the biological behavior of contrast agent
(CA) in tissue, given by the Time-Intensity (TI) enhancement curve, is proposed
to optimize a pixel-attention based generative model. In addition, unlike
traditional normalization and standardization methods, we developed a new
normalization strategy that maintains the contrast enhancement pattern across
the image sequences at multiple timestamps. This ensures the prevalence of the
CA pattern after image preprocessing, unlike conventional approaches.
Furthermore, in order to objectively evaluate the clinical quality of the
synthesized images, two metrics are also introduced to measure the differences
between the TI curves of enhanced regions of the acquired and synthesized
images. The experimental results showed that the proposed strategy generates
images that significantly outperform diagnostic quality in contrast-enhanced
regions while maintaining the spatial features of the entire image. This
results suggest a potential use of synthetic late enhanced images generated via
deep learning in clinical scenarios.",http://arxiv.org/pdf/2409.01596v1,,False
Learning out-of-time-ordered correlators with classical kernel methods,03/09/2024,"John Tanner, Jason Pye, Jingbo Wang","Out-of-Time Ordered Correlators (OTOCs) are widely used to investigate
information scrambling in quantum systems. However, directly computing OTOCs
with classical computers is often impractical. This is due to the need to
simulate the dynamics of quantum many-body systems, which entails
exponentially-scaling computational costs with system size. Similarly, exact
simulation of the dynamics with a quantum computer (QC) will generally require
a fault-tolerant QC, which is currently beyond technological capabilities.
Therefore, alternative approaches are needed for computing OTOCs and related
quantities. In this study, we explore four parameterised sets of Hamiltonians
describing quantum systems of interest in condensed matter physics. For each
set, we investigate whether classical kernel methods can accurately learn the
XZ-OTOC as well as a particular sum of OTOCs, as functions of the Hamiltonian
parameters. We frame the problem as a regression task, generating labelled data
via an efficient numerical algorithm that utilises matrix product operators to
simulate quantum many-body systems, with up to 40 qubits. Using this data, we
train a variety of standard kernel machines and observe that the best kernels
consistently achieve a high coefficient of determination ($R^2$) on the testing
sets, typically between 0.9 and 0.99, and almost always exceeding 0.8. This
demonstrates that classical kernels supplied with a moderate amount of training
data can be used to closely and efficiently approximate OTOCs and related
quantities for a diverse range of quantum many-body systems.",http://arxiv.org/pdf/2409.01592v1,,False
Effective Noise-aware Data Simulation for Domain-adaptive Speech Enhancement Leveraging Dynamic Stochastic Perturbation,03/09/2024,"Chien-Chun Wang, Li-Wei Chen, Hung-Shin Lee, Berlin Chen, Hsin-Min Wang","Cross-domain speech enhancement (SE) is often faced with severe challenges
due to the scarcity of noise and background information in an unseen target
domain, leading to a mismatch between training and test conditions. This study
puts forward a novel data simulation method to address this issue, leveraging
noise-extractive techniques and generative adversarial networks (GANs) with
only limited target noisy speech data. Notably, our method employs a noise
encoder to extract noise embeddings from target-domain data. These embeddings
aptly guide the generator to synthesize utterances acoustically fitted to the
target domain while authentically preserving the phonetic content of the input
clean speech. Furthermore, we introduce the notion of dynamic stochastic
perturbation, which can inject controlled perturbations into the noise
embeddings during inference, thereby enabling the model to generalize well to
unseen noise conditions. Experiments on the VoiceBank-DEMAND benchmark dataset
demonstrate that our domain-adaptive SE method outperforms an existing strong
baseline based on data simulation.",http://arxiv.org/pdf/2409.01545v1,,False
AMG: Avatar Motion Guided Video Generation,02/09/2024,"Zhangsihao Yang, Mengyi Shan, Mohammad Farazi, Wenhui Zhu, Yanxi Chen, Xuanzhao Dong, Yalin Wang","Human video generation task has gained significant attention with the
advancement of deep generative models. Generating realistic videos with human
movements is challenging in nature, due to the intricacies of human body
topology and sensitivity to visual artifacts. The extensively studied 2D media
generation methods take advantage of massive human media datasets, but struggle
with 3D-aware control; whereas 3D avatar-based approaches, while offering more
freedom in control, lack photorealism and cannot be harmonized seamlessly with
background scene. We propose AMG, a method that combines the 2D photorealism
and 3D controllability by conditioning video diffusion models on controlled
rendering of 3D avatars. We additionally introduce a novel data processing
pipeline that reconstructs and renders human avatar movements from dynamic
camera videos. AMG is the first method that enables multi-person diffusion
video generation with precise control over camera positions, human motions, and
background style. We also demonstrate through extensive evaluation that it
outperforms existing human video generation methods conditioned on pose
sequences or driving videos in terms of realism and adaptability.",http://arxiv.org/pdf/2409.01502v1,,False
A causal viewpoint on prediction model performance under changes in case-mix: discrimination and calibration respond differently for prognosis and diagnosis predictions,02/09/2024,Wouter A. C. van Amsterdam,"Prediction models inform important clinical decisions, aiding in diagnosis,
prognosis, and treatment planning. The predictive performance of these models
is typically assessed through discrimination and calibration. However, changes
in the distribution of the data impact model performance. In health-care, a
typical change is a shift in case-mix: for example, for cardiovascular risk
managment, a general practitioner sees a different mix of patients than a
specialist in a tertiary hospital.
  This work introduces a novel framework that differentiates the effects of
case-mix shifts on discrimination and calibration based on the causal direction
of the prediction task. When prediction is in the causal direction (often the
case for prognosis preditions), calibration remains stable under case-mix
shifts, while discrimination does not. Conversely, when predicting in the
anti-causal direction (often with diagnosis predictions), discrimination
remains stable, but calibration does not.
  A simulation study and empirical validation using cardiovascular disease
prediction models demonstrate the implications of this framework. This
framework provides critical insights for evaluating and deploying prediction
models across different clinical settings, emphasizing the importance of
understanding the causal structure of the prediction task.",http://arxiv.org/pdf/2409.01444v1,,False
Self-Directed Learning of Convex Labelings on Graphs,02/09/2024,"Georgy Sokolov, Maximilian Thiessen, Margarita Akhmejanova, Fabio Vitale, Francesco Orabona","We study the problem of learning the clusters of a given graph in the
self-directed learning setup. This learning setting is a variant of online
learning, where rather than an adversary determining the sequence in which
nodes are presented, the learner autonomously and adaptively selects them.
While self-directed learning of Euclidean halfspaces, linear functions, and
general abstract multi-class hypothesis classes was recently considered, no
results previously existed specifically for self-directed node classification
on graphs. In this paper, we address this problem developing efficient
algorithms for it. More specifically, we focus on the case of (geodesically)
convex clusters, i.e., for every two nodes sharing the same label, all nodes on
every shortest path between them also share the same label. In particular, we
devise a polynomial-time algorithm that makes only $3(h(G)+1)^4 \ln n$ mistakes
on graphs with two convex clusters, where $n$ is the total number of nodes and
$h(G)$ is the Hadwiger number, i.e., the size of the largest clique minor of
the graph $G$. We also show that our algorithm is robust to the case that
clusters are slightly non-convex, still achieving a mistake bound logarithmic
in $n$. Finally, for the more standard case of homophilic clusters, where
strongly connected nodes tend to belong the same class, we devise a simple and
efficient algorithm.",http://arxiv.org/pdf/2409.01428v1,,False
Enhancing Sample Efficiency and Exploration in Reinforcement Learning through the Integration of Diffusion Models and Proximal Policy Optimization,02/09/2024,"Gao Tianci, Dmitriev D. Dmitry, Konstantin A. Neusypin, Yang Bo, Rao Shengren","Recent advancements in reinforcement learning (RL) have been fueled by
large-scale data and deep neural networks, particularly for high-dimensional
and complex tasks. Online RL methods like Proximal Policy Optimization (PPO)
are effective in dynamic scenarios but require substantial real-time data,
posing challenges in resource-constrained or slow simulation environments.
Offline RL addresses this by pre-learning policies from large datasets, though
its success depends on the quality and diversity of the data. This work
proposes a framework that enhances PPO algorithms by incorporating a diffusion
model to generate high-quality virtual trajectories for offline datasets. This
approach improves exploration and sample efficiency, leading to significant
gains in cumulative rewards, convergence speed, and strategy stability in
complex tasks. Our contributions are threefold: we explore the potential of
diffusion models in RL, particularly for offline datasets, extend the
application of online RL to offline environments, and experimentally validate
the performance improvements of PPO with diffusion models. These findings
provide new insights and methods for applying RL to high-dimensional, complex
tasks. Finally, we open-source our code at https://github.com/TianciGao/DiffPPO",http://arxiv.org/pdf/2409.01427v1,,False
Imitating Language via Scalable Inverse Reinforcement Learning,02/09/2024,"Markus Wulfmeier, Michael Bloesch, Nino Vieillard, Arun Ahuja, Jorg Bornschein, Sandy Huang, Artem Sokolov, Matt Barnes, Guillaume Desjardins, Alex Bewley, Sarah Maria Elisabeth Bechtle, Jost Tobias Springenberg, Nikola Momchev, Olivier Bachem, Matthieu Geist, Martin Riedmiller","The majority of language model training builds on imitation learning. It
covers pretraining, supervised fine-tuning, and affects the starting conditions
for reinforcement learning from human feedback (RLHF). The simplicity and
scalability of maximum likelihood estimation (MLE) for next token prediction
led to its role as predominant paradigm. However, the broader field of
imitation learning can more effectively utilize the sequential structure
underlying autoregressive generation. We focus on investigating the inverse
reinforcement learning (IRL) perspective to imitation, extracting rewards and
directly optimizing sequences instead of individual token likelihoods and
evaluate its benefits for fine-tuning large language models. We provide a new
angle, reformulating inverse soft-Q-learning as a temporal difference
regularized extension of MLE. This creates a principled connection between MLE
and IRL and allows trading off added complexity with increased performance and
diversity of generations in the supervised fine-tuning (SFT) setting. We find
clear advantages for IRL-based imitation, in particular for retaining diversity
while maximizing task performance, rendering IRL a strong alternative on fixed
SFT datasets even without online data generation. Our analysis of IRL-extracted
reward functions further indicates benefits for more robust reward functions
via tighter integration of supervised and preference-based LLM post-training.",http://arxiv.org/pdf/2409.01369v1,,False
Correlating Time Series with Interpretable Convolutional Kernels,02/09/2024,"Xinyu Chen, HanQin Cai, Fuqiang Liu, Jinhua Zhao","This study addresses the problem of convolutional kernel learning in
univariate, multivariate, and multidimensional time series data, which is
crucial for interpreting temporal patterns in time series and supporting
downstream machine learning tasks. First, we propose formulating convolutional
kernel learning for univariate time series as a sparse regression problem with
a non-negative constraint, leveraging the properties of circular convolution
and circulant matrices. Second, to generalize this approach to multivariate and
multidimensional time series data, we use tensor computations, reformulating
the convolutional kernel learning problem in the form of tensors. This is
further converted into a standard sparse regression problem through
vectorization and tensor unfolding operations. In the proposed methodology, the
optimization problem is addressed using the existing non-negative subspace
pursuit method, enabling the convolutional kernel to capture temporal
correlations and patterns. To evaluate the proposed model, we apply it to
several real-world time series datasets. On the multidimensional rideshare and
taxi trip data from New York City and Chicago, the convolutional kernels reveal
interpretable local correlations and cyclical patterns, such as weekly
seasonality. In the context of multidimensional fluid flow data, both local and
nonlocal correlations captured by the convolutional kernels can reinforce
tensor factorization, leading to performance improvements in fluid flow
reconstruction tasks. Thus, this study lays an insightful foundation for
automatically learning convolutional kernels from time series data, with an
emphasis on interpretability through sparsity and non-negativity constraints.",http://arxiv.org/pdf/2409.01362v1,,False
A Financial Time Series Denoiser Based on Diffusion Model,02/09/2024,"Zhuohan Wang, Carmine Ventre","Financial time series often exhibit low signal-to-noise ratio, posing
significant challenges for accurate data interpretation and prediction and
ultimately decision making. Generative models have gained attention as powerful
tools for simulating and predicting intricate data patterns, with the diffusion
model emerging as a particularly effective method. This paper introduces a
novel approach utilizing the diffusion model as a denoiser for financial time
series in order to improve data predictability and trading performance. By
leveraging the forward and reverse processes of the conditional diffusion model
to add and remove noise progressively, we reconstruct original data from noisy
inputs. Our extensive experiments demonstrate that diffusion model-based
denoised time series significantly enhance the performance on downstream future
return classification tasks. Moreover, trading signals derived from the
denoised data yield more profitable trades with fewer transactions, thereby
minimizing transaction costs and increasing overall trading efficiency.
Finally, we show that by using classifiers trained on denoised time series, we
can recognize the noising state of the market and obtain excess return.",http://arxiv.org/pdf/2409.02138v1,,False
Grounding Language Models in Autonomous Loco-manipulation Tasks,02/09/2024,"Jin Wang, Nikos Tsagarakis","Humanoid robots with behavioral autonomy have consistently been regarded as
ideal collaborators in our daily lives and promising representations of
embodied intelligence. Compared to fixed-based robotic arms, humanoid robots
offer a larger operational space while significantly increasing the difficulty
of control and planning. Despite the rapid progress towards general-purpose
humanoid robots, most studies remain focused on locomotion ability with few
investigations into whole-body coordination and tasks planning, thus limiting
the potential to demonstrate long-horizon tasks involving both mobility and
manipulation under open-ended verbal instructions. In this work, we propose a
novel framework that learns, selects, and plans behaviors based on tasks in
different scenarios. We combine reinforcement learning (RL) with whole-body
optimization to generate robot motions and store them into a motion library. We
further leverage the planning and reasoning features of the large language
model (LLM), constructing a hierarchical task graph that comprises a series of
motion primitives to bridge lower-level execution with higher-level planning.
Experiments in simulation and real-world using the CENTAURO robot show that the
language model based planner can efficiently adapt to new loco-manipulation
tasks, demonstrating high autonomy from free-text commands in unstructured
scenes.",http://arxiv.org/pdf/2409.01326v1,,False
One-Index Vector Quantization Based Adversarial Attack on Image Classification,02/09/2024,"Haiju Fan, Xiaona Qin, Shuang Chen, Hubert P. H. Shum, Ming Li","To improve storage and transmission, images are generally compressed. Vector
quantization (VQ) is a popular compression method as it has a high compression
ratio that suppresses other compression techniques. Despite this, existing
adversarial attack methods on image classification are mostly performed in the
pixel domain with few exceptions in the compressed domain, making them less
applicable in real-world scenarios. In this paper, we propose a novel one-index
attack method in the VQ domain to generate adversarial images by a differential
evolution algorithm, successfully resulting in image misclassification in
victim models. The one-index attack method modifies a single index in the
compressed data stream so that the decompressed image is misclassified. It only
needs to modify a single VQ index to realize an attack, which limits the number
of perturbed indexes. The proposed method belongs to a semi-black-box attack,
which is more in line with the actual attack scenario. We apply our method to
attack three popular image classification models, i.e., Resnet, NIN, and VGG16.
On average, 55.9% and 77.4% of the images in CIFAR-10 and Fashion MNIST,
respectively, are successfully attacked, with a high level of misclassification
confidence and a low level of image perturbation.",http://arxiv.org/pdf/2409.01282v1,,False
Prompt Compression with Context-Aware Sentence Encoding for Fast and Improved LLM Inference,02/09/2024,"Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, Shane Luke","Large language models (LLMs) have triggered a new stream of research focusing
on compressing the context length to reduce the computational cost while
ensuring the retention of helpful information for LLMs to answer the given
question. Token-based removal methods are one of the most prominent approaches
in this direction, but risk losing the semantics of the context caused by
intermediate token removal, especially under high compression ratios, while
also facing challenges in computational efficiency. In this work, we propose
context-aware prompt compression (CPC), a sentence-level prompt compression
technique where its key innovation is a novel context-aware sentence encoder
that provides a relevance score for each sentence for a given question. To
train this encoder, we generate a new dataset consisting of questions,
positives, and negative pairs where positives are sentences relevant to the
question, while negatives are irrelevant context sentences. We train the
encoder in a contrastive setup to learn context-aware sentence representations.
Our method considerably outperforms prior works on prompt compression on
benchmark datasets and is up to 10.93x faster at inference compared to the best
token-level compression method. We also find better improvement for shorter
length constraints in most benchmarks, showing the effectiveness of our
proposed solution in the compression of relevant information in a shorter
context. Finally, we release the code and the dataset for quick reproducibility
and further development: https://github.com/Workday/cpc.",http://arxiv.org/pdf/2409.01227v2,,False
A Perspective on Literary Metaphor in the Context of Generative AI,02/09/2024,"Imke van Heerden, Anil Bas","At the intersection of creative text generation and literary theory, this
study explores the role of literary metaphor and its capacity to generate a
range of meanings. In this regard, literary metaphor is vital to the
development of any particular language. To investigate whether the inclusion of
original figurative language improves textual quality, we trained an LSTM-based
language model in Afrikaans. The network produces phrases containing
compellingly novel figures of speech. Specifically, the emphasis falls on how
AI might be utilised as a defamiliarisation technique, which disrupts expected
uses of language to augment poetic expression. Providing a literary perspective
on text generation, the paper raises thought-provoking questions on aesthetic
value, interpretation and evaluation.",http://arxiv.org/pdf/2409.01053v1,,False
Semantically Controllable Augmentations for Generalizable Robot Learning,02/09/2024,"Zoey Chen, Zhao Mandi, Homanga Bharadhwaj, Mohit Sharma, Shuran Song, Abhishek Gupta, Vikash Kumar","Generalization to unseen real-world scenarios for robot manipulation requires
exposure to diverse datasets during training. However, collecting large
real-world datasets is intractable due to high operational costs. For robot
learning to generalize despite these challenges, it is essential to leverage
sources of data or priors beyond the robot's direct experience. In this work,
we posit that image-text generative models, which are pre-trained on large
corpora of web-scraped data, can serve as such a data source. These generative
models encompass a broad range of real-world scenarios beyond a robot's direct
experience and can synthesize novel synthetic experiences that expose robotic
agents to additional world priors aiding real-world generalization at no extra
cost.
  In particular, our approach leverages pre-trained generative models as an
effective tool for data augmentation. We propose a generative augmentation
framework for semantically controllable augmentations and rapidly multiplying
robot datasets while inducing rich variations that enable real-world
generalization. Based on diverse augmentations of robot data, we show how
scalable robot manipulation policies can be trained and deployed both in
simulation and in unseen real-world environments such as kitchens and
table-tops. By demonstrating the effectiveness of image-text generative models
in diverse real-world robotic applications, our generative augmentation
framework provides a scalable and efficient path for boosting generalization in
robot learning at no extra human cost.",http://arxiv.org/pdf/2409.00951v1,,False
Improving Adaptivity via Over-Parameterization in Sequence Models,02/09/2024,"Yicheng Li, Qian Lin","It is well known that eigenfunctions of a kernel play a crucial role in
kernel regression. Through several examples, we demonstrate that even with the
same set of eigenfunctions, the order of these functions significantly impacts
regression outcomes. Simplifying the model by diagonalizing the kernel, we
introduce an over-parameterized gradient descent in the realm of sequence model
to capture the effects of various orders of a fixed set of eigen-functions.
This method is designed to explore the impact of varying eigenfunction orders.
Our theoretical results show that the over-parameterization gradient flow can
adapt to the underlying structure of the signal and significantly outperform
the vanilla gradient flow method. Moreover, we also demonstrate that deeper
over-parameterization can further enhance the generalization capability of the
model. These results not only provide a new perspective on the benefits of
over-parameterization and but also offer insights into the adaptivity and
generalization potential of neural networks beyond the kernel regime.",http://arxiv.org/pdf/2409.00894v1,,False
