Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
A Practical Memory Injection Attack against LLM Agents,05/03/2025,"Shen Dong, Shaocheng Xu, Pengfei He, Yige Li, Jiliang Tang, Tianming Liu, Hui Liu, Zhen Xiang","Agents based on large language models (LLMs) have demonstrated strong
capabilities in a wide range of complex, real-world applications. However, LLM
agents with a compromised memory bank may easily produce harmful outputs when
the past records retrieved for demonstration are malicious. In this paper, we
propose a novel Memory INJection Attack, MINJA, that enables the injection of
malicious records into the memory bank by only interacting with the agent via
queries and output observations. These malicious records are designed to elicit
a sequence of malicious reasoning steps leading to undesirable agent actions
when executing the victim user's query. Specifically, we introduce a sequence
of bridging steps to link the victim query to the malicious reasoning steps.
During the injection of the malicious record, we propose an indication prompt
to guide the agent to autonomously generate our designed bridging steps. We
also propose a progressive shortening strategy that gradually removes the
indication prompt, such that the malicious record will be easily retrieved when
processing the victim query comes after. Our extensive experiments across
diverse agents demonstrate the effectiveness of MINJA in compromising agent
memory. With minimal requirements for execution, MINJA enables any user to
influence agent memory, highlighting practical risks of LLM agents.",http://arxiv.org/pdf/2503.03704v1,,False
Limits of nonlinear and dispersive fiber propagation for photonic extreme learning,05/03/2025,"Andrei V. Ermolaev, Mathilde Hary, Lev Leybov, Piotr Ryczkowski, Anas Skalli, Daniel Brunner, GoÃ«ry Genty, John M. Dudley","We report a generalized nonlinear Schr\""odinger equation simulation model of
an extreme learning machine based on optical fiber propagation. Using
handwritten digit classification as a benchmark, we study how accuracy depends
on propagation dynamics, as well as parameters governing spectral encoding,
readout, and noise. Test accuracies of over 91% and 93% are found for
propagation in the anomalous and normal dispersion regimes respectively. Our
simulation results also suggest that quantum noise on the input pulses
introduces an intrinsic penalty to ELM performance.",http://arxiv.org/pdf/2503.03649v1,,False
Simulation-Based Performance Evaluation of 3D Object Detection Methods with Deep Learning for a LiDAR Point Cloud Dataset in a SOTIF-related Use Case,05/03/2025,"Milin Patel, Rolf Jung","Safety of the Intended Functionality (SOTIF) addresses sensor performance
limitations and deep learning-based object detection insufficiencies to ensure
the intended functionality of Automated Driving Systems (ADS). This paper
presents a methodology examining the adaptability and performance evaluation of
the 3D object detection methods on a LiDAR point cloud dataset generated by
simulating a SOTIF-related Use Case. The major contributions of this paper
include defining and modelling a SOTIF-related Use Case with 21 diverse weather
conditions and generating a LiDAR point cloud dataset suitable for application
of 3D object detection methods. The dataset consists of 547 frames,
encompassing clear, cloudy, rainy weather conditions, corresponding to
different times of the day, including noon, sunset, and night. Employing
MMDetection3D and OpenPCDET toolkits, the performance of State-of-the-Art
(SOTA) 3D object detection methods is evaluated and compared by testing the
pre-trained Deep Learning (DL) models on the generated dataset using Average
Precision (AP) and Recall metrics.",http://arxiv.org/pdf/2503.03548v1,10.5220/0012707300003702,False
AI-Enabled Conversational Journaling for Advancing Parkinson's Disease Symptom Tracking,05/03/2025,"Mashrur Rashik, Shilpa Sweth, Nishtha Agrawal, Saiyyam Kochar, Kara M Smith, Fateme Rajabiyazdi, Vidya Setlur, Narges Mahyar, Ali Sarvghad","Journaling plays a crucial role in managing chronic conditions by allowing
patients to document symptoms and medication intake, providing essential data
for long-term care. While valuable, traditional journaling methods often rely
on static, self-directed entries, lacking interactive feedback and real-time
guidance. This gap can result in incomplete or imprecise information, limiting
its usefulness for effective treatment. To address this gap, we introduce
PATRIKA, an AI-enabled prototype designed specifically for people with
Parkinson's disease (PwPD). The system incorporates cooperative conversation
principles, clinical interview simulations, and personalization to create a
more effective and user-friendly journaling experience. Through two user
studies with PwPD and iterative refinement of PATRIKA, we demonstrate
conversational journaling's significant potential in patient engagement and
collecting clinically valuable information. Our results showed that generating
probing questions PATRIKA turned journaling into a bi-directional interaction.
Additionally, we offer insights for designing journaling systems for healthcare
and future directions for promoting sustained journaling.",http://arxiv.org/pdf/2503.03532v1,10.1145/3706598.3714280,False
SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning,05/03/2025,"Borong Zhang, Yuhao Zhang, Jiaming Ji, Yingshan Lei, Josef Dai, Yuanpei Chen, Yaodong Yang","Vision-language-action models (VLAs) have shown great potential as generalist
robot policies. However, these models pose urgent safety challenges during
deployment, including the risk of physical harm to the environment, the robot
itself, and humans. How can safety be explicitly incorporated into VLAs? In
this work, we propose SafeVLA, a novel algorithm designed to integrate safety
into VLAs, ensuring the protection of the environment, robot hardware and
humans in real-world settings. SafeVLA effectively balances safety and task
performance by employing large-scale constrained learning within simulated
environments. We demonstrate that SafeVLA outperforms the current
state-of-the-art method in both safety and task performance, achieving average
improvements of 83.58% and 3.85%, respectively, in simulation. By prioritizing
safety, our approach eliminates high-risk behaviors and reduces the upper bound
of unsafe behaviors to 1/35 of that in the current state-of-the-art, thereby
significantly mitigating long-tail risks. Furthermore, the learned safety
constraints generalize to diverse, unseen scenarios, including multiple
out-of-distribution perturbations and tasks. Our data, models and newly
proposed benchmark environment are available at
https://sites.google.com/view/pku-safevla.",http://arxiv.org/pdf/2503.03480v1,,False
Predicting Practically? Domain Generalization for Predictive Analytics in Real-world Environments,05/03/2025,"Hanyu Duan, Yi Yang, Ahmed Abbasi, Kar Yan Tam","Predictive machine learning models are widely used in customer relationship
management (CRM) to forecast customer behaviors and support decision-making.
However, the dynamic nature of customer behaviors often results in significant
distribution shifts between training data and serving data, leading to
performance degradation in predictive models. Domain generalization, which aims
to train models that can generalize to unseen environments without prior
knowledge of their distributions, has become a critical area of research. In
this work, we propose a novel domain generalization method tailored to handle
complex distribution shifts, encompassing both covariate and concept shifts.
Our method builds upon the Distributionally Robust Optimization framework,
optimizing model performance over a set of hypothetical worst-case
distributions rather than relying solely on the training data. Through
simulation experiments, we demonstrate the working mechanism of the proposed
method. We also conduct experiments on a real-world customer churn dataset, and
validate its effectiveness in both temporal and spatial generalization
settings. Finally, we discuss the broader implications of our method for
advancing Information Systems (IS) design research, particularly in building
robust predictive models for dynamic managerial environments.",http://arxiv.org/pdf/2503.03399v1,,False
"Towards Understanding Multi-Round Large Language Model Reasoning: Approximability, Learnability and Generalizability",05/03/2025,"Chenhui Xu, Dancheng Liu, Jiajie Li, Amir Nassereldine, Zhaohui Li, Jinjun Xiong","Recent advancements in cognitive science and multi-round reasoning techniques
for Large Language Models (LLMs) suggest that iterative thinking processes
improve problem-solving performance in complex tasks. Inspired by this,
approaches like Chain-of-Thought, debating, and self-refinement have been
applied to auto-regressive LLMs, achieving significant successes in tasks such
as mathematical reasoning, commonsense reasoning, and multi-hop question
answering. Despite these successes, the theoretical basis for how multi-round
reasoning enhances problem-solving abilities remains underexplored. In this
work, we investigate the approximation, learnability, and generalization
properties of multi-round auto-regressive models. We show that Transformers
with finite context windows are universal approximators for steps of
Turing-computable functions and can approximate any Turing-computable
sequence-to-sequence function through multi-round reasoning. We extend PAC
learning to sequence generation and demonstrate that multi-round generation is
learnable even when the sequence length exceeds the model's context window.
Finally, we examine how generalization error propagates across rounds, and show
how the aforementioned approaches can help constrain this error, ensuring
outputs stay within an expectation boundary. This work sheds light on the
systemic theoretical foundations of multi-round sequence learning and
reasoning, emphasizing its role in inference complexity.",http://arxiv.org/pdf/2503.03128v1,,False
ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation,04/03/2025,"Yufei Wang, Ziyu Wang, Mino Nakura, Pratik Bhowal, Chia-Liang Kuo, Yi-Ting Chen, Zackory Erickson, David Held","This paper presents ArticuBot, in which a single learned policy enables a
robotics system to open diverse categories of unseen articulated objects in the
real world. This task has long been challenging for robotics due to the large
variations in the geometry, size, and articulation types of such objects. Our
system, Articubot, consists of three parts: generating a large number of
demonstrations in physics-based simulation, distilling all generated
demonstrations into a point cloud-based neural policy via imitation learning,
and performing zero-shot sim2real transfer to real robotics systems. Utilizing
sampling-based grasping and motion planning, our demonstration generalization
pipeline is fast and effective, generating a total of 42.3k demonstrations over
322 training articulated objects. For policy learning, we propose a novel
hierarchical policy representation, in which the high-level policy learns the
sub-goal for the end-effector, and the low-level policy learns how to move the
end-effector conditioned on the predicted goal. We demonstrate that this
hierarchical approach achieves much better object-level generalization compared
to the non-hierarchical version. We further propose a novel weighted
displacement model for the high-level policy that grounds the prediction into
the existing 3D structure of the scene, outperforming alternative policy
representations. We show that our learned policy can zero-shot transfer to
three different real robot settings: a fixed table-top Franka arm across two
different labs, and an X-Arm on a mobile base, opening multiple unseen
articulated objects across two labs, real lounges, and kitchens. Videos and
code can be found on our project website: https://articubot.github.io/.",http://arxiv.org/pdf/2503.03045v1,,False
Generative assimilation and prediction for weather and climate,04/03/2025,"Shangshang Yang, Congyi Nai, Xinyan Liu, Weidong Li, Jie Chao, Jingnan Wang, Leyi Wang, Xichen Li, Xi Chen, Bo Lu, Ziniu Xiao, Niklas Boers, Huiling Yuan, Baoxiang Pan","Machine learning models have shown great success in predicting weather up to
two weeks ahead, outperforming process-based benchmarks. However, existing
approaches mostly focus on the prediction task, and do not incorporate the
necessary data assimilation. Moreover, these models suffer from error
accumulation in long roll-outs, limiting their applicability to seasonal
predictions or climate projections. Here, we introduce Generative Assimilation
and Prediction (GAP), a unified deep generative framework for assimilation and
prediction of both weather and climate. By learning to quantify the
probabilistic distribution of atmospheric states under observational,
predictive, and external forcing constraints, GAP excels in a broad range of
weather-climate related tasks, including data assimilation, seamless
prediction, and climate simulation. In particular, GAP is competitive with
state-of-the-art ensemble assimilation, probabilistic weather forecast and
seasonal prediction, yields stable millennial simulations, and reproduces
climate variability from daily to decadal time scales.",http://arxiv.org/pdf/2503.03038v1,,False
Generative Active Adaptation for Drifting and Imbalanced Network Intrusion Detection,04/03/2025,"Ragini Gupta, Shinan Liu, Ruixiao Zhang, Xinyue Hu, Pranav Kommaraju, Xiaoyang Wang, Hadjer Benkraouda, Nick Feamster, Klara Nahrstedt","Machine learning has shown promise in network intrusion detection systems,
yet its performance often degrades due to concept drift and imbalanced data.
These challenges are compounded by the labor-intensive process of labeling
network traffic, especially when dealing with evolving and rare attack types,
which makes selecting the right data for adaptation difficult. To address these
issues, we propose a generative active adaptation framework that minimizes
labeling effort while enhancing model robustness. Our approach employs
density-aware active sampling to identify the most informative samples for
annotation and leverages deep generative models to synthesize diverse samples,
thereby augmenting the training set and mitigating the effects of concept
drift. We evaluate our end-to-end framework on both simulated IDS data and a
real-world ISP dataset, demonstrating significant improvements in intrusion
detection performance. Our method boosts the overall F1-score from 0.60
(without adaptation) to 0.86. Rare attacks such as Infiltration, Web Attack,
and FTP-BruteForce, which originally achieve F1 scores of 0.001, 0.04, and
0.00, improve to 0.30, 0.50, and 0.71, respectively, with generative active
adaptation in the CIC-IDS 2018 dataset. Our framework effectively enhances rare
attack detection while reducing labeling costs, making it a scalable and
adaptive solution for real-world intrusion detection.",http://arxiv.org/pdf/2503.03022v1,,False
Robust time series generation via SchrÃ¶dinger Bridge: a comprehensive evaluation,04/03/2025,"Alexandre Alouadi, Baptiste Barreau, Laurent Carlier, HuyÃªn Pham","We investigate the generative capabilities of the Schr\""odinger Bridge (SB)
approach for time series. The SB framework formulates time series synthesis as
an entropic optimal interpolation transport problem between a reference
probability measure on path space and a target joint distribution. This results
in a stochastic differential equation over a finite horizon that accurately
captures the temporal dynamics of the target time series. While the SB approach
has been largely explored in fields like image generation, there is a scarcity
of studies for its application to time series. In this work, we bridge this gap
by conducting a comprehensive evaluation of the SB method's robustness and
generative performance. We benchmark it against state-of-the-art (SOTA) time
series generation methods across diverse datasets, assessing its strengths,
limitations, and capacity to model complex temporal dependencies. Our results
offer valuable insights into the SB framework's potential as a versatile and
robust tool for time series generation.",http://arxiv.org/pdf/2503.02943v1,,False
Diverse Controllable Diffusion Policy with Signal Temporal Logic,04/03/2025,"Yue Meng, Chuchu fan","Generating realistic simulations is critical for autonomous system
applications such as self-driving and human-robot interactions. However,
driving simulators nowadays still have difficulty in generating controllable,
diverse, and rule-compliant behaviors for road participants: Rule-based models
cannot produce diverse behaviors and require careful tuning, whereas
learning-based methods imitate the policy from data but are not designed to
follow the rules explicitly. Besides, the real-world datasets are by nature
""single-outcome"", making the learning method hard to generate diverse
behaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion
Models to learn controllable, diverse, and rule-aware policy. We first
calibrate the STL on the real-world data, then generate diverse synthetic data
using trajectory optimization, and finally learn the rectified diffusion policy
on the augmented dataset. We test on the NuScenes dataset and our approach can
achieve the most diverse rule-compliant trajectories compared to other
baselines, with a runtime 1/17X to the second-best approach. In the closed-loop
testing, our approach reaches the highest diversity, rule satisfaction rate,
and the least collision rate. Our method can generate varied characteristics
conditional on different STL parameters in testing. A case study on human-robot
encounter scenarios shows our approach can generate diverse and
closed-to-oracle trajectories. The annotation tool, augmented dataset, and code
are available at https://github.com/mengyuest/pSTL-diffusion-policy.",http://arxiv.org/pdf/2503.02924v1,10.1109/LRA.2024.3444668,False
Wikipedia in the Era of LLMs: Evolution and Risks,04/03/2025,"Siming Huang, Yuliang Xu, Mingmeng Geng, Yao Wan, Dongping Chen","In this paper, we present a thorough analysis of the impact of Large Language
Models (LLMs) on Wikipedia, examining the evolution of Wikipedia through
existing data and using simulations to explore potential risks. We begin by
analyzing page views and article content to study Wikipedia's recent changes
and assess the impact of LLMs. Subsequently, we evaluate how LLMs affect
various Natural Language Processing (NLP) tasks related to Wikipedia, including
machine translation and retrieval-augmented generation (RAG). Our findings and
simulation results reveal that Wikipedia articles have been influenced by LLMs,
with an impact of approximately 1%-2% in certain categories. If the machine
translation benchmark based on Wikipedia is influenced by LLMs, the scores of
the models may become inflated, and the comparative results among models might
shift as well. Moreover, the effectiveness of RAG might decrease if the
knowledge base becomes polluted by LLM-generated content. While LLMs have not
yet fully changed Wikipedia's language and knowledge structures, we believe
that our empirical findings signal the need for careful consideration of
potential future risks.",http://arxiv.org/pdf/2503.02879v1,,False
(How) Do Language Models Track State?,04/03/2025,"Belinda Z. Li, Zifan Carl Guo, Jacob Andreas","Transformer language models (LMs) exhibit behaviors -- from storytelling to
code generation -- that appear to require tracking the unobserved state of an
evolving world. How do they do so? We study state tracking in LMs trained or
fine-tuned to compose permutations (i.e., to compute the order of a set of
objects after a sequence of swaps). Despite the simple algebraic structure of
this problem, many other tasks (e.g., simulation of finite automata and
evaluation of boolean expressions) can be reduced to permutation composition,
making it a natural model for state tracking in general. We show that LMs
consistently learn one of two state tracking mechanisms for this task. The
first closely resembles the ""associative scan"" construction used in recent
theoretical work by Liu et al. (2023) and Merrill et al. (2024). The second
uses an easy-to-compute feature (permutation parity) to partially prune the
space of outputs, then refines this with an associative scan. The two
mechanisms exhibit markedly different robustness properties, and we show how to
steer LMs toward one or the other with intermediate training tasks that
encourage or suppress the heuristics. Our results demonstrate that transformer
LMs, whether pretrained or fine-tuned, can learn to implement efficient and
interpretable state tracking mechanisms, and the emergence of these mechanisms
can be predicted and controlled.",http://arxiv.org/pdf/2503.02854v1,,False
SeqFusion: Sequential Fusion of Pre-Trained Models for Zero-Shot Time-Series Forecasting,04/03/2025,"Ting-Ji Huang, Xu-Yang Chen, Han-Jia Ye","Unlike traditional time-series forecasting methods that require extensive
in-task data for training, zero-shot forecasting can directly predict future
values given a target time series without additional training data. Current
zero-shot approaches primarily rely on pre-trained generalized models, with
their performance often depending on the variety and relevance of the
pre-training data, which can raise privacy concerns. Instead of collecting
diverse pre-training data, we introduce SeqFusion in this work, a novel
framework that collects and fuses diverse pre-trained models (PTMs)
sequentially for zero-shot forecasting. Based on the specific temporal
characteristics of the target time series, SeqFusion selects the most suitable
PTMs from a batch of pre-collected PTMs, performs sequential predictions, and
fuses all the predictions while using minimal data to protect privacy. Each of
these PTMs specializes in different temporal patterns and forecasting tasks,
allowing SeqFusion to select by measuring distances in a shared representation
space of the target time series with each PTM. Experiments demonstrate that
SeqFusion achieves competitive accuracy in zero-shot forecasting compared to
state-of-the-art methods.",http://arxiv.org/pdf/2503.02836v1,,False
"Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts",04/03/2025,"Marta Skreta, Tara Akhound-Sadegh, Viktor Ohanesian, Roberto Bondesan, AlÃ¡n Aspuru-Guzik, Arnaud Doucet, Rob Brekelmans, Alexander Tong, Kirill Neklyudov","While score-based generative models are the model of choice across diverse
domains, there are limited tools available for controlling inference-time
behavior in a principled manner, e.g. for composing multiple pretrained models.
Existing classifier-free guidance methods use a simple heuristic to mix
conditional and unconditional scores to approximately sample from conditional
distributions. However, such methods do not approximate the intermediate
distributions, necessitating additional 'corrector' steps. In this work, we
provide an efficient and principled method for sampling from a sequence of
annealed, geometric-averaged, or product distributions derived from pretrained
score-based models. We derive a weighted simulation scheme which we call
Feynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by
carefully accounting for terms in the appropriate partial differential
equations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo
(SMC) resampling algorithms that leverage inference-time scaling to improve
sampling quality. We empirically demonstrate the utility of our methods by
proposing amortized sampling via inference-time temperature annealing,
improving multi-objective molecule generation using pretrained models, and
improving classifier-free guidance for text-to-image generation. Our code is
available at https://github.com/martaskrt/fkc-diffusion.",http://arxiv.org/pdf/2503.02819v1,,False
Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression,04/03/2025,"Nathan Godey, Alessio Devoto, Yu Zhao, Simone Scardapane, Pasquale Minervini, Ãric de la Clergerie, BenoÃ®t Sagot","Autoregressive language models rely on a Key-Value (KV) Cache, which avoids
re-computing past hidden states during generation, making it faster. As model
sizes and context lengths grow, the KV Cache becomes a significant memory
bottleneck, which calls for compression methods that limit its size during
generation. In this paper, we discover surprising properties of Query (Q) and
Key (K) vectors that allow us to efficiently approximate attention scores
without computing the attention maps. We propose Q-Filters, a training-free KV
Cache compression method that filters out less crucial Key-Value pairs based on
a single context-agnostic projection. Contrarily to many alternatives,
Q-Filters is compatible with FlashAttention, as it does not require direct
access to attention weights. Experimental results in long-context settings
demonstrate that Q-Filters is competitive with attention-based compression
methods such as SnapKV in retrieval tasks while consistently outperforming
efficient compression schemes such as Streaming-LLM in generation setups.
Notably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task
with a x32 compression level while reducing the generation perplexity drop by
up to 65% in text generation compared to Streaming-LLM.",http://arxiv.org/pdf/2503.02812v1,,False
RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration,04/03/2025,"Alicia Russell-Gilbert, Sudip Mittal, Shahram Rahimi, Maria Seale, Joseph Jabour, Thomas Arnold, Joshua Church","Anomaly detection in complex industrial environments poses unique challenges,
particularly in contexts characterized by data sparsity and evolving
operational conditions. Predictive maintenance (PdM) in such settings demands
methodologies that are adaptive, transferable, and capable of integrating
domain-specific knowledge. In this paper, we present RAAD-LLM, a novel
framework for adaptive anomaly detection, leveraging large language models
(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach
addresses the aforementioned PdM challenges. By effectively utilizing
domain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time
series data without requiring fine-tuning on specific datasets. The framework's
adaptability mechanism enables it to adjust its understanding of normal
operating conditions dynamically, thus increasing detection accuracy. We
validate this methodology through a real-world application for a plastics
manufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show
significant improvements over our previous model with an accuracy increase from
70.7 to 89.1 on the real-world dataset. By allowing for the enriching of input
series data with semantics, RAAD-LLM incorporates multimodal capabilities that
facilitate more collaborative decision-making between the model and plant
operators. Overall, our findings support RAAD-LLM's ability to revolutionize
anomaly detection methodologies in PdM, potentially leading to a paradigm shift
in how anomaly detection is implemented across various industries.",http://arxiv.org/pdf/2503.02800v1,,False
Generative Modeling of Microweather Wind Velocities for Urban Air Mobility,04/03/2025,"Tristan A. Shah, Michael C. Stanley, James E. Warner","Motivated by the pursuit of safe, reliable, and weather-tolerant urban air
mobility (UAM) solutions, this work proposes a generative modeling approach for
characterizing microweather wind velocities. Microweather, or the weather
conditions in highly localized areas, is particularly complex in urban
environments owing to the chaotic and turbulent nature of wind flows.
Furthermore, traditional means of assessing local wind fields are not generally
viable solutions for UAM applications: 1) field measurements that would rely on
permanent wind profiling systems in operational air space are not practical, 2)
physics-based models that simulate fluid dynamics at a sufficiently high
resolution are not computationally tractable, and 3) data-driven modeling
approaches that are largely deterministic ignore the inherent variability in
turbulent flows that dictates UAM reliability. Thus, advancements in predictive
capabilities are needed to help mitigate the unique operational safety risks
that microweather winds pose for smaller, lighter weight UAM aircraft.
  This work aims to model microweather wind velocities in a manner that is
computationally-efficient, captures random variability, and would only require
a temporary, rather than permanent, field measurement campaign. Inspired by
recent breakthroughs in conditional generative AI such as text-to-image
generation, the proposed approach learns a probabilistic macro-to-microweather
mapping between regional weather forecasts and measured local wind velocities
using generative modeling (denoising diffusion probabilistic models, flow
matching, and Gaussian mixture models). A simple proof of concept was
implemented using a dataset comprised of local (micro) measurements from a
Sonic Detection and Ranging (SoDAR) wind profiler along with (macro) forecast
data from a nearby weather station over the same time period.",http://arxiv.org/pdf/2503.02690v1,,False
Cellular Automaton With CNN,04/03/2025,"Valery Ashu, Zhisong Liu, Heikki Haario, Andreas Rupp","Cellular automata (CA) models are widely used to simulate complex systems
with emergent behaviors, but identifying hidden parameters that govern their
dynamics remains a significant challenge. This study explores the use of
Convolutional Neural Networks (CNN) to identify jump parameters in a
two-dimensional CA model. We propose a custom CNN architecture trained on
CA-generated data to classify jump parameters, which dictates the neighborhood
size and movement rules of cells within the CA. Experiments were conducted
across varying domain sizes (25 x 25 to 150 x 150) and CA iterations (0 to 50),
demonstrating that the accuracy improves with larger domain sizes, as they
provide more spatial information for parameter estimation. Interestingly, while
initial CA iterations enhance the performance, increasing the number of
iterations beyond a certain threshold does not significantly improve accuracy,
suggesting that only specific temporal information is relevant for parameter
identification. The proposed CNN achieves competitive accuracy (89.31) compared
to established architectures like LeNet-5 and AlexNet, while offering
significantly faster inference times, making it suitable for real-time
applications. This study highlights the potential of CNNs as a powerful tool
for fast and accurate parameter estimation in CA models, paving the way for
their use in more complex systems and higher-dimensional domains. Future work
will explore the identification of multiple hidden parameters and extend the
approach to three-dimensional CA models.",http://arxiv.org/pdf/2503.02652v1,,False
Lightweight Channel-wise Dynamic Fusion Model: Non-stationary Time Series Forecasting via Entropy Analysis,04/03/2025,"Tianyu Jia, Zongxia Xie, Yanru Sun, Dilfira Kudrat, Qinghua Hu","Non-stationarity is an intrinsic property of real-world time series and plays
a crucial role in time series forecasting. Previous studies primarily adopt
instance normalization to attenuate the non-stationarity of original series for
better predictability. However, instance normalization that directly removes
the inherent non-stationarity can lead to three issues: (1) disrupting global
temporal dependencies, (2) ignoring channel-specific differences, and (3)
producing over-smoothed predictions. To address these issues, we theoretically
demonstrate that variance can be a valid and interpretable proxy for
quantifying non-stationarity of time series. Based on the analysis, we propose
a novel lightweight \textit{C}hannel-wise \textit{D}ynamic \textit{F}usion
\textit{M}odel (\textit{CDFM}), which selectively and dynamically recovers
intrinsic non-stationarity of the original series, while keeping the
predictability of normalized series. First, we design a Dual-Predictor Module,
which involves two branches: a Time Stationary Predictor for capturing stable
patterns and a Time Non-stationary Predictor for modeling global dynamics
patterns. Second, we propose a Fusion Weight Learner to dynamically
characterize the intrinsic non-stationary information across different samples
based on variance. Finally, we introduce a Channel Selector to selectively
recover non-stationary information from specific channels by evaluating their
non-stationarity, similarity, and distribution consistency, enabling the model
to capture relevant dynamic features and avoid overfitting. Comprehensive
experiments on seven time series datasets demonstrate the superiority and
generalization capabilities of CDFM.",http://arxiv.org/pdf/2503.02609v1,,False
StageDesigner: Artistic Stage Generation for Scenography via Theater Scripts,04/03/2025,"Zhaoxing Gan, Mengtian Li, Ruhua Chen, Zhongxia Ji, Sichen Guo, Huanling Hu, Guangnan Ye, Zuo Hu","In this work, we introduce StageDesigner, the first comprehensive framework
for artistic stage generation using large language models combined with
layout-controlled diffusion models. Given the professional requirements of
stage scenography, StageDesigner simulates the workflows of seasoned artists to
generate immersive 3D stage scenes. Specifically, our approach is divided into
three primary modules: Script Analysis, which extracts thematic and spatial
cues from input scripts; Foreground Generation, which constructs and arranges
essential 3D objects; and Background Generation, which produces a harmonious
background aligned with the narrative atmosphere and maintains spatial
coherence by managing occlusions between foreground and background elements.
Furthermore, we introduce the StagePro-V1 dataset, a dedicated dataset with 276
unique stage scenes spanning different historical styles and annotated with
scripts, images, and detailed 3D layouts, specifically tailored for this task.
Finally, evaluations using both standard and newly proposed metrics, along with
extensive user studies, demonstrate the effectiveness of StageDesigner. Project
can be found at: https://deadsmither5.github.io/2025/01/03/StageDesigner/",http://arxiv.org/pdf/2503.02595v1,,False
SAGE-Amine: Generative Amine Design with Multi-Property Optimization for Efficient CO2 Capture,04/03/2025,"Hocheol Lim, Hyein Cho, Jeonghoon Kim","Efficient CO2 capture is vital for mitigating climate change, with
amine-based solvents being widely used due to their strong reactivity with CO2.
However, optimizing key properties such as basicity, viscosity, and absorption
capacity remains challenging, as traditional methods rely on labor-intensive
experimentation and predefined chemical databases, limiting the exploration of
novel solutions. Here, SAGE-Amine was introduced, a generative modeling
approach that integrates Scoring-Assisted Generative Exploration (SAGE) with
quantitative structure-property relationship models to design new amines
tailored for CO2 capture. Unlike conventional virtual screening restricted to
existing compounds, SAGE-Amine generates novel amines by leveraging
autoregressive natural language processing models trained on amine datasets.
SAGE-Amine identified known amines for CO2 capture from scratch and
successfully performed single-property optimization, increasing basicity or
reducing viscosity or vapor pressure. Furthermore, it facilitated
multi-property optimization, simultaneously achieving high basicity with low
viscosity and vapor pressure. The 10 top-ranked amines were suggested using
SAGE-Amine and their thermodynamic properties were further assessed using
COSMO-RS simulations, confirming their potential for CO2 capture. These results
highlight the potential of generative modeling in accelerating the discovery of
amine solvents and expanding the possibilities for industrial CO2 capture
applications.",http://arxiv.org/pdf/2503.02534v1,,False
Don't Get Too Excited -- Eliciting Emotions in LLMs,04/03/2025,"Gino Franco Fazzi, Julie Skoven Hinge, Stefan Heinrich, Paolo Burelli","This paper investigates the challenges of affect control in large language
models (LLMs), focusing on their ability to express appropriate emotional
states during extended dialogues. We evaluated state-of-the-art open-weight
LLMs to assess their affective expressive range in terms of arousal and
valence. Our study employs a novel methodology combining LLM-based sentiment
analysis with multiturn dialogue simulations between LLMs. We quantify the
models' capacity to express a wide spectrum of emotions and how they fluctuate
during interactions. Our findings reveal significant variations among LLMs in
their ability to maintain consistent affect, with some models demonstrating
more stable emotional trajectories than others. Furthermore, we identify key
challenges in affect control, including difficulties in producing and
maintaining extreme emotional states and limitations in adapting affect to
changing conversational contexts. These findings have important implications
for the development of more emotionally intelligent AI systems and highlight
the need for improved affect modelling in LLMs.",http://arxiv.org/pdf/2503.02457v1,10.1145/3706599.3720191,False
Sparse Meets Dense: Unified Generative Recommendations with Cascaded Sparse-Dense Representations,04/03/2025,"Yuhao Yang, Zhi Ji, Zhaopeng Li, Yi Li, Zhonglin Mo, Yue Ding, Kai Chen, Zijian Zhang, Jie Li, Shuanglong Li, Lin Liu","Generative models have recently gained attention in recommendation systems by
directly predicting item identifiers from user interaction sequences. However,
existing methods suffer from significant information loss due to the separation
of stages such as quantization and sequence modeling, hindering their ability
to achieve the modeling precision and accuracy of sequential dense retrieval
techniques. Integrating generative and dense retrieval methods remains a
critical challenge. To address this, we introduce the Cascaded Organized
Bi-Represented generAtive retrieval (COBRA) framework, which innovatively
integrates sparse semantic IDs and dense vectors through a cascading process.
Our method alternates between generating these representations by first
generating sparse IDs, which serve as conditions to aid in the generation of
dense vectors. End-to-end training enables dynamic refinement of dense
representations, capturing both semantic insights and collaborative signals
from user-item interactions. During inference, COBRA employs a coarse-to-fine
strategy, starting with sparse ID generation and refining them into dense
vectors via the generative model. We further propose BeamFusion, an innovative
approach combining beam search with nearest neighbor scores to enhance
inference flexibility and recommendation diversity. Extensive experiments on
public datasets and offline tests validate our method's robustness. Online A/B
tests on a real-world advertising platform with over 200 million daily users
demonstrate substantial improvements in key metrics, highlighting COBRA's
practical advantages.",http://arxiv.org/pdf/2503.02453v1,,False
BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modelling,04/03/2025,"Hao Li, Yu-Hao Huang, Chang Xu, Viktor Schlegel, Ren-He Jiang, Riza Batista-Navarro, Goran Nenadic, Jiang Bian","Time-series Generation (TSG) is a prominent research area with broad
applications in simulations, data augmentation, and counterfactual analysis.
While existing methods have shown promise in unconditional single-domain TSG,
real-world applications demand for cross-domain approaches capable of
controlled generation tailored to domain-specific constraints and
instance-level requirements. In this paper, we argue that text can provide
semantic insights, domain information and instance-specific temporal patterns,
to guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused
on generating realistic time series by incorporating textual descriptions. To
address data scarcity in this setting, we propose a novel LLM-based Multi-Agent
framework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,
we introduce BRIDGE, a hybrid text-controlled TSG framework that integrates
semantic prototypes with text description for supporting domain-level guidance.
This approach achieves state-of-the-art generation fidelity on 11 of 12
datasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared
to no text input generation, highlighting its potential for generating tailored
time-series data.",http://arxiv.org/pdf/2503.02445v2,,False
Artificial Intelligence in Reactor Physics: Current Status and Future Prospects,04/03/2025,"Ruizhi Zhang, Shengfeng Zhu, Kan Wang, Ding She, Jean-Philippe Argaud, Bertrand Bouriquet, Qing Li, Helin Gong","Reactor physics is the study of neutron properties, focusing on using models
to examine the interactions between neutrons and materials in nuclear reactors.
Artificial intelligence (AI) has made significant contributions to reactor
physics, e.g., in operational simulations, safety design, real-time monitoring,
core management and maintenance. This paper presents a comprehensive review of
AI approaches in reactor physics, especially considering the category of
Machine Learning (ML), with the aim of describing the application scenarios,
frontier topics, unsolved challenges and future research directions. From
equation solving and state parameter prediction to nuclear industry
applications, this paper provides a step-by-step overview of ML methods applied
to steady-state, transient and combustion problems. Most literature works
achieve industry-demanded models by enhancing the efficiency of deterministic
methods or correcting uncertainty methods, which leads to successful
applications. However, research on ML methods in reactor physics is somewhat
fragmented, and the ability to generalize models needs to be strengthened.
Progress is still possible, especially in addressing theoretical challenges and
enhancing industrial applications such as building surrogate models and digital
twins.",http://arxiv.org/pdf/2503.02440v1,,False
VisAgent: Narrative-Preserving Story Visualization Framework,04/03/2025,"Seungkwon Kim, GyuTae Park, Sangyeon Kim, Seung-Hun Nam","Story visualization is the transformation of narrative elements into image
sequences. While existing research has primarily focused on visual contextual
coherence, the deeper narrative essence of stories often remains overlooked.
This limitation hinders the practical application of these approaches, as
generated images frequently fail to capture the intended meaning and nuances of
the narrative fully. To address these challenges, we propose VisAgent, a
training-free multi-agent framework designed to comprehend and visualize
pivotal scenes within a given story. By considering story distillation,
semantic consistency, and contextual coherence, VisAgent employs an agentic
workflow. In this workflow, multiple specialized agents collaborate to: (i)
refine layered prompts based on the narrative structure and (ii) seamlessly
integrate \gt{generated} elements, including refined prompts, scene elements,
and subject placement, into the final image. The empirically validated
effectiveness confirms the framework's suitability for practical story
visualization applications.",http://arxiv.org/pdf/2503.02399v1,,False
PersonaX: A Recommendation Agent Oriented User Modeling Framework for Long Behavior Sequence,04/03/2025,"Yunxiao Shi, Wujiang Xu, Zeqi Zhang, Xing Zi, Qiang Wu, Min Xu","Recommendation agents leverage large language models for user modeling LLM UM
to construct textual personas guiding alignment with real users. However
existing LLM UM methods struggle with long user generated content UGC due to
context limitations and performance degradation. To address this sampling
strategies prioritize relevance or recency are often applied yet they
inevitably neglect the diverse user interests embedded within the discarded
behaviors resulting in incomplete modeling and degraded profiling quality.
Furthermore relevance based sampling requires real time retrieval forcing the
user modeling process to operate online which introduces significant latency
overhead. In this paper we propose PersonaX an agent agnostic LLM UM framework
that tackles these challenges through sub behavior sequence SBS selection and
offline multi persona construction. PersonaX extracts compact SBS segments
offline to capture diverse user interests generating fine grained textual
personas that are cached for efficient online retrieval. This approach ensures
that the user persona used for prompting remains highly relevant to the current
context while eliminating the need for online user modeling. For SBS selection
we ensure both efficiency length less than five and high representational
quality by balancing prototypicality and diversity within the sampled data.
Extensive experiments validate the effectiveness and versatility of PersonaX in
high quality user profiling. Utilizing only 30 to 50 percent of the behavioral
data with a sequence length of 480 integrating PersonaX with AgentCF yields an
absolute performance improvement of 3 to 11 percent while integration with
Agent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic
framework sets a new benchmark for scalable user modeling paving the way for
more accurate and efficient LLM driven recommendation agents.",http://arxiv.org/pdf/2503.02398v1,,False
MindSimulator: Exploring Brain Concept Localization via Synthetic FMRI,04/03/2025,"Guangyin Bao, Qi Zhang, Zixuan Gong, Zhuojia Wu, Duoqian Miao","Concept-selective regions within the human cerebral cortex exhibit
significant activation in response to specific visual stimuli associated with
particular concepts. Precisely localizing these regions stands as a crucial
long-term goal in neuroscience to grasp essential brain functions and
mechanisms. Conventional experiment-driven approaches hinge on manually
constructed visual stimulus collections and corresponding brain activity
recordings, constraining the support and coverage of concept localization.
Additionally, these stimuli often consist of concept objects in unnatural
contexts and are potentially biased by subjective preferences, thus prompting
concerns about the validity and generalizability of the identified regions. To
address these limitations, we propose a data-driven exploration approach. By
synthesizing extensive brain activity recordings, we statistically localize
various concept-selective regions. Our proposed MindSimulator leverages
advanced generative technologies to learn the probability distribution of brain
activity conditioned on concept-oriented visual stimuli. This enables the
creation of simulated brain recordings that reflect real neural response
patterns. Using the synthetic recordings, we successfully localize several
well-studied concept-selective regions and validate them against empirical
findings, achieving promising prediction accuracy. The feasibility opens
avenues for exploring novel concept-selective regions and provides prior
hypotheses for future neuroscience research.",http://arxiv.org/pdf/2503.02351v1,,False
Text2Scenario: Text-Driven Scenario Generation for Autonomous Driving Test,04/03/2025,"Xuan Cai, Xuesong Bai, Zhiyong Cui, Danmu Xie, Daocheng Fu, Haiyang Yu, Yilong Ren","Autonomous driving (AD) testing constitutes a critical methodology for
assessing performance benchmarks prior to product deployment. The creation of
segmented scenarios within a simulated environment is acknowledged as a robust
and effective strategy; however, the process of tailoring these scenarios often
necessitates laborious and time-consuming manual efforts, thereby hindering the
development and implementation of AD technologies. In response to this
challenge, we introduce Text2Scenario, a framework that leverages a Large
Language Model (LLM) to autonomously generate simulation test scenarios that
closely align with user specifications, derived from their natural language
inputs. Specifically, an LLM, equipped with a meticulously engineered input
prompt scheme functions as a text parser for test scenario descriptions,
extracting from a hierarchically organized scenario repository the components
that most accurately reflect the user's preferences. Subsequently, by
exploiting the precedence of scenario components, the process involves
sequentially matching and linking scenario representations within a Domain
Specific Language corpus, ultimately fabricating executable test scenarios. The
experimental results demonstrate that such prompt engineering can meticulously
extract the nuanced details of scenario elements embedded within various
descriptive formats, with the majority of generated scenarios aligning closely
with the user's initial expectations, allowing for the efficient and precise
evaluation of diverse AD stacks void of the labor-intensive need for manual
scenario configuration. Project page:
https://caixxuan.github.io/Text2Scenario.GitHub.io.",http://arxiv.org/pdf/2503.02911v1,,False
DreamerV3 for Traffic Signal Control: Hyperparameter Tuning and Performance,04/03/2025,"Qiang Li, Yinhan Lin, Qin Luo, Lina Yu","Reinforcement learning (RL) has evolved into a widely investigated technology
for the development of smart TSC strategies. However, current RL algorithms
necessitate excessive interaction with the environment to learn effective
policies, making them impractical for large-scale tasks. The DreamerV3
algorithm presents compelling properties for policy learning. It summarizes
general dynamics knowledge about the environment and enables the prediction of
future outcomes of potential actions from past experience, reducing the
interaction with the environment through imagination training. In this paper, a
corridor TSC model is trained using the DreamerV3 algorithm to explore the
benefits of world models for TSC strategy learning. In RL environment design,
to manage congestion levels effectively, both the state and reward functions
are defined based on queue length, and the action is designed to manage queue
length efficiently. Using the SUMO simulation platform, the two hyperparameters
(training ratio and model size) of the DreamerV3 algorithm were tuned and
analyzed across different OD matrix scenarios. We discovered that choosing a
smaller model size and initially attempting several medium training ratios can
significantly reduce the time spent on hyperparameter tuning. Additionally, we
found that the approach is generally applicable as it can solve two TSC task
scenarios with the same hyperparameters. Regarding the claimed data-efficiency
of the DreamerV3 algorithm, due to the significant fluctuation of the episode
reward curve in the early stages of training, it can only be confirmed that
larger model sizes exhibit modest data-efficiency, and no evidence was found
that increasing the training ratio accelerates convergence.",http://arxiv.org/pdf/2503.02279v1,,False
REAct: Rational Exponential Activation for Better Learning and Generalization in PINNs,04/03/2025,"Sourav Mishra, Shreya Hallikeri, Suresh Sundaram","Physics-Informed Neural Networks (PINNs) offer a promising approach to
simulating physical systems. Still, their application is limited by
optimization challenges, mainly due to the lack of activation functions that
generalize well across several physical systems. Existing activation functions
often lack such flexibility and generalization power. To address this issue, we
introduce Rational Exponential Activation (REAct), a generalized form of tanh
consisting of four learnable shape parameters. Experiments show that REAct
outperforms many standard and benchmark activations, achieving an MSE three
orders of magnitude lower than tanh on heat problems and generalizing well to
finer grids and points beyond the training domain. It also excels at function
approximation tasks and improves noise rejection in inverse problems, leading
to more accurate parameter estimates across varying noise levels.",http://arxiv.org/pdf/2503.02267v1,,False
Large Language Models as Natural Selector for Embodied Soft Robot Design,04/03/2025,"Changhe Chen, Xiaohao Xu, Xiangdong Wang, Xiaonan Huang","Designing soft robots is a complex and iterative process that demands
cross-disciplinary expertise in materials science, mechanics, and control,
often relying on intuition and extensive experimentation. While Large Language
Models (LLMs) have demonstrated impressive reasoning abilities, their capacity
to learn and apply embodied design principles--crucial for creating functional
robotic systems--remains largely unexplored. This paper introduces
RoboCrafter-QA, a novel benchmark to evaluate whether LLMs can learn
representations of soft robot designs that effectively bridge the gap between
high-level task descriptions and low-level morphological and material choices.
RoboCrafter-QA leverages the EvoGym simulator to generate a diverse set of soft
robot design challenges, spanning robotic locomotion, manipulation, and
balancing tasks. Our experiments with state-of-the-art multi-modal LLMs reveal
that while these models exhibit promising capabilities in learning design
representations, they struggle with fine-grained distinctions between designs
with subtle performance differences. We further demonstrate the practical
utility of LLMs for robot design initialization. Our code and benchmark will be
available to encourage the community to foster this exciting research
direction.",http://arxiv.org/pdf/2503.02249v1,,False
Towards Heisenberg limit without critical slowing down via quantum reinforcement learning,04/03/2025,"Hang Xu, Tailong Xiao, Jingzheng Huang, Ming He, Jianping Fan, Guihua Zeng","Critical ground states of quantum many-body systems have emerged as vital
resources for quantum-enhanced sensing. Traditional methods to prepare these
states often rely on adiabatic evolution, which may diminish the quantum
sensing advantage. In this work, we propose a quantum reinforcement learning
(QRL)-enhanced critical sensing protocol for quantum many-body systems with
exotic phase diagrams. Starting from product states and utilizing
QRL-discovered gate sequences, we explore sensing accuracy in the presence of
unknown external magnetic fields, covering both local and global regimes. Our
results demonstrate that QRL-learned sequences reach the finite quantum speed
limit and generalize effectively across systems of arbitrary size, ensuring
accuracy regardless of preparation time. This method can robustly achieve
Heisenberg and super-Heisenberg limits, even in noisy environments with
practical Pauli measurements. Our study highlights the efficacy of QRL in
enabling precise quantum state preparation, thereby advancing scalable,
high-accuracy quantum critical sensing.",http://arxiv.org/pdf/2503.02210v1,,False
X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography from Chest Radiography via Tri-Modal Contrastive Learning,04/03/2025,"Jianzhong You, Yuan Gao, Sangwook Kim, Chris Mcintosh","Computed tomography (CT) is a key imaging modality for diagnosis, yet its
clinical utility is marred by high radiation exposure and long turnaround
times, restricting its use for larger-scale screening. Although chest
radiography (CXR) is more accessible and safer, existing CXR foundation models
focus primarily on detecting diseases that are readily visible on the CXR.
Recently, works have explored training disease classification models on
simulated CXRs, but they remain limited to recognizing a single disease type
from CT. CT foundation models have also emerged with significantly improved
detection of pathologies in CT. However, the generalized application of
CT-derived labels on CXR has remained illusive. In this study, we propose
X2CT-CLIP, a tri-modal knowledge transfer learning framework that bridges the
modality gap between CT and CXR while reducing the computational burden of
model training. Our approach is the first work to enable multi-abnormality
classification in CT, using CXR, by transferring knowledge from 3D CT volumes
and associated radiology reports to a CXR encoder via a carefully designed
tri-modal alignment mechanism in latent space. Extensive evaluations on three
multi-label CT datasets demonstrate that our method outperforms
state-of-the-art baselines in cross-modal retrieval, few-shot adaptation, and
external validation. These results highlight the potential of CXR, enriched
with knowledge derived from CT, as a viable efficient alternative for disease
detection in resource-limited settings.",http://arxiv.org/pdf/2503.02162v1,,False
Frankenstein Optimizer: Harnessing the Potential by Revisiting Optimization Tricks,04/03/2025,"Chia-Wei Hsu, Nien-Ti Tsou, Yu-Cheng Chen, Yang Jeong Park, Ju Li","Gradient-based optimization drives the unprecedented performance of modern
deep neural network models across diverse applications. Adaptive algorithms
have accelerated neural network training due to their rapid convergence rates;
however, they struggle to find ``flat minima"" reliably, resulting in suboptimal
generalization compared to stochastic gradient descent (SGD). By revisiting
various adaptive algorithms' mechanisms, we propose the Frankenstein optimizer,
which combines their advantages. The proposed Frankenstein dynamically adjusts
first- and second-momentum coefficients according to the optimizer's current
state to directly maintain consistent learning dynamics and immediately reflect
sudden gradient changes. Extensive experiments across several research domains
such as computer vision, natural language processing, few-shot learning, and
scientific simulations show that Frankenstein surpasses existing adaptive
algorithms and SGD empirically regarding convergence speed and generalization
performance. Furthermore, this research deepens our understanding of adaptive
algorithms through centered kernel alignment analysis and loss landscape
visualization during the learning process.",http://arxiv.org/pdf/2503.02147v1,,False
Forgetting Transformer: Softmax Attention with a Forget Gate,03/03/2025,"Zhixuan Lin, Evgenii Nikishin, Xu Owen He, Aaron Courville","An essential component of modern recurrent sequence models is the forget
gate. While Transformers do not have an explicit recurrent form, we show that a
forget gate can be naturally incorporated into Transformers by down-weighting
the unnormalized attention scores in a data-dependent way. We name this
attention mechanism the Forgetting Attention and the resulting model the
Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on
long-context language modeling, length extrapolation, and short-context
downstream tasks, while performing on par with the Transformer on long-context
downstream tasks. Moreover, it is compatible with the FlashAttention algorithm
and does not require any positional embeddings. Several analyses, including the
needle-in-the-haystack test, show that FoX also retains the Transformer's
superior long-context capabilities over recurrent sequence models such as
Mamba-2, HGRN2, and DeltaNet. We also introduce a ""Pro"" block design that
incorporates some common architectural components in recurrent sequence models
and find it significantly improves the performance of both FoX and the
Transformer. Our code is available at
https://github.com/zhixuan-lin/forgetting-transformer.",http://arxiv.org/pdf/2503.02130v1,,False
Provable Benefits of Task-Specific Prompts for In-context Learning,03/03/2025,"Xiangyu Chang, Yingcong Li, Muti Kara, Samet Oymak, Amit K. Roy-Chowdhury","The in-context learning capabilities of modern language models have motivated
a deeper mathematical understanding of sequence models. A line of recent work
has shown that linear attention models can emulate projected gradient descent
iterations to implicitly learn the task vector from the data provided in the
context window. In this work, we consider a novel setting where the global task
distribution can be partitioned into a union of conditional task distributions.
We then examine the use of task-specific prompts and prediction heads for
learning the prior information associated with the conditional task
distribution using a one-layer attention model. Our results on loss landscape
show that task-specific prompts facilitate a covariance-mean decoupling where
prompt-tuning explains the conditional mean of the distribution whereas the
variance is learned/explained through in-context learning. Incorporating
task-specific head further aids this process by entirely decoupling estimation
of mean and variance components. This covariance-mean perspective similarly
explains how jointly training prompt and attention weights can provably help
over fine-tuning after pretraining.",http://arxiv.org/pdf/2503.02102v2,,False
Linear Representations of Political Perspective Emerge in Large Language Models,03/03/2025,"Junsol Kim, James Evans, Aaron Schein","Large language models (LLMs) have demonstrated the ability to generate text
that realistically reflects a range of different subjective human perspectives.
This paper studies how LLMs are seemingly able to reflect more liberal versus
more conservative viewpoints among other political perspectives in American
politics. We show that LLMs possess linear representations of political
perspectives within activation space, wherein more similar perspectives are
represented closer together. To do so, we probe the attention heads across the
layers of three open transformer-based LLMs (\texttt{Llama-2-7b-chat},
\texttt{Mistral-7b-instruct}, \texttt{Vicuna-7b}). We first prompt models to
generate text from the perspectives of different U.S.~lawmakers. We then
identify sets of attention heads whose activations linearly predict those
lawmakers' DW-NOMINATE scores, a widely-used and validated measure of political
ideology. We find that highly predictive heads are primarily located in the
middle layers, often speculated to encode high-level concepts and tasks. Using
probes only trained to predict lawmakers' ideology, we then show that the same
probes can predict measures of news outlets' slant from the activations of
models prompted to simulate text from those news outlets. These linear probes
allow us to visualize, interpret, and monitor ideological stances implicitly
adopted by an LLM as it generates open-ended responses. Finally, we demonstrate
that by applying linear interventions to these attention heads, we can steer
the model outputs toward a more liberal or conservative stance. Overall, our
research suggests that LLMs possess a high-level linear representation of
American political ideology and that by leveraging recent advances in
mechanistic interpretability, we can identify, monitor, and steer the
subjective perspective underlying generated text.",http://arxiv.org/pdf/2503.02080v1,,False
RiboGen: RNA Sequence and Structure Co-Generation with Equivariant MultiFlow,03/03/2025,"Dana Rubin, Allan dos Santos Costa, Manvitha Ponnapati, Joseph Jacobson","Ribonucleic acid (RNA) plays fundamental roles in biological systems, from
carrying genetic information to performing enzymatic function. Understanding
and designing RNA can enable novel therapeutic application and biotechnological
innovation. To enhance RNA design, in this paper we introduce RiboGen, the
first deep learning model to simultaneously generate RNA sequence and all-atom
3D structure. RiboGen leverages the standard Flow Matching with Discrete Flow
Matching in a multimodal data representation. RiboGen is based on Euclidean
Equivariant neural networks for efficiently processing and learning
three-dimensional geometry. Our experiments show that RiboGen can efficiently
generate chemically plausible and self-consistent RNA samples. Our results
suggest that co-generation of sequence and structure is a competitive approach
for modeling RNA.",http://arxiv.org/pdf/2503.02058v1,,False
Dynamic Search for Inference-Time Alignment in Diffusion Models,03/03/2025,"Xiner Li, Masatoshi Uehara, Xingyu Su, Gabriele Scalia, Tommaso Biancalani, Aviv Regev, Sergey Levine, Shuiwang Ji","Diffusion models have shown promising generative capabilities across diverse
domains, yet aligning their outputs with desired reward functions remains a
challenge, particularly in cases where reward functions are non-differentiable.
Some gradient-free guidance methods have been developed, but they often
struggle to achieve optimal inference-time alignment. In this work, we newly
frame inference-time alignment in diffusion as a search problem and propose
Dynamic Search for Diffusion (DSearch), which subsamples from denoising
processes and approximates intermediate node rewards. It also dynamically
adjusts beam width and tree expansion to efficiently explore high-reward
generations. To refine intermediate decisions, DSearch incorporates adaptive
scheduling based on noise levels and a lookahead heuristic function. We
validate DSearch across multiple domains, including biological sequence design,
molecular optimization, and image generation, demonstrating superior reward
optimization compared to existing approaches.",http://arxiv.org/pdf/2503.02039v1,,False
Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs,03/03/2025,"Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, Dong Chen, Dongdong Chen, Junkun Chen, Weizhu Chen, Yen-Chun Chen, Yi-ling Chen, Qi Dai, Xiyang Dai, Ruchao Fan, Mei Gao, Min Gao, Amit Garg, Abhishek Goswami, Junheng Hao, Amr Hendy, Yuxuan Hu, Xin Jin, Mahmoud Khademi, Dongwoo Kim, Young Jin Kim, Gina Lee, Jinyu Li, Yunsheng Li, Chen Liang, Xihui Lin, Zeqi Lin, Mengchen Liu, Yang Liu, Gilsinia Lopez, Chong Luo, Piyush Madan, Vadim Mazalov, Ali Mousavi, Anh Nguyen, Jing Pan, Daniel Perez-Becker, Jacob Platin, Thomas Portet, Kai Qiu, Bo Ren, Liliang Ren, Sambuddha Roy, Ning Shang, Yelong Shen, Saksham Singhal, Subhojit Som, Xia Song, Tetyana Sych, Praneetha Vaddamanu, Shuohang Wang, Yiming Wang, Zhenghao Wang, Haibin Wu, Haoran Xu, Weijian Xu, Yifan Yang, Ziyi Yang, Donghan Yu, Ishmam Zabir, Jianwen Zhang, Li Lyna Zhang, Yunan Zhang, Xiren Zhou","We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable
language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language
model trained on high-quality web and synthetic data, significantly
outperforming recent open-source models of similar size and matching the
performance of models twice its size on math and coding tasks requiring complex
reasoning. This achievement is driven by a carefully curated synthetic data
recipe emphasizing high-quality math and coding datasets. Compared to its
predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of
200K tokens to better support multilingual applications, as well as group query
attention for more efficient long-sequence generation. Phi-4-Multimodal is a
multimodal model that integrates text, vision, and speech/audio input
modalities into a single model. Its novel modality extension approach leverages
LoRA adapters and modality-specific routers to allow multiple inference modes
combining various modalities without interference. For example, it now ranks
first in the OpenASR leaderboard to date, although the LoRA component of the
speech/audio modality has just 460 million parameters. Phi-4-Multimodal
supports scenarios involving (vision + language), (vision + speech), and
(speech/audio) inputs, outperforming larger vision-language and speech-language
models on a wide range of tasks. Additionally, we experiment to further train
Phi-4-Mini to enhance its reasoning capabilities. Despite its compact
3.8-billion-parameter size, this experimental version achieves reasoning
performance on par with or surpassing significantly larger models, including
DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.",http://arxiv.org/pdf/2503.01743v1,,False
Self-attention-based Diffusion Model for Time-series Imputation in Partial Blackout Scenarios,03/03/2025,"Mohammad Rafid Ul Islam, Prasad Tadepalli, Alan Fern","Missing values in multivariate time series data can harm machine learning
performance and introduce bias. These gaps arise from sensor malfunctions,
blackouts, and human error and are typically addressed by data imputation.
Previous work has tackled the imputation of missing data in random, complete
blackouts and forecasting scenarios. The current paper addresses a more general
missing pattern, which we call ""partial blackout,"" where a subset of features
is missing for consecutive time steps. We introduce a two-stage imputation
process using self-attention and diffusion processes to model feature and
temporal correlations. Notably, our model effectively handles missing data
during training, enhancing adaptability and ensuring reliable imputation and
performance, even with incomplete datasets. Our experiments on benchmark and
two real-world time series datasets demonstrate that our model outperforms the
state-of-the-art in partial blackout scenarios and shows better scalability.",http://arxiv.org/pdf/2503.01737v1,,False
KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via KeyFrame Interpolation,03/03/2025,"Antoni Bigata, MichaÅ StypuÅkowski, Rodrigo Mira, Stella Bounareli, Konstantinos Vougioukas, Zoe Landgraf, Nikita Drobyshev, Maciej Zieba, Stavros Petridis, Maja Pantic","Current audio-driven facial animation methods achieve impressive results for
short videos but suffer from error accumulation and identity drift when
extended to longer durations. Existing methods attempt to mitigate this through
external spatial control, increasing long-term consistency but compromising the
naturalness of motion. We propose KeyFace, a novel two-stage diffusion-based
framework, to address these issues. In the first stage, keyframes are generated
at a low frame rate, conditioned on audio input and an identity frame, to
capture essential facial expressions and movements over extended periods of
time. In the second stage, an interpolation model fills in the gaps between
keyframes, ensuring smooth transitions and temporal coherence. To further
enhance realism, we incorporate continuous emotion representations and handle a
wide range of non-speech vocalizations (NSVs), such as laughter and sighs. We
also introduce two new evaluation metrics for assessing lip synchronization and
NSV generation. Experimental results show that KeyFace outperforms
state-of-the-art methods in generating natural, coherent facial animations over
extended durations, successfully encompassing NSVs and continuous emotions.",http://arxiv.org/pdf/2503.01715v1,,False
Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens,03/03/2025,"Xinsheng Wang, Mingqi Jiang, Ziyang Ma, Ziyu Zhang, Songxiang Liu, Linqin Li, Zheng Liang, Qixi Zheng, Rui Wang, Xiaoqin Feng, Weizhen Bian, Zhen Ye, Sitong Cheng, Ruibin Yuan, Zhixian Zhao, Xinfa Zhu, Jiahao Pan, Liumeng Xue, Pengcheng Zhu, Yunlin Chen, Zhifei Li, Xie Chen, Lei Xie, Yike Guo, Wei Xue","Recent advancements in large language models (LLMs) have driven significant
progress in zero-shot text-to-speech (TTS) synthesis. However, existing
foundation models rely on multi-stage processing or complex architectures for
predicting multiple codebooks, limiting efficiency and integration flexibility.
To overcome these challenges, we introduce Spark-TTS, a novel system powered by
BiCodec, a single-stream speech codec that decomposes speech into two
complementary token types: low-bitrate semantic tokens for linguistic content
and fixed-length global tokens for speaker attributes. This disentangled
representation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT)
generation approach, enables both coarse-grained control (e.g., gender,
speaking style) and fine-grained adjustments (e.g., precise pitch values,
speaking rate). To facilitate research in controllable TTS, we introduce
VoxBox, a meticulously curated 100,000-hour dataset with comprehensive
attribute annotations. Extensive experiments demonstrate that Spark-TTS not
only achieves state-of-the-art zero-shot voice cloning but also generates
highly customizable voices that surpass the limitations of reference-based
synthesis. Source code, pre-trained models, and audio samples are available at
https://github.com/SparkAudio/Spark-TTS.",http://arxiv.org/pdf/2503.01710v1,,False
Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via Symbolic Code Generation,03/03/2025,"Yongchao Chen, Yilun Hao, Yang Zhang, Chuchu Fan","Recent works have shown great potentials of Large Language Models (LLMs) in
robot task and motion planning (TAMP). Current LLM approaches generate text- or
code-based reasoning chains with sub-goals and action plans. However, they do
not fully leverage LLMs' symbolic computing and code generation capabilities.
Many robot TAMP tasks involve complex optimization under multiple constraints,
where pure textual reasoning is insufficient. While augmenting LLMs with
predefined solvers and planners improves performance, it lacks generalization
across tasks. Given LLMs' growing coding proficiency, we enhance their TAMP
capabilities by steering them to generate code as symbolic planners for
optimization and constraint verification. Unlike prior work that uses code to
interface with robot action modules, we steer LLMs to generate code as solvers,
planners, and checkers for TAMP tasks requiring symbolic computing, while still
leveraging textual reasoning to incorporate common sense. With a multi-round
guidance and answer evolution framework, the proposed Code-as-Symbolic-Planner
improves success rates by average 24.1\% over best baseline methods across
seven typical TAMP tasks and three popular LLMs. Code-as-Symbolic-Planner shows
strong effectiveness and generalizability across discrete and continuous
environments, 2D/3D simulations and real-world settings, as well as single- and
multi-robot tasks with diverse requirements. See our project website
https://yongchao98.github.io/Code-Symbol-Planner/ for prompts, videos, and
code.",http://arxiv.org/pdf/2503.01700v1,,False
Perceptual Motor Learning with Active Inference Framework for Robust Lateral Control,03/03/2025,"Elahe Delavari, John Moore, Junho Hong, Jaerock Kwon","This paper presents a novel Perceptual Motor Learning (PML) framework
integrated with Active Inference (AIF) to enhance lateral control in Highly
Automated Vehicles (HAVs). PML, inspired by human motor learning, emphasizes
the seamless integration of perception and action, enabling efficient
decision-making in dynamic environments. Traditional autonomous driving
approaches--including modular pipelines, imitation learning, and reinforcement
learning--struggle with adaptability, generalization, and computational
efficiency. In contrast, PML with AIF leverages a generative model to minimize
prediction error (""surprise"") and actively shape vehicle control based on
learned perceptual-motor representations. Our approach unifies deep learning
with active inference principles, allowing HAVs to perform lane-keeping
maneuvers with minimal data and without extensive retraining across different
environments. Extensive experiments in the CARLA simulator demonstrate that PML
with AIF enhances adaptability without increasing computational overhead while
achieving performance comparable to conventional methods. These findings
highlight the potential of PML-driven active inference as a robust alternative
for real-world autonomous driving applications.",http://arxiv.org/pdf/2503.01676v2,,False
Using (Not so) Large Language Models for Generating Simulation Models in a Formal DSL -- A Study on Reaction Networks,03/03/2025,"Justin N. Kreikemeyer, MiÅosz Jankowski, Pia Wilsdorf, Adelinde M. Uhrmacher","Formal languages are an integral part of modeling and simulation. They allow
the distillation of knowledge into concise simulation models amenable to
automatic execution, interpretation, and analysis. However, the arguably most
humanly accessible means of expressing models is through natural language,
which is not easily interpretable by computers. Here, we evaluate how a Large
Language Model (LLM) might be used for formalizing natural language into
simulation models. Existing studies only explored using very large LLMs, like
the commercial GPT models, without fine-tuning model weights. To close this
gap, we show how an open-weights, 7B-parameter Mistral model can be fine-tuned
to translate natural language descriptions to reaction network models in a
domain-specific language, offering a self-hostable, compute-, and memory
efficient alternative. To this end, we develop a synthetic data generator to
serve as the basis for fine-tuning and evaluation. Our quantitative evaluation
shows that our fine-tuned Mistral model can recover the ground truth simulation
model in up to 84.5% of cases. In addition, our small-scale user study
demonstrates the model's practical potential for one-time generation as well as
interactive modeling in various domains. While promising, in its current form,
the fine-tuned small LLM cannot catch up with large LLMs. We conclude that
higher-quality training data are required, and expect future small and
open-source LLMs to offer new opportunities.",http://arxiv.org/pdf/2503.01675v1,,False
CAPS: Context-Aware Priority Sampling for Enhanced Imitation Learning in Autonomous Driving,03/03/2025,"Hamidreza Mirkhani, Behzad Khamidehi, Ehsan Ahmadi, Fazel Arasteh, Mohammed Elmahgiubi, Weize Zhang, Umar Rajguru, Kasra Rezaee","In this paper, we introduce CAPS (Context-Aware Priority Sampling), a novel
method designed to enhance data efficiency in learning-based autonomous driving
systems. CAPS addresses the challenge of imbalanced training datasets in
imitation learning by leveraging Vector Quantized Variational Autoencoders
(VQ-VAEs). The use of VQ-VAE provides a structured and interpretable data
representation, which helps reveal meaningful patterns in the data. These
patterns are used to group the data into clusters, with each sample being
assigned a cluster ID. The cluster IDs are then used to re-balance the dataset,
ensuring that rare yet valuable samples receive higher priority during
training. By ensuring a more diverse and informative training set, CAPS
improves the generalization of the trained planner across a wide range of
driving scenarios. We evaluate our method through closed-loop simulations in
the CARLA environment. The results on Bench2Drive scenarios demonstrate that
our framework outperforms state-of-the-art methods, leading to notable
improvements in model performance.",http://arxiv.org/pdf/2503.01650v1,,False
CoT-VLM4Tar: Chain-of-Thought Guided Vision-Language Models for Traffic Anomaly Resolution,03/03/2025,"Tianchi Ren, Haibo Hu, Jiacheng Zuo, Xinhong Chen, Jianping Wang, Chun Jason Xue, Jen-Ming Wu, Nan Guan","With the acceleration of urbanization, modern urban traffic systems are
becoming increasingly complex, leading to frequent traffic anomalies. These
anomalies encompass not only common traffic jams but also more challenging
issues such as phantom traffic jams, intersection deadlocks, and accident
liability analysis, which severely impact traffic flow, vehicular safety, and
overall transportation efficiency. Currently, existing solutions primarily rely
on manual intervention by traffic police or artificial intelligence-based
detection systems. However, these methods often suffer from response delays and
inconsistent management due to inadequate resources, while AI detection
systems, despite enhancing efficiency to some extent, still struggle to handle
complex traffic anomalies in a real-time and precise manner. To address these
issues, we propose CoT-VLM4Tar: (Chain of Thought Visual-Language Model for
Traffic Anomaly Resolution), this innovative approach introduces a new
chain-of-thought to guide the VLM in analyzing, reasoning, and generating
solutions for traffic anomalies with greater reasonable and effective solution,
and to evaluate the performance and effectiveness of our method, we developed a
closed-loop testing framework based on the CARLA simulator. Furthermore, to
ensure seamless integration of the solutions generated by the VLM with the
CARLA simulator, we implement an itegration module that converts these
solutions into executable commands. Our results demonstrate the effectiveness
of VLM in the resolution of real-time traffic anomalies, providing a
proof-of-concept for its integration into autonomous traffic management
systems.",http://arxiv.org/pdf/2503.01632v1,,False
Triple-Stream Deep Feature Selection with Metaheuristic Optimization and Machine Learning for Multi-Stage Hypertensive Retinopathy Diagnosis,03/03/2025,"Suleyman Burcin Suyun, Mustafa Yurdakul, Sakir Tasdemir, Serkan Bilic","Hypertensive retinopathy (HR) is a severe eye disease that may cause
permanent vision loss if not diagnosed early. Traditional diagnostic methods
are time-consuming and subjective, highlighting the need for an automated,
reliable system. Existing studies often use a single Deep Learning (DL) model,
struggling to distinguish HR stages. This study introduces a three-stage
approach to enhance HR diagnosis accuracy. Initially, 14 CNN models were
tested, identifying DenseNet169, MobileNet, and ResNet152 as the most
effective. DenseNet169 achieved 87.73% accuracy, 87.75% precision, 87.73%
recall, 87.67% F1-score, and 0.8359 Cohen's Kappa. MobileNet followed with
86.40% accuracy, 86.60% precision, 86.40% recall, 86.31% F1-score, and 0.8180
Cohen's Kappa. ResNet152 ranked third with 85.87% accuracy, 86.01% precision,
85.87% recall, 85.83% F1-score, and 0.8188 Cohen's Kappa. In the second stage,
deep features from these models were fused and classified using Machine
Learning (ML) algorithms (SVM, RF, XGBoost). SVM (sigmoid kernel) performed
best with 92.00% accuracy, 91.93% precision, 92.00% recall, 91.91% F1-score,
and 0.8930 Cohen's Kappa. The third stage applied meta-heuristic optimization
(GA, ABC, PSO, HHO) for feature selection. HHO yielded 94.66% accuracy,
precision, and recall, 94.64% F1-score, and 0.9286 Cohen's Kappa. The proposed
approach surpassed single CNN models and previous studies in HR diagnosis
accuracy and generalization.",http://arxiv.org/pdf/2503.01603v1,,False
SENSEI: Semantic Exploration Guided by Foundation Models to Learn Versatile World Models,03/03/2025,"Cansu Sancaktar, Christian Gumbsch, Andrii Zadaianchuk, Pavel Kolev, Georg Martius","Exploration is a cornerstone of reinforcement learning (RL). Intrinsic
motivation attempts to decouple exploration from external, task-based rewards.
However, established approaches to intrinsic motivation that follow general
principles such as information gain, often only uncover low-level interactions.
In contrast, children's play suggests that they engage in meaningful high-level
behavior by imitating or interacting with their caregivers. Recent work has
focused on using foundation models to inject these semantic biases into
exploration. However, these methods often rely on unrealistic assumptions, such
as language-embedded environments or access to high-level actions. We propose
SEmaNtically Sensible ExploratIon (SENSEI), a framework to equip model-based RL
agents with an intrinsic motivation for semantically meaningful behavior.
SENSEI distills a reward signal of interestingness from Vision Language Model
(VLM) annotations, enabling an agent to predict these rewards through a world
model. Using model-based RL, SENSEI trains an exploration policy that jointly
maximizes semantic rewards and uncertainty. We show that in both robotic and
video game-like simulations SENSEI discovers a variety of meaningful behaviors
from image observations and low-level actions. SENSEI provides a general tool
for learning from foundation model feedback, a crucial research direction, as
VLMs become more powerful.",http://arxiv.org/pdf/2503.01584v1,,False
FlowDec: A flow-based full-band general audio codec with high perceptual quality,03/03/2025,"Simon Welker, Matthew Le, Ricky T. Q. Chen, Wei-Ning Hsu, Timo Gerkmann, Alexander Richard, Yi-Chiao Wu","We propose FlowDec, a neural full-band audio codec for general audio sampled
at 48 kHz that combines non-adversarial codec training with a stochastic
postfilter based on a novel conditional flow matching method. Compared to the
prior work ScoreDec which is based on score matching, we generalize from speech
to general audio and move from 24 kbit/s to as low as 4 kbit/s, while improving
output quality and reducing the required postfilter DNN evaluations from 60 to
6 without any fine-tuning or distillation techniques. We provide theoretical
insights and geometric intuitions for our approach in comparison to ScoreDec as
well as another recent work that uses flow matching, and conduct ablation
studies on our proposed components. We show that FlowDec is a competitive
alternative to the recent GAN-dominated stream of neural codecs, achieving FAD
scores better than those of the established GAN-based codec DAC and listening
test scores that are on par, and producing qualitatively more natural
reconstructions for speech and harmonic structures in music.",http://arxiv.org/pdf/2503.01485v1,,False
Primer C-VAE: An interpretable deep learning primer design method to detect emerging virus variants,03/03/2025,"Hanyu Wang, Emmanuel K. Tsinda, Anthony J. Dunn, Francis Chikweto, Alain B. Zemkoho","Motivation: PCR is more economical and quicker than Next Generation
Sequencing for detecting target organisms, with primer design being a critical
step. In epidemiology with rapidly mutating viruses, designing effective
primers is challenging. Traditional methods require substantial manual
intervention and struggle to ensure effective primer design across different
strains. For organisms with large, similar genomes like Escherichia coli and
Shigella flexneri, differentiating between species is also difficult but
crucial.
  Results: We developed Primer C-VAE, a model based on a Variational
Auto-Encoder framework with Convolutional Neural Networks to identify variants
and generate specific primers. Using SARS-CoV-2, our model classified variants
(alpha, beta, gamma, delta, omicron) with 98% accuracy and generated
variant-specific primers. These primers appeared with >95% frequency in target
variants and <5% in others, showing good performance in in-silico PCR tests.
For Alpha, Delta, and Omicron, our primer pairs produced fragments <200 bp,
suitable for qPCR detection. The model also generated effective primers for
organisms with longer gene sequences like E. coli and S. flexneri.
  Conclusion: Primer C-VAE is an interpretable deep learning approach for
developing specific primer pairs for target organisms. This flexible,
semi-automated and reliable tool works regardless of sequence completeness and
length, allowing for qPCR applications and can be applied to organisms with
large and highly similar genomes.",http://arxiv.org/pdf/2503.01459v1,,False
How simple can you go? An off-the-shelf transformer approach to molecular dynamics,03/03/2025,"Max Eissler, Tim Korjakow, Stefan Ganscha, Oliver T. Unke, Klaus-Robert MÃ¼ller, Stefan Gugler","Most current neural networks for molecular dynamics (MD) include physical
inductive biases, resulting in specialized and complex architectures. This is
in contrast to most other machine learning domains, where specialist approaches
are increasingly replaced by general-purpose architectures trained on vast
datasets. In line with this trend, several recent studies have questioned the
necessity of architectural features commonly found in MD models, such as
built-in rotational equivariance or energy conservation. In this work, we
contribute to the ongoing discussion by evaluating the performance of an MD
model with as few specialized architectural features as possible. We present a
recipe for MD using an Edge Transformer, an ""off-the-shelf'' transformer
architecture that has been minimally modified for the MD domain, termed MD-ET.
Our model implements neither built-in equivariance nor energy conservation. We
use a simple supervised pre-training scheme on $\sim$30 million molecular
structures from the QCML database. Using this ""off-the-shelf'' approach, we
show state-of-the-art results on several benchmarks after fine-tuning for a
small number of steps. Additionally, we examine the effects of being only
approximately equivariant and energy conserving for MD simulations, proposing a
novel method for distinguishing the errors resulting from non-equivariance from
other sources of inaccuracies like numerical rounding errors. While our model
exhibits runaway energy increases on larger structures, we show approximately
energy-conserving NVE simulations for a range of small structures.",http://arxiv.org/pdf/2503.01431v2,,False
Surgical Vision World Model,03/03/2025,"Saurabh Koju, Saurav Bastola, Prashant Shrestha, Sanskar Amgain, Yash Raj Shrestha, Rudra P. K. Poudel, Binod Bhattarai","Realistic and interactive surgical simulation has the potential to facilitate
crucial applications, such as medical professional training and autonomous
surgical agent training. In the natural visual domain, world models have
enabled action-controlled data generation, demonstrating the potential to train
autonomous agents in interactive simulated environments when large-scale real
data acquisition is infeasible. However, such works in the surgical domain have
been limited to simplified computer simulations, and lack realism. Furthermore,
existing literature in world models has predominantly dealt with action-labeled
data, limiting their applicability to real-world surgical data, where obtaining
action annotation is prohibitively expensive. Inspired by the recent success of
Genie in leveraging unlabeled video game data to infer latent actions and
enable action-controlled data generation, we propose the first surgical vision
world model. The proposed model can generate action-controllable surgical data
and the architecture design is verified with extensive experiments on the
unlabeled SurgToolLoc-2022 dataset. Codes and implementation details are
available at https://github.com/bhattarailab/Surgical-Vision-World-Model",http://arxiv.org/pdf/2503.02904v1,,False
ACTIVA: Amortized Causal Effect Estimation without Graphs via Transformer-based Variational Autoencoder,03/03/2025,"Andreas Sauter, Saber Salehkaleybar, Aske Plaat, Erman Acar","Predicting the distribution of outcomes under hypothetical interventions is
crucial in domains like healthcare, economics, and policy-making. Current
methods often rely on strong assumptions, such as known causal graphs or
parametric models, and lack amortization across problem instances, limiting
their practicality. We propose a novel transformer-based conditional
variational autoencoder architecture, named ACTIVA, that extends causal
transformer encoders to predict causal effects as mixtures of Gaussians. Our
method requires no causal graph and predicts interventional distributions given
only observational data and a queried intervention. By amortizing over many
simulated instances, it enables zero-shot generalization to novel datasets
without retraining. Experiments demonstrate accurate predictions for synthetic
and semi-synthetic data, showcasing the effectiveness of our graph-free,
amortized causal inference approach.",http://arxiv.org/pdf/2503.01290v1,,False
Decision-Focused Fine-Tuning of Time Series Foundation Models for Dispatchable Feeder Optimization,03/03/2025,"Maximilian Beichter, Nils Friederich, Janik Pinter, Dorina Werling, Kaleb Phipps, Sebastian Beichter, Oliver Neumann, Ralf Mikut, Veit Hagenmeyer, Benedikt Heidrich","Time series foundation models provide a universal solution for generating
forecasts to support optimization problems in energy systems. Those foundation
models are typically trained in a prediction-focused manner to maximize
forecast quality. In contrast, decision-focused learning directly improves the
resulting value of the forecast in downstream optimization rather than merely
maximizing forecasting quality. The practical integration of forecast values
into forecasting models is challenging, particularly when addressing complex
applications with diverse instances, such as buildings. This becomes even more
complicated when instances possess specific characteristics that require
instance-specific, tailored predictions to increase the forecast value. To
tackle this challenge, we use decision-focused fine-tuning within time series
foundation models to offer a scalable and efficient solution for
decision-focused learning applied to the dispatchable feeder optimization
problem. To obtain more robust predictions for scarce building data, we use
Moirai as a state-of-the-art foundation model, which offers robust and
generalized results with few-shot parameter-efficient fine-tuning. Comparing
the decision-focused fine-tuned Moirai with a state-of-the-art classical
prediction-focused fine-tuning Morai, we observe an improvement of 9.45% in
average total daily costs.",http://arxiv.org/pdf/2503.01936v1,,False
Gaussian Process Surrogate Models for Efficient Estimation of Structural Response Distributions and Order Statistics,03/03/2025,"Vegard Flovik, Sebastian Winter, Christian Agrell","Engineering disciplines often rely on extensive simulations to ensure that
structures are designed to withstand harsh conditions while avoiding
over-engineering for unlikely scenarios. Assessments such as Serviceability
Limit State (SLS) involve evaluating weather events, including estimating loads
not expected to be exceeded more than a specified number of times (e.g., 100)
throughout the structure's design lifetime. Although physics-based simulations
provide robust and detailed insights, they are computationally expensive,
making it challenging to generate statistically valid representations of a wide
range of weather conditions.
  To address these challenges, we propose an approach using Gaussian Process
(GP) surrogate models trained on a limited set of simulation outputs to
directly generate the structural response distribution. We apply this method to
an SLS assessment for estimating the order statistics \(Y_{100}\), representing
the 100th highest response, of a structure exposed to 25 years of historical
weather observations. Our results indicate that the GP surrogate models provide
comparable results to full simulations but at a fraction of the computational
cost.",http://arxiv.org/pdf/2503.01242v1,,False
Tera-MIND: Tera-scale mouse brain simulation via spatial mRNA-guided diffusion,03/03/2025,"Jiqing Wu, Ingrid Berg, Yawei Li, Ender Konukoglu, Viktor H. Koelzer","Holistic 3D modeling of molecularly defined brain structures is crucial for
understanding complex brain functions. Emerging tissue profiling technologies
enable the construction of a comprehensive atlas of the mammalian brain with
sub-cellular resolution and spatially resolved gene expression data. However,
such tera-scale volumetric datasets present significant computational
challenges in understanding complex brain functions within their native 3D
spatial context. Here, we propose the novel generative approach
$\textbf{Tera-MIND}$, which can simulate $\textbf{Tera}$-scale $\textbf{M}$ouse
bra$\textbf{IN}$s in 3D using a patch-based and boundary-aware
$\textbf{D}$iffusion model. Taking spatial transcriptomic data as the
conditional input, we generate virtual mouse brains with comprehensive cellular
morphological detail at teravoxel scale. Through the lens of 3D $gene$-$gene$
self-attention, we identify spatial molecular interactions for key
transcriptomic pathways in the murine brain, exemplified by glutamatergic and
dopaminergic neuronal systems. Importantly, these $in$-$silico$ biological
findings are consistent and reproducible across three tera-scale virtual mouse
brains. Therefore, Tera-MIND showcases a promising path toward efficient and
generative simulations of whole organ systems for biomedical research. Project
website: https://musikisomorphie.github.io/Tera-MIND.html",http://arxiv.org/pdf/2503.01220v2,,False
Architectural and Inferential Inductive Biases For Exchangeable Sequence Modeling,03/03/2025,"Daksh Mittal, Ang Li, Tzu-Ching Yen, Daniel Guetta, Hongseok Namkoong","Autoregressive models have emerged as a powerful framework for modeling
exchangeable sequences - i.i.d. observations when conditioned on some latent
factor - enabling direct modeling of uncertainty from missing data (rather than
a latent). Motivated by the critical role posterior inference plays as a
subroutine in decision-making (e.g., active learning, bandits), we study the
inferential and architectural inductive biases that are most effective for
exchangeable sequence modeling. For the inference stage, we highlight a
fundamental limitation of the prevalent single-step generation approach:
inability to distinguish between epistemic and aleatoric uncertainty. Instead,
a long line of works in Bayesian statistics advocates for multi-step
autoregressive generation; we demonstrate this ""correct approach"" enables
superior uncertainty quantification that translates into better performance on
downstream decision-making tasks. This naturally leads to the next question:
which architectures are best suited for multi-step inference? We identify a
subtle yet important gap between recently proposed Transformer architectures
for exchangeable sequences (Muller et al., 2022; Nguyen & Grover, 2022; Ye &
Namkoong, 2024), and prove that they in fact cannot guarantee exchangeability
despite introducing significant computational overhead. We illustrate our
findings using controlled synthetic settings, demonstrating how custom
architectures can significantly underperform standard causal masks,
underscoring the need for new architectural innovations.",http://arxiv.org/pdf/2503.01215v1,,False
Unify and Anchor: A Context-Aware Transformer for Cross-Domain Time Series Forecasting,03/03/2025,"Xiaobin Hong, Jiawen Zhang, Wenzhong Li, Sanglu Lu, Jia Li","The rise of foundation models has revolutionized natural language processing
and computer vision, yet their best practices to time series forecasting
remains underexplored. Existing time series foundation models often adopt
methodologies from these fields without addressing the unique characteristics
of time series data. In this paper, we identify two key challenges in
cross-domain time series forecasting: the complexity of temporal patterns and
semantic misalignment. To tackle these issues, we propose the ``Unify and
Anchor"" transfer paradigm, which disentangles frequency components for a
unified perspective and incorporates external context as domain anchors for
guided adaptation. Based on this framework, we introduce ContexTST, a
Transformer-based model that employs a time series coordinator for structured
representation and the Transformer blocks with a context-informed
mixture-of-experts mechanism for effective cross-domain generalization.
Extensive experiments demonstrate that ContexTST advances state-of-the-art
forecasting performance while achieving strong zero-shot transferability across
diverse domains.",http://arxiv.org/pdf/2503.01157v1,,False
"A General Neural Network Potential for Energetic Materials with C, H, N, and O elements",03/03/2025,"Mingjie Wen, Jiahe Han, Wenjuan Li, Xiaoya Chang, Qingzhao Chu, Dongping Chen","The discovery and optimization of high-energy materials (HEMs) are
constrained by the prohibitive computational expense and prolonged development
cycles inherent in conventional approaches. In this work, we develop a general
neural network potential (NNP) that efficiently predicts the structural,
mechanical, and decomposition properties of HEMs composed of C, H, N, and O.
Our framework leverages pre-trained NNP models, fine-tuned using transfer
learning on energy and force data derived from density functional theory (DFT)
calculations. This strategy enables rapid adaptation across 20 different HEM
systems while maintaining DFT-level accuracy, significantly reducing
computational costs. A key aspect of this work is the ability of NNP model to
capture the chemical activity space of HEMs, accurately describe the key atomic
interactions and reaction mechanisms during thermal decomposition. The general
NNP model has been applied in molecular dynamics (MD) simulations and validated
with experimental data for various HEM structures. Results show that the NNP
model accurately predicts the structural, mechanical, and decomposition
properties of HEMs by effectively describing their chemical activity space.
Compared to traditional force fields, it offers superior DFT-level accuracy and
generalization across both microscopic and macroscopic properties, reducing the
computational and experimental costs. This work provides an efficient strategy
for the design and development of HEMs and proposes a promising framework for
integrating DFT, machine learning, and experimental methods in materials
research. (To facilitate further research and practical applications, we
open-source our NNP model on GitHub:
https://github.com/MingjieWen/General-NNP-model-for-C-H-N-O-Energetic-Materials.)",http://arxiv.org/pdf/2503.01932v1,,False
Large AI Model for Delay-Doppler Domain Channel Prediction in 6G OTFS-Based Vehicular Networks,03/03/2025,"Jianzhe Xue, Dongcheng Yuan, Zhanxi Ma, Tiankai Jiang, Yu Sun, Haibo Zhou, Xuemin Shen","Channel prediction is crucial for high-mobility vehicular networks, as it
enables the anticipation of future channel conditions and the proactive
adjustment of communication strategies. However, achieving accurate vehicular
channel prediction is challenging due to significant Doppler effects and rapid
channel variations resulting from high-speed vehicle movement and complex
propagation environments. In this paper, we propose a novel delay-Doppler (DD)
domain channel prediction framework tailored for high-mobility vehicular
networks. By transforming the channel representation into the DD domain, we
obtain an intuitive, sparse, and stable depiction that closely aligns with the
underlying physical propagation processes, effectively reducing the complex
vehicular channel to a set of time-series parameters with enhanced
predictability. Furthermore, we leverage the large artificial intelligence (AI)
model to predict these DD-domain time-series parameters, capitalizing on their
advanced ability to model temporal correlations. The zero-shot capability of
the pre-trained large AI model facilitates accurate channel predictions without
requiring task-specific training, while subsequent fine-tuning on specific
vehicular channel data further improves prediction accuracy. Extensive
simulation results demonstrate the effectiveness of our DD-domain channel
prediction framework and the superior accuracy of the large AI model in
predicting time-series channel parameters, thereby highlighting the potential
of our approach for robust vehicular communication systems.",http://arxiv.org/pdf/2503.01116v1,,False
Road Boundary Detection Using 4D mmWave Radar for Autonomous Driving,03/03/2025,"Yuyan Wu, Hae Young Noh","Detecting road boundaries, the static physical edges of the available driving
area, is important for safe navigation and effective path planning in
autonomous driving and advanced driver-assistance systems (ADAS).
Traditionally, road boundary detection in autonomous driving relies on cameras
and LiDAR. However, they are vulnerable to poor lighting conditions, such as
nighttime and direct sunlight glare, or prohibitively expensive for low-end
vehicles. To this end, this paper introduces 4DRadarRBD, the first road
boundary detection method based on 4D mmWave radar which is cost-effective and
robust in complex driving scenarios. The main idea is that road boundaries
(e.g., fences, bushes, roadblocks), reflect millimeter waves, thus generating
point cloud data for the radar. To overcome the challenge that the 4D mmWave
radar point clouds contain many noisy points, we initially reduce noisy points
via physical constraints for road boundaries and then segment the road boundary
points from the noisy points by incorporating a distance-based loss which
penalizes for falsely detecting the points far away from the actual road
boundaries. In addition, we capture the temporal dynamics of point cloud
sequences by utilizing each point's deviation from the vehicle
motion-compensated road boundary detection result obtained from the previous
frame, along with the spatial distribution of the point cloud for point-wise
road boundary segmentation. We evaluated 4DRadarRBD through real-world driving
tests and achieved a road boundary point segmentation accuracy of 93$\%$, with
a median distance error of up to 0.023 m and an error reduction of 92.6$\%$
compared to the baseline model.",http://arxiv.org/pdf/2503.01930v1,,False
