Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills,09/05/2025,"Niladri Shekhar Dutt, Duygu Ceylan, Niloy J. Mitra","Retouching is an essential task in post-manipulation of raw photographs.
Generative editing, guided by text or strokes, provides a new tool accessible
to users but can easily change the identity of the original objects in
unacceptable and unpredictable ways. In contrast, although traditional
procedural edits, as commonly supported by photoediting tools (e.g., Gimp,
Lightroom), are conservative, they are still preferred by professionals.
Unfortunately, professional quality retouching involves many individual
procedural editing operations that is challenging to plan for most novices. In
this paper, we ask if a multimodal large language model (MLLM) can be taught to
critique raw photographs, suggest suitable remedies, and finally realize them
with a given set of pre-authored procedural image operations. We demonstrate
that MLLMs can be first made aware of the underlying image processing
operations, by training them to solve specially designed visual puzzles.
Subsequently, such an operation-aware MLLM can both plan and propose edit
sequences. To facilitate training, given a set of expert-edited photos, we
synthesize a reasoning dataset by procedurally manipulating the expert edits
and then grounding a pretrained LLM on the visual adjustments, to synthesize
reasoning for finetuning. The proposed retouching operations are, by
construction, understandable by the users, preserve object details and
resolution, and can be optionally overridden. We evaluate our setup on a
variety of test examples and show advantages, in terms of explainability and
identity preservation, over existing generative and other procedural
alternatives. Code, data, models, and supplementary results can be found via
our project website at https://monetgpt.github.io.",http://arxiv.org/pdf/2505.06176v1,10.1145/3730926,False
Universal Approximation Theorem for Deep Q-Learning via FBSDE System,09/05/2025,Qian Qi,"The approximation capabilities of Deep Q-Networks (DQNs) are commonly
justified by general Universal Approximation Theorems (UATs) that do not
leverage the intrinsic structural properties of the optimal Q-function, the
solution to a Bellman equation. This paper establishes a UAT for a class of
DQNs whose architecture is designed to emulate the iterative refinement process
inherent in Bellman updates. A central element of our analysis is the
propagation of regularity: while the transformation induced by a single Bellman
operator application exhibits regularity, for which Backward Stochastic
Differential Equations (BSDEs) theory provides analytical tools, the uniform
regularity of the entire sequence of value iteration iterates--specifically,
their uniform Lipschitz continuity on compact domains under standard Lipschitz
assumptions on the problem data--is derived from finite-horizon dynamic
programming principles. We demonstrate that layers of a deep residual network,
conceived as neural operators acting on function spaces, can approximate the
action of the Bellman operator. The resulting approximation theorem is thus
intrinsically linked to the control problem's structure, offering a proof
technique wherein network depth directly corresponds to iterations of value
function refinement, accompanied by controlled error propagation. This
perspective reveals a dynamic systems view of the network's operation on a
space of value functions.",http://arxiv.org/pdf/2505.06023v1,,False
Examining the Source of Defects from a Mechanical Perspective for 3D Anomaly Detection,09/05/2025,"Hanzhe Liang, Aoran Wang, Jie Zhou, Xin Jin, Can Gao, Jinbao Wang","In this paper, we go beyond identifying anomalies only in structural terms
and think about better anomaly detection motivated by anomaly causes. Most
anomalies are regarded as the result of unpredictable defective forces from
internal and external sources, and their opposite forces are sought to correct
the anomalies. We introduced a Mechanics Complementary framework for 3D anomaly
detection (MC4AD) to generate internal and external Corrective forces for each
point. A Diverse Anomaly-Generation (DA-Gen) module is first proposed to
simulate various anomalies. Then, we present a Corrective Force Prediction
Network (CFP-Net) with complementary representations for point-level
representation to simulate the different contributions of internal and external
corrective forces. A combined loss was proposed, including a new symmetric loss
and an overall loss, to constrain the corrective forces properly. As a
highlight, we consider 3D anomaly detection in industry more comprehensively,
creating a hierarchical quality control strategy based on a three-way decision
and contributing a dataset named Anomaly-IntraVariance with intraclass variance
to evaluate the model. On the proposed and existing five datasets, we obtained
nine state-of-the-art performers with the minimum parameters and the fastest
inference speed. The source is available at
https://github.com/hzzzzzhappy/MC4AD",http://arxiv.org/pdf/2505.05901v1,,False
A novel Neural-ODE model for the state of health estimation of lithium-ion battery using charging curve,09/05/2025,"Yiming Li, Man He, Jiapeng Liu","The state of health (SOH) of lithium-ion batteries (LIBs) is crucial for
ensuring the safe and reliable operation of electric vehicles. Nevertheless,
the prevailing SOH estimation methods often have limited generalizability. This
paper introduces a data-driven approach for estimating the SOH of LIBs, which
is designed to improve generalization. We construct a hybrid model named ACLA,
which integrates the attention mechanism, convolutional neural network (CNN),
and long short-term memory network (LSTM) into the augmented neural ordinary
differential equation (ANODE) framework. This model employs normalized charging
time corresponding to specific voltages in the constant current charging phase
as input and outputs the SOH as well as remaining useful of life. The model is
trained on NASA and Oxford datasets and validated on the TJU and HUST datasets.
Compared to the benchmark models NODE and ANODE, ACLA exhibits higher accuracy
with root mean square errors (RMSE) for SOH estimation as low as 1.01% and
2.24% on the TJU and HUST datasets, respectively.",http://arxiv.org/pdf/2505.05803v1,,False
What Is Next for LLMs? Next-Generation AI Computing Hardware Using Photonic Chips,09/05/2025,"Renjie Li, Wenjie Wei, Qi Xin, Xiaoli Liu, Sixuan Mao, Erik Ma, Zijian Chen, Malu Zhang, Haizhou Li, Zhaoyu Zhang","Large language models (LLMs) are rapidly pushing the limits of contemporary
computing hardware. For example, training GPT-3 has been estimated to consume
around 1300 MWh of electricity, and projections suggest future models may
require city-scale (gigawatt) power budgets. These demands motivate exploration
of computing paradigms beyond conventional von Neumann architectures. This
review surveys emerging photonic hardware optimized for next-generation
generative AI computing. We discuss integrated photonic neural network
architectures (e.g., Mach-Zehnder interferometer meshes, lasers,
wavelength-multiplexed microring resonators) that perform ultrafast matrix
operations. We also examine promising alternative neuromorphic devices,
including spiking neural network circuits and hybrid spintronic-photonic
synapses, which combine memory and processing. The integration of
two-dimensional materials (graphene, TMDCs) into silicon photonic platforms is
reviewed for tunable modulators and on-chip synaptic elements.
Transformer-based LLM architectures (self-attention and feed-forward layers)
are analyzed in this context, identifying strategies and challenges for mapping
dynamic matrix multiplications onto these novel hardware substrates. We then
dissect the mechanisms of mainstream LLMs, such as ChatGPT, DeepSeek, and
LLaMA, highlighting their architectural similarities and differences. We
synthesize state-of-the-art components, algorithms, and integration methods,
highlighting key advances and open issues in scaling such systems to mega-sized
LLM models. We find that photonic computing systems could potentially surpass
electronic processors by orders of magnitude in throughput and energy
efficiency, but require breakthroughs in memory, especially for long-context
windows and long token sequences, and in storage of ultra-large datasets.",http://arxiv.org/pdf/2505.05794v1,,False
Rethinking Graph Out-Of-Distribution Generalization: A Learnable Random Walk Perspective,09/05/2025,"Henan Sun, Xunkai Li, Lei Zhu, Junyi Han, Guang Zeng, Ronghua Li, Guoren Wang","Out-Of-Distribution (OOD) generalization has gained increasing attentions for
machine learning on graphs, as graph neural networks (GNNs) often exhibit
performance degradation under distribution shifts. Existing graph OOD methods
tend to follow the basic ideas of invariant risk minimization and structural
causal models, interpreting the invariant knowledge across datasets under
various distribution shifts as graph topology or graph spectrum. However, these
interpretations may be inconsistent with real-world scenarios, as neither
invariant topology nor spectrum is assured. In this paper, we advocate the
learnable random walk (LRW) perspective as the instantiation of invariant
knowledge, and propose LRW-OOD to realize graph OOD generalization learning.
Instead of employing fixed probability transition matrix (i.e.,
degree-normalized adjacency matrix), we parameterize the transition matrix with
an LRW-sampler and a path encoder. Furthermore, we propose the kernel density
estimation (KDE)-based mutual information (MI) loss to generate random walk
sequences that adhere to OOD principles. Extensive experiment demonstrates that
our model can effectively enhance graph OOD generalization under various types
of distribution shifts and yield a significant accuracy improvement of 3.87%
over state-of-the-art graph OOD generalization baselines.",http://arxiv.org/pdf/2505.05785v1,,False
Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions,09/05/2025,"Dhruvesh Patel, Aishwarya Sahoo, Avinash Amballa, Tahira Naseem, Tim G. J. Rudner, Andrew McCallum","Autoregressive models (ARMs), which predict subsequent tokens one-by-one
``from left to right,'' have achieved significant success across a wide range
of sequence generation tasks. However, they struggle to accurately represent
sequences that require satisfying sophisticated constraints or whose sequential
dependencies are better addressed by out-of-order generation. Masked Diffusion
Models (MDMs) address some of these limitations, but the process of unmasking
multiple tokens simultaneously in MDMs can introduce incoherences, and MDMs
cannot handle arbitrary infilling constraints when the number of tokens to be
filled in is not known in advance. In this work, we introduce Insertion
Language Models (ILMs), which learn to insert tokens at arbitrary positions in
a sequence -- that is, they select jointly both the position and the vocabulary
element to be inserted. By inserting tokens one at a time, ILMs can represent
strong dependencies between tokens, and their ability to generate sequences in
arbitrary order allows them to accurately model sequences where token
dependencies do not follow a left-to-right sequential structure. To train ILMs,
we propose a tailored network parameterization and use a simple denoising
objective. Our empirical evaluation demonstrates that ILMs outperform both ARMs
and MDMs on common planning tasks. Furthermore, we show that ILMs outperform
MDMs and perform on par with ARMs in an unconditional text generation task
while offering greater flexibility than MDMs in arbitrary-length text
infilling.",http://arxiv.org/pdf/2505.05755v1,,False
Crowding Out The Noise: Algorithmic Collective Action Under Differential Privacy,09/05/2025,"Rushabh Solanki, Meghana Bhange, Ulrich AÃ¯vodji, Elliot Creager","The integration of AI into daily life has generated considerable attention
and excitement, while also raising concerns about automating algorithmic harms
and re-entrenching existing social inequities. While the responsible deployment
of trustworthy AI systems is a worthy goal, there are many possible ways to
realize it, from policy and regulation to improved algorithm design and
evaluation. In fact, since AI trains on social data, there is even a
possibility for everyday users, citizens, or workers to directly steer its
behavior through Algorithmic Collective Action, by deliberately modifying the
data they share with a platform to drive its learning process in their favor.
This paper considers how these grassroots efforts to influence AI interact with
methods already used by AI firms and governments to improve model
trustworthiness. In particular, we focus on the setting where the AI firm
deploys a differentially private model, motivated by the growing regulatory
focus on privacy and data protection. We investigate how the use of
Differentially Private Stochastic Gradient Descent (DPSGD) affects the
collective's ability to influence the learning process. Our findings show that
while differential privacy contributes to the protection of individual data, it
introduces challenges for effective algorithmic collective action. We
characterize lower bounds on the success of algorithmic collective action under
differential privacy as a function of the collective's size and the firm's
privacy parameters, and verify these trends experimentally by simulating
collective action during the training of deep neural network classifiers across
several datasets.",http://arxiv.org/pdf/2505.05707v1,,False
