Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation,26/09/2024,"Xin Li, Siyuan Huang, Qiaojun Yu, Zhengkai Jiang, Ce Hao, Yimeng Zhu, Hongsheng Li, Peng Gao, Cewu Lu","Automating garment manipulation poses a significant challenge for assistive
robotics due to the diverse and deformable nature of garments. Traditional
approaches typically require separate models for each garment type, which
limits scalability and adaptability. In contrast, this paper presents a unified
approach using vision-language models (VLMs) to improve keypoint prediction
across various garment categories. By interpreting both visual and semantic
information, our model enables robots to manage different garment states with a
single model. We created a large-scale synthetic dataset using advanced
simulation techniques, allowing scalable training without extensive real-world
data. Experimental results indicate that the VLM-based method significantly
enhances keypoint detection accuracy and task success rates, providing a more
flexible and general solution for robotic garment manipulation. In addition,
this research also underscores the potential of VLMs to unify various garment
manipulation tasks within a single framework, paving the way for broader
applications in home automation and assistive robotics for future.",http://arxiv.org/pdf/2409.18082v1,,False
HARMONIC: A Framework for Explanatory Cognitive Robots,26/09/2024,"Sanjay Oruganti, Sergei Nirenburg, Marjorie McShane, Jesse English, Michael K. Roberts, Christian Arndt","We present HARMONIC, a framework for implementing cognitive robots that
transforms general-purpose robots into trusted teammates capable of complex
decision-making, natural communication and human-level explanation. The
framework supports interoperability between a strategic (cognitive) layer for
high-level decision-making and a tactical (robot) layer for low-level control
and execution. We describe the core features of the framework and our initial
implementation, in which HARMONIC was deployed on a simulated UGV and drone
involved in a multi-robot search and retrieval task.",http://arxiv.org/pdf/2409.18037v1,,False
Joint Localization and Planning using Diffusion,26/09/2024,"L. Lao Beyer, S. Karaman","Diffusion models have been successfully applied to robotics problems such as
manipulation and vehicle path planning. In this work, we explore their
application to end-to-end navigation -- including both perception and planning
-- by considering the problem of jointly performing global localization and
path planning in known but arbitrary 2D environments. In particular, we
introduce a diffusion model which produces collision-free paths in a global
reference frame given an egocentric LIDAR scan, an arbitrary map, and a desired
goal position. To this end, we implement diffusion in the space of paths in
SE(2), and describe how to condition the denoising process on both obstacles
and sensor observations. In our evaluation, we show that the proposed
conditioning techniques enable generalization to realistic maps of considerably
different appearance than the training environment, demonstrate our model's
ability to accurately describe ambiguous solutions, and run extensive
simulation experiments showcasing our model's use as a real-time, end-to-end
localization and planning stack.",http://arxiv.org/pdf/2409.17995v1,,False
Adaptive Stream Processing on Edge Devices through Active Inference,26/09/2024,"Boris Sedlak, Victor Casamayor Pujol, Andrea Morichetta, Praveen Kumar Donta, Schahram Dustdar","The current scenario of IoT is witnessing a constant increase on the volume
of data, which is generated in constant stream, calling for novel architectural
and logical solutions for processing it. Moving the data handling towards the
edge of the computing spectrum guarantees better distribution of load and, in
principle, lower latency and better privacy. However, managing such a structure
is complex, especially when requirements, also referred to Service Level
Objectives (SLOs), specified by applications' owners and infrastructure
managers need to be ensured. Despite the rich number of proposals of Machine
Learning (ML) based management solutions, researchers and practitioners yet
struggle to guarantee long-term prediction and control, and accurate
troubleshooting. Therefore, we present a novel ML paradigm based on Active
Inference (AIF) -- a concept from neuroscience that describes how the brain
constantly predicts and evaluates sensory information to decrease long-term
surprise. We implement it and evaluate it in a heterogeneous real stream
processing use case, where an AIF-based agent continuously optimizes the
fulfillment of three SLOs for three autonomous driving services running on
multiple devices. The agent used causal knowledge to gradually develop an
understanding of how its actions are related to requirements fulfillment, and
which configurations to favor. Through this approach, our agent requires up to
thirty iterations to converge to the optimal solution, showing the capability
of offering accurate results in a short amount of time. Furthermore, thanks to
AIF and its causal structures, our method guarantees full transparency on the
decision making, making the interpretation of the results and the
troubleshooting effortless.",http://arxiv.org/pdf/2409.17937v1,,False
Sample compression unleashed : New generalization bounds for real valued losses,26/09/2024,"Mathieu Bazinet, Valentina Zantedeschi, Pascal Germain","The sample compression theory provides generalization guarantees for
predictors that can be fully defined using a subset of the training dataset and
a (short) message string, generally defined as a binary sequence. Previous
works provided generalization bounds for the zero-one loss, which is
restrictive, notably when applied to deep learning approaches. In this paper,
we present a general framework for deriving new sample compression bounds that
hold for real-valued losses. We empirically demonstrate the tightness of the
bounds and their versatility by evaluating them on different types of models,
e.g., neural networks and decision forests, trained with the Pick-To-Learn
(P2L) meta-algorithm, which transforms the training method of any
machine-learning predictor to yield sample-compressed predictors. In contrast
to existing P2L bounds, ours are valid in the non-consistent case.",http://arxiv.org/pdf/2409.17932v1,,False
Model-Free versus Model-Based Reinforcement Learning for Fixed-Wing UAV Attitude Control Under Varying Wind Conditions,26/09/2024,"David Olivares, Pierre Fournier, Pavan Vasishta, Julien Marzat","This paper evaluates and compares the performance of model-free and
model-based reinforcement learning for the attitude control of fixed-wing
unmanned aerial vehicles using PID as a reference point. The comparison focuses
on their ability to handle varying flight dynamics and wind disturbances in a
simulated environment. Our results show that the Temporal Difference Model
Predictive Control agent outperforms both the PID controller and other
model-free reinforcement learning methods in terms of tracking accuracy and
robustness over different reference difficulties, particularly in nonlinear
flight regimes. Furthermore, we introduce actuation fluctuation as a key metric
to assess energy efficiency and actuator wear, and we test two different
approaches from the literature: action variation penalty and conditioning for
action policy smoothness. We also evaluate all control methods when subject to
stochastic turbulence and gusts separately, so as to measure their effects on
tracking performance, observe their limitations and outline their implications
on the Markov decision process formalism.",http://arxiv.org/pdf/2409.17896v1,,False
A multi-source data power load forecasting method using attention mechanism-based parallel cnn-gru,26/09/2024,"Chao Min, Yijia Wang, Bo Zhang, Xin Ma, Junyi Cui","Accurate power load forecasting is crucial for improving energy efficiency
and ensuring power supply quality. Considering the power load forecasting
problem involves not only dynamic factors like historical load variations but
also static factors such as climate conditions that remain constant over
specific periods. From the model-agnostic perspective, this paper proposes a
parallel structure network to extract important information from both dynamic
and static data. Firstly, based on complexity learning theory, it is
demonstrated that models integrated through parallel structures exhibit
superior generalization abilities compared to individual base learners.
Additionally, the higher the independence between base learners, the stronger
the generalization ability of the parallel structure model. This suggests that
the structure of machine learning models inherently contains significant
information. Building on this theoretical foundation, a parallel convolutional
neural network (CNN)-gate recurrent unit (GRU) attention model (PCGA) is
employed to address the power load forecasting issue, aiming to effectively
integrate the influences of dynamic and static features. The CNN module is
responsible for capturing spatial characteristics from static data, while the
GRU module captures long-term dependencies in dynamic time series data. The
attention layer is designed to focus on key information from the
spatial-temporal features extracted by the parallel CNN-GRU. To substantiate
the advantages of the parallel structure model in extracting and integrating
multi-source information, a series of experiments are conducted.",http://arxiv.org/pdf/2409.17889v1,,False
AMARO: All Heavy-Atom Transferable Neural Network Potentials of Protein Thermodynamics,26/09/2024,"Antonio Mirarchi, Raul P. Pelaez, Guillem Simeon, Gianni De Fabritiis","All-atom molecular simulations offer detailed insights into macromolecular
phenomena, but their substantial computational cost hinders the exploration of
complex biological processes. We introduce Advanced Machine-learning Atomic
Representation Omni-force-field (AMARO), a new neural network potential (NNP)
that combines an O(3)-equivariant message-passing neural network architecture,
TensorNet, with a coarse-graining map that excludes hydrogen atoms. AMARO
demonstrates the feasibility of training coarser NNP, without prior energy
terms, to run stable protein dynamics with scalability and generalization
capabilities.",http://arxiv.org/pdf/2409.17852v1,,False
Machine Learning-based vs Deep Learning-based Anomaly Detection in Multivariate Time Series for Spacecraft Attitude Sensors,26/09/2024,"R. Gallon, F. Schiemenz, A. Krstova, A. Menicucci, E. Gill","In the framework of Failure Detection, Isolation and Recovery (FDIR) on
spacecraft, new AI-based approaches are emerging in the state of the art to
overcome the limitations commonly imposed by traditional threshold checking.
  The present research aims at characterizing two different approaches to the
problem of stuck values detection in multivariate time series coming from
spacecraft attitude sensors. The analysis reveals the performance differences
in the two approaches, while commenting on their interpretability and
generalization to different scenarios.",http://arxiv.org/pdf/2409.17841v1,,False
Ordinary Differential Equations for Enhanced 12-Lead ECG Generation,26/09/2024,"Yakir Yehuda, Kira Radinsky","In the realm of artificial intelligence, the generation of realistic training
data for supervised learning tasks presents a significant challenge. This is
particularly true in the synthesis of electrocardiograms (ECGs), where the
objective is to develop a synthetic 12-lead ECG model. The primary complexity
of this task stems from accurately modeling the intricate biological and
physiological interactions among different ECG leads. Although mathematical
process simulators have shed light on these dynamics, effectively incorporating
this understanding into generative models is not straightforward. In this work,
we introduce an innovative method that employs ordinary differential equations
(ODEs) to enhance the fidelity of generating 12-lead ECG data. This approach
integrates a system of ODEs that represent cardiac dynamics directly into the
generative model's optimization process, allowing for the production of
biologically plausible ECG training data that authentically reflects real-world
variability and inter-lead dependencies. We conducted an empirical analysis of
thousands of ECGs and found that incorporating cardiac simulation insights into
the data generation process significantly improves the accuracy of heart
abnormality classifiers trained on this synthetic 12-lead ECG data.",http://arxiv.org/pdf/2409.17833v1,,False
Generative Modeling of Molecular Dynamics Trajectories,26/09/2024,"Bowen Jing, Hannes Stärk, Tommi Jaakkola, Bonnie Berger","Molecular dynamics (MD) is a powerful technique for studying microscopic
phenomena, but its computational cost has driven significant interest in the
development of deep learning-based surrogate models. We introduce generative
modeling of molecular trajectories as a paradigm for learning flexible
multi-task surrogate models of MD from data. By conditioning on appropriately
chosen frames of the trajectory, we show such generative models can be adapted
to diverse tasks such as forward simulation, transition path sampling, and
trajectory upsampling. By alternatively conditioning on part of the molecular
system and inpainting the rest, we also demonstrate the first steps towards
dynamics-conditioned molecular design. We validate the full set of these
capabilities on tetrapeptide simulations and show that our model can produce
reasonable ensembles of protein monomers. Altogether, our work illustrates how
generative modeling can unlock value from MD data towards diverse downstream
tasks that are not straightforward to address with existing methods or even MD
itself. Code is available at https://github.com/bjing2016/mdgen.",http://arxiv.org/pdf/2409.17808v1,,False
Continual learning with task specialist,26/09/2024,"Indu Solomon, Aye Phyu Phyu Aung, Uttam Kumar, Senthilnath Jayavelu","Continual learning (CL) adapt the deep learning scenarios with timely updated
datasets. However, existing CL models suffer from the catastrophic forgetting
issue, where new knowledge replaces past learning. In this paper, we propose
Continual Learning with Task Specialists (CLTS) to address the issues of
catastrophic forgetting and limited labelled data in real-world datasets by
performing class incremental learning of the incoming stream of data. The model
consists of Task Specialists (T S) and Task Predictor (T P ) with pre-trained
Stable Diffusion (SD) module. Here, we introduce a new specialist to handle a
new task sequence and each T S has three blocks; i) a variational autoencoder
(V AE) to learn the task distribution in a low dimensional latent space, ii) a
K-Means block to perform data clustering and iii) Bootstrapping Language-Image
Pre-training (BLIP ) model to generate a small batch of captions from the input
data. These captions are fed as input to the pre-trained stable diffusion model
(SD) for the generation of task samples. The proposed model does not store any
task samples for replay, instead uses generated samples from SD to train the T
P module. A comparison study with four SOTA models conducted on three
real-world datasets shows that the proposed model outperforms all the selected
baselines",http://arxiv.org/pdf/2409.17806v1,,False
Autoregressive Generation Strategies for Top-K Sequential Recommendations,26/09/2024,"Anna Volodkevich, Danil Gusak, Anton Klenitskiy, Alexey Vasilev","The goal of modern sequential recommender systems is often formulated in
terms of next-item prediction. In this paper, we explore the applicability of
generative transformer-based models for the Top-K sequential recommendation
task, where the goal is to predict items a user is likely to interact with in
the ""near future"".
  We explore commonly used autoregressive generation strategies, including
greedy decoding, beam search, and temperature sampling, to evaluate their
performance for the Top-K sequential recommendation task. In addition, we
propose novel Reciprocal Rank Aggregation (RRA) and Relevance Aggregation (RA)
generation strategies based on multi-sequence generation with temperature
sampling and subsequent aggregation.
  Experiments on diverse datasets give valuable insights regarding commonly
used strategies' applicability and show that suggested approaches improve
performance on longer time horizons compared to widely-used Top-K prediction
approach and single-sequence autoregressive generation strategies.",http://arxiv.org/pdf/2409.17730v1,,False
Episodic Memory Verbalization using Hierarchical Representations of Life-Long Robot Experience,26/09/2024,"Leonard Bärmann, Chad DeChant, Joana Plewnia, Fabian Peller-Konrad, Daniel Bauer, Tamim Asfour, Alex Waibel","Verbalization of robot experience, i.e., summarization of and question
answering about a robot's past, is a crucial ability for improving human-robot
interaction. Previous works applied rule-based systems or fine-tuned deep
models to verbalize short (several-minute-long) streams of episodic data,
limiting generalization and transferability. In our work, we apply large
pretrained models to tackle this task with zero or few examples, and
specifically focus on verbalizing life-long experiences. For this, we derive a
tree-like data structure from episodic memory (EM), with lower levels
representing raw perception and proprioception data, and higher levels
abstracting events to natural language concepts. Given such a hierarchical
representation built from the experience stream, we apply a large language
model as an agent to interactively search the EM given a user's query,
dynamically expanding (initially collapsed) tree nodes to find the relevant
information. The approach keeps computational costs low even when scaling to
months of robot experience data. We evaluate our method on simulated household
robot data, human egocentric videos, and real-world robot recordings,
demonstrating its flexibility and scalability.",http://arxiv.org/pdf/2409.17702v1,,False
MIO: A Foundation Model on Multimodal Tokens,26/09/2024,"Zekun Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning Shi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, Wenhao Huang","In this paper, we introduce MIO, a novel foundation model built on multimodal
tokens, capable of understanding and generating speech, text, images, and
videos in an end-to-end, autoregressive manner. While the emergence of large
language models (LLMs) and multimodal large language models (MM-LLMs) propels
advancements in artificial general intelligence through their versatile
capabilities, they still lack true any-to-any understanding and generation.
Recently, the release of GPT-4o has showcased the remarkable potential of
any-to-any LLMs for complex real-world tasks, enabling omnidirectional input
and output across images, speech, and text. However, it is closed-source and
does not support the generation of multimodal interleaved sequences. To address
this gap, we present MIO, which is trained on a mixture of discrete tokens
across four modalities using causal multimodal modeling. MIO undergoes a
four-stage training process: (1) alignment pre-training, (2) interleaved
pre-training, (3) speech-enhanced pre-training, and (4) comprehensive
supervised fine-tuning on diverse textual, visual, and speech tasks. Our
experimental results indicate that MIO exhibits competitive, and in some cases
superior, performance compared to previous dual-modal baselines, any-to-any
model baselines, and even modality-specific baselines. Moreover, MIO
demonstrates advanced capabilities inherent to its any-to-any feature, such as
interleaved video-text generation, chain-of-visual-thought reasoning, visual
guideline generation, instructional image editing, etc.",http://arxiv.org/pdf/2409.17692v1,,False
Graph Edit Distance with General Costs Using Neural Set Divergence,26/09/2024,"Eeshaan Jain, Indradyumna Roy, Saswat Meher, Soumen Chakrabarti, Abir De","Graph Edit Distance (GED) measures the (dis-)similarity between two given
graphs, in terms of the minimum-cost edit sequence that transforms one graph to
the other. However, the exact computation of GED is NP-Hard, which has recently
motivated the design of neural methods for GED estimation. However, they do not
explicitly account for edit operations with different costs. In response, we
propose GRAPHEDX, a neural GED estimator that can work with general costs
specified for the four edit operations, viz., edge deletion, edge addition,
node deletion and node addition. We first present GED as a quadratic assignment
problem (QAP) that incorporates these four costs. Then, we represent each graph
as a set of node and edge embeddings and use them to design a family of neural
set divergence surrogates. We replace the QAP terms corresponding to each
operation with their surrogates. Computing such neural set divergence require
aligning nodes and edges of the two graphs. We learn these alignments using a
Gumbel-Sinkhorn permutation generator, additionally ensuring that the node and
edge alignments are consistent with each other. Moreover, these alignments are
cognizant of both the presence and absence of edges between node-pairs.
Experiments on several datasets, under a variety of edit cost settings, show
that GRAPHEDX consistently outperforms state-of-the-art methods and heuristics
in terms of prediction error.",http://arxiv.org/pdf/2409.17687v1,,False
FactorSim: Generative Simulation via Factorized Representation,26/09/2024,"Fan-Yun Sun, S. I. Harini, Angela Yi, Yihan Zhou, Alex Zook, Jonathan Tremblay, Logan Cross, Jiajun Wu, Nick Haber","Generating simulations to train intelligent agents in game-playing and
robotics from natural language input, from user input or task documentation,
remains an open-ended challenge. Existing approaches focus on parts of this
challenge, such as generating reward functions or task hyperparameters. Unlike
previous work, we introduce FACTORSIM that generates full simulations in code
from language input that can be used to train agents. Exploiting the structural
modularity specific to coded simulations, we propose to use a factored
partially observable Markov decision process representation that allows us to
reduce context dependence during each step of the generation. For evaluation,
we introduce a generative simulation benchmark that assesses the generated
simulation code's accuracy and effectiveness in facilitating zero-shot
transfers in reinforcement learning settings. We show that FACTORSIM
outperforms existing methods in generating simulations regarding prompt
alignment (e.g., accuracy), zero-shot transfer abilities, and human evaluation.
We also demonstrate its effectiveness in generating robotic tasks.",http://arxiv.org/pdf/2409.17652v1,,False
Good Data Is All Imitation Learning Needs,26/09/2024,"Amir Samadi, Konstantinos Koufos, Kurt Debattista, Mehrdad Dianati","In this paper, we address the limitations of traditional teacher-student
models, imitation learning, and behaviour cloning in the context of
Autonomous/Automated Driving Systems (ADS), where these methods often struggle
with incomplete coverage of real-world scenarios. To enhance the robustness of
such models, we introduce the use of Counterfactual Explanations (CFEs) as a
novel data augmentation technique for end-to-end ADS. CFEs, by generating
training samples near decision boundaries through minimal input modifications,
lead to a more comprehensive representation of expert driver strategies,
particularly in safety-critical scenarios. This approach can therefore help
improve the model's ability to handle rare and challenging driving events, such
as anticipating darting out pedestrians, ultimately leading to safer and more
trustworthy decision-making for ADS. Our experiments in the CARLA simulator
demonstrate that CF-Driver outperforms the current state-of-the-art method,
achieving a higher driving score and lower infraction rates. Specifically,
CF-Driver attains a driving score of 84.2, surpassing the previous best model
by 15.02 percentage points. These results highlight the effectiveness of
incorporating CFEs in training end-to-end ADS. To foster further research, the
CF-Driver code is made publicly available.",http://arxiv.org/pdf/2409.17605v1,,False
Advancing Open-Set Domain Generalization Using Evidential Bi-Level Hardest Domain Scheduler,26/09/2024,"Kunyu Peng, Di Wen, Kailun Yang, Ao Luo, Yufan Chen, Jia Fu, M. Saquib Sarfraz, Alina Roitberg, Rainer Stiefelhagen","In Open-Set Domain Generalization (OSDG), the model is exposed to both new
variations of data appearance (domains) and open-set conditions, where both
known and novel categories are present at test time. The challenges of this
task arise from the dual need to generalize across diverse domains and
accurately quantify category novelty, which is critical for applications in
dynamic environments. Recently, meta-learning techniques have demonstrated
superior results in OSDG, effectively orchestrating the meta-train and -test
tasks by employing varied random categories and predefined domain partition
strategies. These approaches prioritize a well-designed training schedule over
traditional methods that focus primarily on data augmentation and the
enhancement of discriminative feature learning. The prevailing meta-learning
models in OSDG typically utilize a predefined sequential domain scheduler to
structure data partitions. However, a crucial aspect that remains inadequately
explored is the influence brought by strategies of domain schedulers during
training. In this paper, we observe that an adaptive domain scheduler benefits
more in OSDG compared with prefixed sequential and random domain schedulers. We
propose the Evidential Bi-Level Hardest Domain Scheduler (EBiL-HaDS) to achieve
an adaptive domain scheduler. This method strategically sequences domains by
assessing their reliabilities in utilizing a follower network, trained with
confidence scores learned in an evidential manner, regularized by max rebiasing
discrepancy, and optimized in a bi-level manner. The results show that our
method substantially improves OSDG performance and achieves more discriminative
embeddings for both the seen and unseen categories. The source code will be
available at https://github.com/KPeng9510/EBiL-HaDS.",http://arxiv.org/pdf/2409.17555v1,,False
Optimizing the Induced Correlation in Omnibus Joint Graph Embeddings,26/09/2024,"Konstantinos Pantazis, Michael Trosset, William N. Frost, Carey E. Priebe, Vince Lyzinski","Theoretical and empirical evidence suggests that joint graph embedding
algorithms induce correlation across the networks in the embedding space. In
the Omnibus joint graph embedding framework, previous results explicitly
delineated the dual effects of the algorithm-induced and model-inherent
correlations on the correlation across the embedded networks. Accounting for
and mitigating the algorithm-induced correlation is key to subsequent
inference, as sub-optimal Omnibus matrix constructions have been demonstrated
to lead to loss in inference fidelity. This work presents the first efforts to
automate the Omnibus construction in order to address two key questions in this
joint embedding framework: the correlation-to-OMNI problem and the flat
correlation problem. In the flat correlation problem, we seek to understand the
minimum algorithm-induced flat correlation (i.e., the same across all graph
pairs) produced by a generalized Omnibus embedding. Working in a subspace of
the fully general Omnibus matrices, we prove both a lower bound for this flat
correlation and that the classical Omnibus construction induces the maximal
flat correlation. In the correlation-to-OMNI problem, we present an algorithm
-- named corr2Omni -- that, from a given matrix of estimated pairwise graph
correlations, estimates the matrix of generalized Omnibus weights that induces
optimal correlation in the embedding space. Moreover, in both simulated and
real data settings, we demonstrate the increased effectiveness of our corr2Omni
algorithm versus the classical Omnibus construction.",http://arxiv.org/pdf/2409.17544v1,,False
From News to Forecast: Integrating Event Analysis in LLM-Based Time Series Forecasting with Reflection,26/09/2024,"Xinlei Wang, Maike Feng, Jing Qiu, Jinjin Gu, Junhua Zhao","This paper introduces a novel approach to enhance time series forecasting
using Large Language Models (LLMs) and Generative Agents. With language as a
medium, our method adaptively integrates various social events into forecasting
models, aligning news content with time series fluctuations for enriched
insights. Specifically, we utilize LLM-based agents to iteratively filter out
irrelevant news and employ human-like reasoning and reflection to evaluate
predictions. This enables our model to analyze complex events, such as
unexpected incidents and shifts in social behavior, and continuously refine the
selection logic of news and the robustness of the agent's output. By compiling
selected news with time series data, we fine-tune the LLaMa2 pre-trained model.
The results demonstrate significant improvements in forecasting accuracy and
suggest a potential paradigm shift in time series forecasting by effectively
harnessing unstructured news data.",http://arxiv.org/pdf/2409.17515v1,,False
"Comparing Unidirectional, Bidirectional, and Word2vec Models for Discovering Vulnerabilities in Compiled Lifted Code",26/09/2024,"Gary A. McCully, John D. Hastings, Shengjie Xu, Adam Fortier","Ransomware and other forms of malware cause significant financial and
operational damage to organizations by exploiting long-standing and often
difficult-to-detect software vulnerabilities. To detect vulnerabilities such as
buffer overflows in compiled code, this research investigates the application
of unidirectional transformer-based embeddings, specifically GPT-2. Using a
dataset of LLVM functions, we trained a GPT-2 model to generate embeddings,
which were subsequently used to build LSTM neural networks to differentiate
between vulnerable and non-vulnerable code. Our study reveals that embeddings
from the GPT-2 model significantly outperform those from bidirectional models
of BERT and RoBERTa, achieving an accuracy of 92.5% and an F1-score of 89.7%.
LSTM neural networks were developed with both frozen and unfrozen embedding
model layers. The model with the highest performance was achieved when the
embedding layers were unfrozen. Further, the research finds that, in exploring
the impact of different optimizers within this domain, the SGD optimizer
demonstrates superior performance over Adam. Overall, these findings reveal
important insights into the potential of unidirectional transformer-based
approaches in enhancing cybersecurity defenses.",http://arxiv.org/pdf/2409.17513v1,,False
CadVLM: Bridging Language and Vision in the Generation of Parametric CAD Sketches,26/09/2024,"Sifan Wu, Amir Khasahmadi, Mor Katz, Pradeep Kumar Jayaraman, Yewen Pu, Karl Willis, Bang Liu","Parametric Computer-Aided Design (CAD) is central to contemporary mechanical
design. However, it encounters challenges in achieving precise parametric
sketch modeling and lacks practical evaluation metrics suitable for mechanical
design. We harness the capabilities of pre-trained foundation models, renowned
for their successes in natural language processing and computer vision, to
develop generative models specifically for CAD. These models are adept at
understanding complex geometries and design reasoning, a crucial advancement in
CAD technology. In this paper, we propose CadVLM, an end-to-end vision language
model for CAD generation. Our approach involves adapting pre-trained foundation
models to manipulate engineering sketches effectively, integrating both sketch
primitive sequences and sketch images. Extensive experiments demonstrate
superior performance on multiple CAD sketch generation tasks such as CAD
autocompletion, CAD autoconstraint, and image conditional generation. To our
knowledge, this is the first instance of a multimodal Large Language Model
(LLM) being successfully applied to parametric CAD generation, representing a
pioneering step in the field of computer-aided mechanical design.",http://arxiv.org/pdf/2409.17457v1,,False
