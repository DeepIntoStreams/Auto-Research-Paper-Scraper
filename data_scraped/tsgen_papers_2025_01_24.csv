Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
Matrix Completion in Group Testing: Bounds and Simulations,23/01/2025,"Trung-Khang Tran, Thach V. Bui","The main goal of group testing is to identify a small number of defective
items in a large population of items. A test on a subset of items is positive
if the subset contains at least one defective item and negative otherwise. In
non-adaptive design, all tests can be tested simultaneously and represented by
a measurement matrix in which a row and a column represent a test and an item,
respectively. An entry in row $i$ and column $j$ is 1 if item $j$ belongs to
the test $i$ and is 0 otherwise. Given an unknown set of defective items, the
objective is to design a measurement matrix such that, by observing its
corresponding outcome vector, the defective items can be recovered efficiently.
The basic trait of this approach is that the measurement matrix has remained
unchanged throughout the course of generating the outcome vector and recovering
defective items. In this paper, we study the case in which some entries in the
measurement matrix are erased, called \emph{the missing measurement matrix},
before the recovery phase of the defective items, and our objective is to fully
recover the measurement matrix from the missing measurement matrix. In
particular, we show that some specific rows with erased entries provide
information aiding the recovery while others do not. Given measurement matrices
and erased entries follow the Bernoulli distribution, we show that before the
erasing event happens, sampling sufficient sets of defective items and their
corresponding outcome vectors can help us recover the measurement matrix from
the missing measurement matrix.",http://arxiv.org/pdf/2501.13780v1,,False
EventVL: Understand Event Streams via Multimodal Large Language Model,23/01/2025,"Pengteng Li, Yunfan Lu, Pinghao Song, Wuyang Li, Huizai Yao, Hui Xiong","The event-based Vision-Language Model (VLM) recently has made good progress
for practical vision tasks. However, most of these works just utilize CLIP for
focusing on traditional perception tasks, which obstruct model understanding
explicitly the sufficient semantics and context from event streams. To address
the deficiency, we propose EventVL, the first generative event-based MLLM
(Multimodal Large Language Model) framework for explicit semantic
understanding. Specifically, to bridge the data gap for connecting different
modalities semantics, we first annotate a large event-image/video-text dataset,
containing almost 1.4 million high-quality pairs of data, which enables
effective learning across various scenes, e.g., drive scene or human motion.
After that, we design Event Spatiotemporal Representation to fully explore the
comprehensive information by diversely aggregating and segmenting the event
stream. To further promote a compact semantic space, Dynamic Semantic Alignment
is introduced to improve and complete sparse semantic spaces of events.
Extensive experiments show that our EventVL can significantly surpass existing
MLLM baselines in event captioning and scene description generation tasks. We
hope our research could contribute to the development of the event vision
community.",http://arxiv.org/pdf/2501.13707v1,,False
GenTL: A General Transfer Learning Model for Building Thermal Dynamics,23/01/2025,"Fabian Raisch, Thomas Krug, Christoph Goebel, Benjamin Tischler","Transfer Learning (TL) is an emerging field in modeling building thermal
dynamics. This method reduces the data required for a data-driven model of a
target building by leveraging knowledge from a source building. Consequently,
it enables the creation of data-efficient models that can be used for advanced
control and fault detection & diagnosis. A major limitation of the TL approach
is its inconsistent performance across different sources. Although accurate
source-building selection for a target is crucial, it remains a persistent
challenge.
  We present GenTL, a general transfer learning model for single-family houses
in Central Europe. GenTL can be efficiently fine-tuned to a large variety of
target buildings. It is pretrained on a Long Short-Term Memory (LSTM) network
with data from 450 different buildings. The general transfer learning model
eliminates the need for source-building selection by serving as a universal
source for fine-tuning. Comparative analysis with conventional single-source to
single-target TL demonstrates the efficacy and reliability of the general
pretraining approach. Testing GenTL on 144 target buildings for fine-tuning
reveals an average prediction error (RMSE) reduction of 42.1 % compared to
fine-tuning single-source models.",http://arxiv.org/pdf/2501.13703v1,,False
A Transformer-based Autoregressive Decoder Architecture for Hierarchical Text Classification,23/01/2025,"Younes Yousef, Lukas Galke, Ansgar Scherp","Recent approaches in hierarchical text classification (HTC) rely on the
capabilities of a pre-trained transformer model and exploit the label semantics
and a graph encoder for the label hierarchy. In this paper, we introduce an
effective hierarchical text classifier RADAr (Transformer-based Autoregressive
Decoder Architecture) that is based only on an off-the-shelf RoBERTa
transformer to process the input and a custom autoregressive decoder with two
decoder layers for generating the classification output. Thus, unlike existing
approaches for HTC, the encoder of RADAr has no explicit encoding of the label
hierarchy and the decoder solely relies on the label sequences of the samples
observed during training. We demonstrate on three benchmark datasets that RADAr
achieves results competitive to the state of the art with less training and
inference time. Our model consistently performs better when organizing the
label sequences from children to parents versus the inverse, as done in
existing HTC approaches. Our experiments show that neither the label semantics
nor an explicit graph encoder for the hierarchy is needed. This has strong
practical implications for HTC as the architecture has fewer requirements and
provides a speed-up by a factor of 2 at inference time. Moreover, training a
separate decoder from scratch in conjunction with fine-tuning the encoder
allows future researchers and practitioners to exchange the encoder part as new
models arise. The source code is available at
https://github.com/yousef-younes/RADAr.",http://arxiv.org/pdf/2501.13598v1,10.3233/FAIA240661,False
Contrastive Representation Learning Helps Cross-institutional Knowledge Transfer: A Study in Pediatric Ventilation Management,23/01/2025,"Yuxuan, Liu, Jinpei Han, Padmanabhan Ramnarayan, A. Aldo Faisal","Clinical machine learning deployment across institutions faces significant
challenges when patient populations and clinical practices differ
substantially. We present a systematic framework for cross-institutional
knowledge transfer in clinical time series, demonstrated through pediatric
ventilation management between a general pediatric intensive care unit (PICU)
and a cardiac-focused unit. Using contrastive predictive coding (CPC) for
representation learning, we investigate how different data regimes and
fine-tuning strategies affect knowledge transfer across institutional
boundaries. Our results show that while direct model transfer performs poorly,
CPC with appropriate fine-tuning enables effective knowledge sharing between
institutions, with benefits particularly evident in limited data scenarios.
Analysis of transfer patterns reveals an important asymmetry: temporal
progression patterns transfer more readily than point-of-care decisions,
suggesting practical pathways for cross-institutional deployment. Through a
systematic evaluation of fine-tuning approaches and transfer patterns, our work
provides insights for developing more generalizable clinical decision support
systems while enabling smaller specialized units to leverage knowledge from
larger centers.",http://arxiv.org/pdf/2501.13587v1,,False
Explainable AI-aided Feature Selection and Model Reduction for DRL-based V2X Resource Allocation,23/01/2025,"Nasir Khan, Asmaa Abdallah, Abdulkadir Celik, Ahmed M. Eltawil, Sinem Coleri","Artificial intelligence (AI) is expected to significantly enhance radio
resource management (RRM) in sixth-generation (6G) networks. However, the lack
of explainability in complex deep learning (DL) models poses a challenge for
practical implementation. This paper proposes a novel explainable AI (XAI)-
based framework for feature selection and model complexity reduction in a
model-agnostic manner. Applied to a multi-agent deep reinforcement learning
(MADRL) setting, our approach addresses the joint sub-band assignment and power
allocation problem in cellular vehicle-to-everything (V2X) communications. We
propose a novel two-stage systematic explainability framework leveraging
feature relevance-oriented XAI to simplify the DRL agents. While the former
stage generates a state feature importance ranking of the trained models using
Shapley additive explanations (SHAP)-based importance scores, the latter stage
exploits these importance-based rankings to simplify the state space of the
agents by removing the least important features from the model input.
Simulation results demonstrate that the XAI-assisted methodology achieves 97%
of the original MADRL sum-rate performance while reducing optimal state
features by 28%, average training time by 11%, and trainable weight parameters
by 46% in a network with eight vehicular pairs.",http://arxiv.org/pdf/2501.13552v1,,False
RECALL: Library-Like Behavior In Language Models is Enhanced by Self-Referencing Causal Cycles,23/01/2025,"Munachiso Nwadike, Zangir Iklassov, Toluwani Aremu, Tatsuya Hiraoka, Velibor Bojkovic, Benjamin Heinzerling, Hilal Alqaubeh, Martin Takáč, Kentaro Inui","We introduce the concept of the self-referencing causal cycle (abbreviated
RECALL) - a mechanism that enables large language models (LLMs) to bypass the
limitations of unidirectional causality, which underlies a phenomenon known as
the reversal curse. When an LLM is prompted with sequential data, it often
fails to recall preceding context. For example, when we ask an LLM to recall
the line preceding ""O say does that star-spangled banner yet wave"" in the U.S.
National Anthem, it often fails to correctly return ""Gave proof through the
night that our flag was still there"" - this is due to the reversal curse. It
occurs because language models such as ChatGPT and Llama generate text based on
preceding tokens, requiring facts to be learned and reproduced in a consistent
token order. While the reversal curse is often viewed as a limitation, we offer
evidence of an alternative view: it is not always an obstacle in practice. We
find that RECALL is driven by what we designate as cycle tokens - sequences
that connect different parts of the training data, enabling recall of preceding
tokens from succeeding ones. Through rigorous probabilistic formalization and
controlled experiments, we demonstrate how the cycles they induce influence a
model's ability to reproduce information. To facilitate reproducibility, we
provide our code and experimental details at
https://anonymous.4open.science/r/remember-B0B8/.",http://arxiv.org/pdf/2501.13491v1,,False
MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods,23/01/2025,"Zukang Xu, Yuxuan Yue, Xing Hu, Zhihang Yuan, Zixu Jiang, Zhixuan Chen, Jiangyong Yu, Chen Xu, Sifan Zhou, Dawei Yang","Mamba is an efficient sequence model that rivals Transformers and
demonstrates significant potential as a foundational architecture for various
tasks. Quantization is commonly used in neural networks to reduce model size
and computational latency. However, applying quantization to Mamba remains
underexplored, and existing quantization methods, which have been effective for
CNN and Transformer models, appear inadequate for Mamba models (e.g., Quarot
suffers a 21% accuracy drop on Vim-T$^\dagger$ even under W8A8). We have
pioneered the exploration of this issue and identified several key challenges.
First, significant outliers are present in gate projections, output
projections, and matrix multiplications. Second, Mamba's unique parallel scan
further amplifies these outliers, leading to uneven and heavy-tailed data
distributions. Third, even with the application of the Hadamard transform, the
variance across channels in weights and activations still remains inconsistent.
To these ends, we propose MambaQuant, a post-training quantization (PTQ)
framework consisting of: 1) Karhunen-Loeve Transformation (KLT) enhanced
rotation, rendering the rotation matrix adaptable to diverse channel
distributions. 2) Smooth-Fused rotation, which equalizes channel variances and
can merge additional parameters into model weights. Experiments show that
MambaQuant can quantize both weights and activations into 8-bit with less than
1% accuracy loss for Mamba-based vision and language tasks. To the best of our
knowledge, MambaQuant is the first comprehensive PTQ design for the Mamba
family, paving the way for further advancements in its application.",http://arxiv.org/pdf/2501.13484v1,,False
Robust Amortized Bayesian Inference with Self-Consistency Losses on Unlabeled Data,23/01/2025,"Aayush Mishra, Daniel Habermann, Marvin Schmitt, Stefan T. Radev, Paul-Christian Bürkner","Neural amortized Bayesian inference (ABI) can solve probabilistic inverse
problems orders of magnitude faster than classical methods. However, neural ABI
is not yet sufficiently robust for widespread and safe applicability. In
particular, when performing inference on observations outside of the scope of
the simulated data seen during training, for example, because of model
misspecification, the posterior approximations are likely to become highly
biased. Due to the bad pre-asymptotic behavior of current neural posterior
estimators in the out-of-simulation regime, the resulting estimation biases
cannot be fixed in acceptable time by just simulating more training data. In
this proof-of-concept paper, we propose a semi-supervised approach that enables
training not only on (labeled) simulated data generated from the model, but
also on unlabeled data originating from any source, including real-world data.
To achieve the latter, we exploit Bayesian self-consistency properties that can
be transformed into strictly proper losses without requiring knowledge of true
parameter values, that is, without requiring data labels. The results of our
initial experiments show remarkable improvements in the robustness of ABI on
out-of-simulation data. Even if the observed data is far away from both labeled
and unlabeled training data, inference remains highly accurate. If our findings
also generalize to other scenarios and model classes, we believe that our new
method represents a major breakthrough in neural ABI.",http://arxiv.org/pdf/2501.13483v1,,False
Zero-Shot Trajectory Planning for Signal Temporal Logic Tasks,23/01/2025,"Ruijia Liu, Ancheng Hou, Xiao Yu, Xiang Yin","Signal Temporal Logic (STL) is a powerful specification language for
describing complex temporal behaviors of continuous signals, making it
well-suited for high-level robotic task descriptions. However, generating
executable plans for STL tasks is challenging, as it requires consideration of
the coupling between the task specification and the system dynamics. Existing
approaches either follow a model-based setting that explicitly requires
knowledge of the system dynamics or adopt a task-oriented data-driven approach
to learn plans for specific tasks. In this work, we investigate the problem of
generating executable STL plans for systems whose dynamics are unknown a
priori. We propose a new planning framework that uses only task-agnostic data
during the offline training stage, enabling zero-shot generalization to new STL
tasks. Our framework is hierarchical, involving: (i) decomposing the STL task
into a set of progress and time constraints, (ii) searching for time-aware
waypoints guided by task-agnostic data, and (iii) generating trajectories using
a pre-trained safe diffusion model. Simulation results demonstrate the
effectiveness of our method indeed in achieving zero-shot generalization to
various STL tasks.",http://arxiv.org/pdf/2501.13457v1,,False
Emotion estimation from video footage with LSTM,23/01/2025,Samer Attrah,"Emotion estimation in general is a field that has been studied for a long
time, and several approaches exist using machine learning. in this paper, we
present an LSTM model, that processes the blend-shapes produced by the library
MediaPipe, for a face detected in a live stream of a camera, to estimate the
main emotion from the facial expressions, this model is trained on the FER2013
dataset and delivers a result of 71% accuracy and 62% f1-score which meets the
accuracy benchmark of the FER2013 dataset, with significantly reduced
computation costs. https://github.com/
Samir-atra/Emotion_estimation_from_video_footage_with_LSTM_ML_algorithm",http://arxiv.org/pdf/2501.13432v1,,False
DoMINO: A Decomposable Multi-scale Iterative Neural Operator for Modeling Large Scale Engineering Simulations,23/01/2025,"Rishikesh Ranade, Mohammad Amin Nabian, Kaustubh Tangsali, Alexey Kamenev, Oliver Hennigh, Ram Cherukuri, Sanjay Choudhry","Numerical simulations play a critical role in design and development of
engineering products and processes. Traditional computational methods, such as
CFD, can provide accurate predictions but are computationally expensive,
particularly for complex geometries. Several machine learning (ML) models have
been proposed in the literature to significantly reduce computation time while
maintaining acceptable accuracy. However, ML models often face limitations in
terms of accuracy and scalability and depend on significant mesh downsampling,
which can negatively affect prediction accuracy and generalization. In this
work, we propose a novel ML model architecture, DoMINO (Decomposable
Multi-scale Iterative Neural Operator) developed in NVIDIA Modulus to address
the various challenges of machine learning based surrogate modeling of
engineering simulations. DoMINO is a point cloudbased ML model that uses local
geometric information to predict flow fields on discrete points. The DoMINO
model is validated for the automotive aerodynamics use case using the DrivAerML
dataset. Through our experiments we demonstrate the scalability, performance,
accuracy and generalization of our model to both in-distribution and
out-of-distribution testing samples. Moreover, the results are analyzed using a
range of engineering specific metrics important for validating numerical
simulations.",http://arxiv.org/pdf/2501.13350v1,,False
Sparse identification of nonlinear dynamics and Koopman operators with Shallow Recurrent Decoder Networks,23/01/2025,"Mars Liyao Gao, Jan P. Williams, J. Nathan Kutz","Spatiotemporal modeling of real-world data poses a challenging problem due to
inherent high dimensionality, measurement noise, and expensive data collection
procedures. In this paper, we present Sparse Identification of Nonlinear
Dynamics with SHallow REcurrent Decoder networks (SINDy-SHRED), a method to
jointly solve the sensing and model identification problems with simple
implementation, efficient computation, and robust performance. SINDy-SHRED uses
Gated Recurrent Units (GRUs) to model the temporal sequence of sensor
measurements along with a shallow decoder network to reconstruct the full
spatiotemporal field from the latent state space using only a few available
sensors. Our proposed algorithm introduces a SINDy-based regularization;
beginning with an arbitrary latent state space, the dynamics of the latent
space progressively converges to a SINDy-class functional, provided the
projection remains within the set. In restricting SINDy to a linear model, the
architecture produces a Koopman-SHRED model which enforces a linear latent
space dynamics. We conduct a systematic experimental study including synthetic
PDE data, real-world sensor measurements for sea surface temperature, and
direct video data. With no explicit encoder, SINDy-SHRED and Koopman-SHRED
enable efficient training with minimal hyperparameter tuning and laptop-level
computing; further, it demonstrates robust generalization in a variety of
applications with minimal to no hyperparameter adjustments. Finally, the
interpretable SINDy and Koopman models of latent state dynamics enables
accurate long-term video predictions, achieving state-of-the-art performance
and outperforming all baseline methods considered, including Convolutional
LSTM, PredRNN, ResNet, and SimVP.",http://arxiv.org/pdf/2501.13329v1,,False
Parallel Belief Contraction via Order Aggregation,23/01/2025,"Jake Chandler, Richard Booth","The standard ``serial'' (aka ``singleton'') model of belief contraction
models the manner in which an agent's corpus of beliefs responds to the removal
of a single item of information. One salient extension of this model introduces
the idea of ``parallel'' (aka ``package'' or ``multiple'') change, in which an
entire set of items of information are simultaneously removed. Existing
research on the latter has largely focussed on single-step parallel
contraction: understanding the behaviour of beliefs after a single parallel
contraction. It has also focussed on generalisations to the parallel case of
serial contraction operations whose characteristic properties are extremely
weak. Here we consider how to extend serial contraction operations that obey
stronger properties. Potentially more importantly, we also consider the
iterated case: the behaviour of beliefs after a sequence of parallel
contractions. We propose a general method for extending serial iterated belief
change operators to handle parallel change based on an n-ary generalisation of
Booth & Chandler's TeamQueue binary order aggregators.",http://arxiv.org/pdf/2501.13295v1,,False
