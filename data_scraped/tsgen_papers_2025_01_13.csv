Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
Machine Learning Force-Field Approach for Itinerant Electron Magnets,10/01/2025,"Sheng Zhang, Yunhao Fan, Kotaro Shimizu, Gia-Wei Chern","We review the recent development of machine-learning (ML) force-field
frameworks for Landau-Lifshitz-Gilbert (LLG) dynamics simulations of itinerant
electron magnets, focusing on the general theory and implementations of
symmetry-invariant representations of spin configurations. The crucial
properties that such magnetic descriptors must satisfy are differentiability
with respect to spin rotations and invariance to both lattice point-group
symmetry and internal spin rotation symmetry. We propose an efficient
implementation based on the concept of reference irreducible representations,
modified from the group-theoretical power-spectrum and bispectrum methods. The
ML framework is demonstrated using the s-d models, which are widely applied in
spintronics research. We show that LLG simulations based on local fields
predicted by the trained ML models successfully reproduce representative
non-collinear spin structures, including 120$^\circ$, tetrahedral, and skyrmion
crystal orders of the triangular-lattice s-d models. Large-scale thermal quench
simulations enabled by ML models further reveal intriguing freezing dynamics
and glassy stripe states consisting of skyrmions and bi-merons. Our work
highlights the utility of ML force-field approach to dynamical modeling of
complex spin orders in itinerant electron magnets.",http://arxiv.org/pdf/2501.06171v1,,False
GenMol: A Drug Discovery Generalist with Discrete Diffusion,10/01/2025,"Seul Lee, Karsten Kreis, Srimukh Prasad Veccham, Meng Liu, Danny Reidenbach, Yuxing Peng, Saee Paliwal, Weili Nie, Arash Vahdat","Drug discovery is a complex process that involves multiple scenarios and
stages, such as fragment-constrained molecule generation, hit generation and
lead optimization. However, existing molecular generative models can only
tackle one or two of these scenarios and lack the flexibility to address
various aspects of the drug discovery pipeline. In this paper, we present
Generalist Molecular generative model (GenMol), a versatile framework that
addresses these limitations by applying discrete diffusion to the Sequential
Attachment-based Fragment Embedding (SAFE) molecular representation. GenMol
generates SAFE sequences through non-autoregressive bidirectional parallel
decoding, thereby allowing utilization of a molecular context that does not
rely on the specific token ordering and enhanced computational efficiency.
Moreover, under the discrete diffusion framework, we introduce fragment
remasking, a strategy that optimizes molecules by replacing fragments with
masked tokens and regenerating them, enabling effective exploration of chemical
space. GenMol significantly outperforms the previous GPT-based model trained on
SAFE representations in de novo generation and fragment-constrained generation,
and achieves state-of-the-art performance in goal-directed hit generation and
lead optimization. These experimental results demonstrate that GenMol can
tackle a wide range of drug discovery tasks, providing a unified and versatile
approach for molecular design.",http://arxiv.org/pdf/2501.06158v1,,False
From discrete-time policies to continuous-time diffusion samplers: Asymptotic equivalences and faster training,10/01/2025,"Julius Berner, Lorenz Richter, Marcin Sendera, Jarrid Rector-Brooks, Nikolay Malkin","We study the problem of training neural stochastic differential equations, or
diffusion models, to sample from a Boltzmann distribution without access to
target samples. Existing methods for training such models enforce time-reversal
of the generative and noising processes, using either differentiable simulation
or off-policy reinforcement learning (RL). We prove equivalences between
families of objectives in the limit of infinitesimal discretization steps,
linking entropic RL methods (GFlowNets) with continuous-time objects (partial
differential equations and path space measures). We further show that an
appropriate choice of coarse time discretization during training allows greatly
improved sample efficiency and the use of time-local objectives, achieving
competitive performance on standard sampling benchmarks with reduced
computational cost.",http://arxiv.org/pdf/2501.06148v1,,False
Emergent Symbol-like Number Variables in Artificial Neural Networks,10/01/2025,"Satchel Grant, Noah D. Goodman, James L. McClelland","What types of numeric representations emerge in Neural Networks (NNs)? To
what degree do NNs induce abstract, mutable, slot-like numeric variables, and
in what situations do these representations emerge? How do these
representations change over learning, and how can we understand the neural
implementations in ways that are unified across different NNs? In this work, we
approach these questions by first training sequence based neural systems using
Next Token Prediction (NTP) objectives on numeric tasks. We then seek to
understand the neural solutions through the lens of causal abstractions or
symbolic algorithms. We use a combination of causal interventions and
visualization methods to find that artificial neural models do indeed develop
analogs of interchangeable, mutable, latent number variables purely from the
NTP objective. We then ask how variations on the tasks and model architectures
affect the models' learned solutions to find that these symbol-like numeric
representations do not form for every variant of the task, and transformers
solve the problem in a notably different way than their recurrent counterparts.
We then show how the symbol-like variables change over the course of training
to find a strong correlation between the models' task performance and the
alignment of their symbol-like representations. Lastly, we show that in all
cases, some degree of gradience exists in these neural symbols, highlighting
the difficulty of finding simple, interpretable symbolic stories of how neural
networks perform numeric tasks. Taken together, our results are consistent with
the view that neural networks can approximate interpretable symbolic programs
of number cognition, but the particular program they approximate and the extent
to which they approximate it can vary widely, depending on the network
architecture, training data, extent of training, and network size.",http://arxiv.org/pdf/2501.06141v1,,False
Supervision policies can shape long-term risk management in general-purpose AI models,10/01/2025,"Manuel Cebrian, Emilia Gomez, David Fernandez Llorca","The rapid proliferation and deployment of General-Purpose AI (GPAI) models,
including large language models (LLMs), present unprecedented challenges for AI
supervisory entities. We hypothesize that these entities will need to navigate
an emergent ecosystem of risk and incident reporting, likely to exceed their
supervision capacity. To investigate this, we develop a simulation framework
parameterized by features extracted from the diverse landscape of risk,
incident, or hazard reporting ecosystems, including community-driven platforms,
crowdsourcing initiatives, and expert assessments. We evaluate four supervision
policies: non-prioritized (first-come, first-served), random selection,
priority-based (addressing the highest-priority risks first), and
diversity-prioritized (balancing high-priority risks with comprehensive
coverage across risk types). Our results indicate that while priority-based and
diversity-prioritized policies are more effective at mitigating high-impact
risks, particularly those identified by experts, they may inadvertently neglect
systemic issues reported by the broader community. This oversight can create
feedback loops that amplify certain types of reporting while discouraging
others, leading to a skewed perception of the overall risk landscape. We
validate our simulation results with several real-world datasets, including one
with over a million ChatGPT interactions, of which more than 150,000
conversations were identified as risky. This validation underscores the complex
trade-offs inherent in AI risk supervision and highlights how the choice of
risk management policies can shape the future landscape of AI risks across
diverse GPAI models used in society.",http://arxiv.org/pdf/2501.06137v1,,False
Inferring High-Order Couplings with Neural Networks,10/01/2025,"Aurélien Decelle, Alfonso de Jesús Navas Gómez, Beatriz Seoane","Maximum-entropy methods, rooted in the inverse Ising/Potts problem from
statistical mechanics, have become indispensable tools for modeling pairwise
interactions in disciplines such as bioinformatics, ecology, and neuroscience.
Despite their remarkable success, these methods often overlook high-order
interactions that may be crucial in complex systems. Conversely, while modern
machine learning approaches can capture such interactions, existing
interpretable frameworks are computationally expensive, making it impractical
to assess the relevance of high-order interactions in real-world scenarios.
Restricted Boltzmann Machines (RBMs) offer a computationally efficient
alternative by encoding statistical correlations via hidden nodes in a
bipartite neural network. Here, we present a method that maps RBMs exactly onto
generalized Potts models with interactions of arbitrary high order. This
approach leverages large-$N$ approximations, facilitated by the simple
architecture of the RBM, to enable the efficient extraction of effective
many-body couplings with minimal computational cost. This mapping also enables
the development of a general formal framework for the extraction of effective
higher-order interactions in arbitrarily complex probabilistic models.
Additionally, we introduce a robust formalism for gauge fixing within the
generalized Potts model. We validate our method by accurately recovering two-
and three-body interactions from synthetic datasets. Additionally, applying our
framework to protein sequence data demonstrates its effectiveness in
reconstructing protein contact maps, achieving performance comparable to
state-of-the-art inverse Potts models. These results position RBMs as a
powerful and efficient tool for investigating high-order interactions in
complex systems.",http://arxiv.org/pdf/2501.06108v1,,False
Text2Playlist: Generating Personalized Playlists from Text on Deezer,10/01/2025,"Mathieu Delcluze, Antoine Khoury, Clémence Vast, Valerio Arnaudo, Léa Briand, Walid Bendada, Thomas Bouabça","The streaming service Deezer heavily relies on the search to help users
navigate through its extensive music catalog. Nonetheless, it is primarily
designed to find specific items and does not lead directly to a smooth
listening experience. We present Text2Playlist, a stand-alone tool that
addresses these limitations. Text2Playlist leverages generative AI, music
information retrieval and recommendation systems to generate query-specific and
personalized playlists, successfully deployed at scale.",http://arxiv.org/pdf/2501.05894v1,,False
AdaPRL: Adaptive Pairwise Regression Learning with Uncertainty Estimation for Universal Regression Tasks,10/01/2025,"Fuhang Liang, Rucong Xu, Deng Lin","Current deep regression models usually learn in point-wise way that treat
each sample as an independent input, neglecting the relative ordering among
different data. Consequently, the regression model could neglect the data 's
interrelationships, potentially resulting in suboptimal performance. Moreover,
the existence of aleatoric uncertainty in the training data may drive the model
to capture non-generalizable patterns, contributing to increased overfitting.
To address these issues, we propose a novel adaptive pairwise learning
framework (AdaPRL) for regression tasks which leverages the relative
differences between data points and integrates with deep probabilistic models
to quantify the uncertainty associated with the predictions. Additionally, we
adapt AdaPRL for applications in multi-task learning and multivariate time
series forecasting. Extensive experiments with several real-world regression
datasets including recommendation systems, age estimation, time series
forecasting, natural language understanding, finance, and industry datasets
show that AdaPRL is compatible with different backbone networks in various
tasks and achieves state-of-the-art performance on the vast majority of tasks,
highlighting its notable potential including enhancing prediction accuracy and
ranking ability, increasing generalization capability, improving robustness to
noisy data, improving resilience to reduced data, and enhancing
interpretability, etc.",http://arxiv.org/pdf/2501.05809v1,,False
Robust Counterfactual Explanations under Model Multiplicity Using Multi-Objective Optimization,10/01/2025,Keita Kinjo,"In recent years, explainability in machine learning has gained importance. In
this context, counterfactual explanation (CE), which is an explanation method
that uses examples, has attracted attention. However, it has been pointed out
that CE is not robust when there are multiple machine-learning models. These
problems are important when using machine learning to make safe decisions. In
this paper, we propose robust CEs that introduce a new viewpoint - Pareto
improvement - and a method that uses multi-objective optimization to generate
it. To evaluate the proposed method, we conducted experiments using both
simulated and actual data. The results demonstrate that the proposed method is
robust and useful. We believe that this research will contribute to a wide
range of research areas, such as explainability in machine learning,
decision-making, and action planning based on machine learning.",http://arxiv.org/pdf/2501.05795v1,,False
Covariate Dependent Mixture of Bayesian Networks,10/01/2025,"Roman Marchant, Dario Draca, Gilad Francis, Sahand Assadzadeh, Mathew Varidel, Frank Iorfino, Sally Cripps","Learning the structure of Bayesian networks from data provides insights into
underlying processes and the causal relationships that generate the data, but
its usefulness depends on the homogeneity of the data population, a condition
often violated in real-world applications. In such cases, using a single
network structure for inference can be misleading, as it may not capture
sub-population differences. To address this, we propose a novel approach of
modelling a mixture of Bayesian networks where component probabilities depend
on individual characteristics. Our method identifies both network structures
and demographic predictors of sub-population membership, aiding personalised
interventions. We evaluate our method through simulations and a youth mental
health case study, demonstrating its potential to improve tailored
interventions in health, education, and social policy.",http://arxiv.org/pdf/2501.05745v1,,False
Element-wise Attention Is All You Need,10/01/2025,Guoxin Feng,"The self-attention (SA) mechanism has demonstrated superior performance
across various domains, yet it suffers from substantial complexity during both
training and inference. The next-generation architecture, aiming at retaining
the competitive performance of SA while achieving low-cost inference and
efficient long-sequence training, primarily focuses on three approaches: linear
attention, linear RNNs, and state space models. Although these approaches
achieve reduced complexity than SA, they all have built-in performance
degradation factors, such as diminished “spikiness” and compression of
historical information. In contrast to these approaches, we propose a novel
element-wise attention mechanism, which uses the element-wise squared Euclidean
distance, instead of the dot product operation, to compute similarity and
approximates the quadratic complexity term $\exp(q_{ic}k_{jc})$ with a Taylor
polynomial. This design achieves remarkable efficiency: during training, the
element-wise attention has a complexity of $\mathcal{O}(tLD)$, making
long-sequence training both computationally and memory efficient, where $L$ is
the sequence length, $D$ is the feature dimension, and $t$ is the highest order
of the polynomial; during inference, it can be reformulated as recurrent neural
networks, achieving a inference complexity of $\mathcal{O}(tD)$. Furthermore,
the element-wise attention circumvents the performance degradation factors
present in these approaches and achieves performance comparable to SA in both
causal and non-causal forms.",http://arxiv.org/pdf/2501.05730v1,,False
Network Diffuser for Placing-Scheduling Service Function Chains with Inverse Demonstration,10/01/2025,"Zuyuan Zhang, Vaneet Aggarwal, Tian Lan","Network services are increasingly managed by considering chained-up virtual
network functions and relevant traffic flows, known as the Service Function
Chains (SFCs). To deal with sequential arrivals of SFCs in an online fashion,
we must consider two closely-coupled problems - an SFC placement problem that
maps SFCs to servers/links in the network and an SFC scheduling problem that
determines when each SFC is executed. Solving the whole SFC problem targeting
these two optimizations jointly is extremely challenging. In this paper, we
propose a novel network diffuser using conditional generative modeling for this
SFC placing-scheduling optimization. Recent advances in generative AI and
diffusion models have made it possible to generate high-quality images/videos
and decision trajectories from language description. We formulate the SFC
optimization as a problem of generating a state sequence for planning and
perform graph diffusion on the state trajectories to enable extraction of SFC
decisions, with SFC optimization constraints and objectives as conditions. To
address the lack of demonstration data due to NP-hardness and exponential
problem space of the SFC optimization, we also propose a novel and somewhat
maverick approach -- Rather than solving instances of this difficult
optimization, we start with randomly-generated solutions as input, and then
determine appropriate SFC optimization problems that render these solutions
feasible. This inverse demonstration enables us to obtain sufficient expert
demonstrations, i.e., problem-solution pairs, through further optimization. In
our numerical evaluations, the proposed network diffuser outperforms learning
and heuristic baselines, by $\sim$20\% improvement in SFC reward and $\sim$50\%
reduction in SFC waiting time and blocking rate.",http://arxiv.org/pdf/2501.05673v1,,False
Learning to Measure Quantum Neural Networks,10/01/2025,"Samuel Yen-Chi Chen, Huan-Hsin Tseng, Hsin-Yi Lin, Shinjae Yoo","The rapid progress in quantum computing (QC) and machine learning (ML) has
attracted growing attention, prompting extensive research into quantum machine
learning (QML) algorithms to solve diverse and complex problems. Designing
high-performance QML models demands expert-level proficiency, which remains a
significant obstacle to the broader adoption of QML. A few major hurdles
include crafting effective data encoding techniques and parameterized quantum
circuits, both of which are crucial to the performance of QML models.
Additionally, the measurement phase is frequently overlooked-most current QML
models rely on pre-defined measurement protocols that often fail to account for
the specific problem being addressed. We introduce a novel approach that makes
the observable of the quantum system-specifically, the Hermitian
matrix-learnable. Our method features an end-to-end differentiable learning
framework, where the parameterized observable is trained alongside the ordinary
quantum circuit parameters simultaneously. Using numerical simulations, we show
that the proposed method can identify observables for variational quantum
circuits that lead to improved outcomes, such as higher classification
accuracy, thereby boosting the overall performance of QML models.",http://arxiv.org/pdf/2501.05663v1,,False
Interpretable Enzyme Function Prediction via Residue-Level Detection,10/01/2025,"Zhao Yang, Bing Su, Jiahao Chen, Ji-Rong Wen","Predicting multiple functions labeled with Enzyme Commission (EC) numbers
from the enzyme sequence is of great significance but remains a challenge due
to its sparse multi-label classification nature, i.e., each enzyme is typically
associated with only a few labels out of more than 6000 possible EC numbers.
However, existing machine learning algorithms generally learn a fixed global
representation for each enzyme to classify all functions, thereby they lack
interpretability and the fine-grained information of some function-specific
local residue fragments may be overwhelmed. Here we present an attention-based
framework, namely ProtDETR (Protein Detection Transformer), by casting enzyme
function prediction as a detection problem. It uses a set of learnable
functional queries to adaptatively extract different local representations from
the sequence of residue-level features for predicting different EC numbers.
ProtDETR not only significantly outperforms existing deep learning-based enzyme
function prediction methods, but also provides a new interpretable perspective
on automatically detecting different local regions for identifying different
functions through cross-attentions between queries and residue-level features.
Code is available at https://github.com/yangzhao1230/ProtDETR.",http://arxiv.org/pdf/2501.05644v1,,False
