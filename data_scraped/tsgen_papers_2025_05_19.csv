Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
Modeling cognitive processes of natural reading with transformer-based Language Models,16/05/2025,"Bruno Bianchi, Ferm√≠n Travi, Juan E. Kamienkowski","Recent advances in Natural Language Processing (NLP) have led to the
development of highly sophisticated language models for text generation. In
parallel, neuroscience has increasingly employed these models to explore
cognitive processes involved in language comprehension. Previous research has
shown that models such as N-grams and LSTM networks can partially account for
predictability effects in explaining eye movement behaviors, specifically Gaze
Duration, during reading. In this study, we extend these findings by evaluating
transformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate
this relationship. Our results indicate that these architectures outperform
earlier models in explaining the variance in Gaze Durations recorded from
Rioplantense Spanish readers. However, similar to previous studies, these
models still fail to account for the entirety of the variance captured by human
predictability. These findings suggest that, despite their advancements,
state-of-the-art language models continue to predict language in ways that
differ from human readers.",http://arxiv.org/pdf/2505.11485v1,,False
EdgeWisePersona: A Dataset for On-Device User Profiling from Natural Language Interactions,16/05/2025,"Patryk Bartkowiak, Michal Podstawski","This paper introduces a novel dataset and evaluation benchmark designed to
assess and improve small language models deployable on edge devices, with a
focus on user profiling from multi-session natural language interactions in
smart home environments. At the core of the dataset are structured user
profiles, each defined by a set of routines - context-triggered, repeatable
patterns of behavior that govern how users interact with their home systems.
Using these profiles as input, a large language model (LLM) generates
corresponding interaction sessions that simulate realistic, diverse, and
context-aware dialogues between users and their devices.
  The primary task supported by this dataset is profile reconstruction:
inferring user routines and preferences solely from interactions history. To
assess how well current models can perform this task under realistic
conditions, we benchmarked several state-of-the-art compact language models and
compared their performance against large foundation models. Our results show
that while small models demonstrate some capability in reconstructing profiles,
they still fall significantly short of large models in accurately capturing
user behavior. This performance gap poses a major challenge - particularly
because on-device processing offers critical advantages, such as preserving
user privacy, minimizing latency, and enabling personalized experiences without
reliance on the cloud. By providing a realistic, structured testbed for
developing and evaluating behavioral modeling under these constraints, our
dataset represents a key step toward enabling intelligent, privacy-respecting
AI systems that learn and adapt directly on user-owned devices.",http://arxiv.org/pdf/2505.11417v1,,False
Uncertainty quantification with approximate variational learning for wearable photoplethysmography prediction tasks,16/05/2025,"Ciaran Bench, Vivek Desai, Mohammad Moulaeifard, Nils Strodthoff, Philip Aston, Andrew Thompson","Photoplethysmography (PPG) signals encode information about relative changes
in blood volume that can be used to assess various aspects of cardiac health
non-invasively, e.g.\ to detect atrial fibrillation (AF) or predict blood
pressure (BP). Deep networks are well-equipped to handle the large quantities
of data acquired from wearable measurement devices. However, they lack
interpretability and are prone to overfitting, leaving considerable risk for
poor performance on unseen data and misdiagnosis. Here, we describe the use of
two scalable uncertainty quantification techniques: Monte Carlo Dropout and the
recently proposed Improved Variational Online Newton. These techniques are used
to assess the trustworthiness of models trained to perform AF classification
and BP regression from raw PPG time series. We find that the choice of
hyperparameters has a considerable effect on the predictive performance of the
models and on the quality and composition of predicted uncertainties. E.g. the
stochasticity of the model parameter sampling determines the proportion of the
total uncertainty that is aleatoric, and has varying effects on predictive
performance and calibration quality dependent on the chosen uncertainty
quantification technique and the chosen expression of uncertainty. We find
significant discrepancy in the quality of uncertainties over the predicted
classes, emphasising the need for a thorough evaluation protocol that assesses
local and adaptive calibration. This work suggests that the choice of
hyperparameters must be carefully tuned to balance predictive performance and
calibration quality, and that the optimal parameterisation may vary depending
on the chosen expression of uncertainty.",http://arxiv.org/pdf/2505.11412v1,,False
IISE PG&E Energy Analytics Challenge 2025: Hourly-Binned Regression Models Beat Transformers in Load Forecasting,16/05/2025,"Millend Roy, Vladimir Pyltsov, Yinbo Hu","Accurate electricity load forecasting is essential for grid stability,
resource optimization, and renewable energy integration. While
transformer-based deep learning models like TimeGPT have gained traction in
time-series forecasting, their effectiveness in long-term electricity load
prediction remains uncertain. This study evaluates forecasting models ranging
from classical regression techniques to advanced deep learning architectures
using data from the ESD 2025 competition. The dataset includes two years of
historical electricity load data, alongside temperature and global horizontal
irradiance (GHI) across five sites, with a one-day-ahead forecasting horizon.
Since actual test set load values remain undisclosed, leveraging predicted
values would accumulate errors, making this a long-term forecasting challenge.
We employ (i) Principal Component Analysis (PCA) for dimensionality reduction
and (ii) frame the task as a regression problem, using temperature and GHI as
covariates to predict load for each hour, (iii) ultimately stacking 24 models
to generate yearly forecasts.
  Our results reveal that deep learning models, including TimeGPT, fail to
consistently outperform simpler statistical and machine learning approaches due
to the limited availability of training data and exogenous variables. In
contrast, XGBoost, with minimal feature engineering, delivers the lowest error
rates across all test cases while maintaining computational efficiency. This
highlights the limitations of deep learning in long-term electricity
forecasting and reinforces the importance of model selection based on dataset
characteristics rather than complexity. Our study provides insights into
practical forecasting applications and contributes to the ongoing discussion on
the trade-offs between traditional and modern forecasting methods.",http://arxiv.org/pdf/2505.11390v1,,False
Anti-aliasing of neural distortion effects via model fine tuning,16/05/2025,"Alistair Carson, Alec Wright, Stefan Bilbao","Neural networks have become ubiquitous with guitar distortion effects
modelling in recent years. Despite their ability to yield perceptually
convincing models, they are susceptible to frequency aliasing when driven by
high frequency and high gain inputs. Nonlinear activation functions create both
the desired harmonic distortion and unwanted aliasing distortion as the
bandwidth of the signal is expanded beyond the Nyquist frequency. Here, we
present a method for reducing aliasing in neural models via a teacher-student
fine tuning approach, where the teacher is a pre-trained model with its weights
frozen, and the student is a copy of this with learnable parameters. The
student is fine-tuned against an aliasing-free dataset generated by passing
sinusoids through the original model and removing non-harmonic components from
the output spectra. Our results show that this method significantly suppresses
aliasing for both long-short-term-memory networks (LSTM) and temporal
convolutional networks (TCN). In the majority of our case studies, the
reduction in aliasing was greater than that achieved by two times oversampling.
One side-effect of the proposed method is that harmonic distortion components
are also affected. This adverse effect was found to be model-dependent, with
the LSTM models giving the best balance between anti-aliasing and preserving
the perceived similarity to an analog reference device.",http://arxiv.org/pdf/2505.11375v1,,False
Training NTK to Generalize with KARE,16/05/2025,"Johannes Schwab, Bryan Kelly, Semyon Malamud, Teng Andrea Xu","The performance of the data-dependent neural tangent kernel (NTK; Jacot et
al. (2018)) associated with a trained deep neural network (DNN) often matches
or exceeds that of the full network. This implies that DNN training via
gradient descent implicitly performs kernel learning by optimizing the NTK. In
this paper, we propose instead to optimize the NTK explicitly. Rather than
minimizing empirical risk, we train the NTK to minimize its generalization
error using the recently developed Kernel Alignment Risk Estimator (KARE; Jacot
et al. (2020)). Our simulations and real data experiments show that NTKs
trained with KARE consistently match or significantly outperform the original
DNN and the DNN- induced NTK (the after-kernel). These results suggest that
explicitly trained kernels can outperform traditional end-to-end DNN
optimization in certain settings, challenging the conventional dominance of
DNNs. We argue that explicit training of NTK is a form of over-parametrized
feature learning.",http://arxiv.org/pdf/2505.11347v1,,False
Temporally-Grounded Language Generation: A Benchmark for Real-Time Vision-Language Models,16/05/2025,"Keunwoo Peter Yu, Joyce Chai","Vision-language models (VLMs) have shown remarkable progress in offline tasks
such as image captioning and video question answering. However, real-time
interactive environments impose new demands on VLMs, requiring them to generate
utterances that are not only semantically accurate but also precisely timed. We
identify two core capabilities necessary for such settings --
$\textit{perceptual updating}$ and $\textit{contingency awareness}$ -- and
propose a new benchmark task, $\textbf{Temporally-Grounded Language Generation
(TGLG)}$, to evaluate them. TGLG requires models to generate utterances in
response to streaming video such that both content and timing align with
dynamic visual input. To support this benchmark, we curate evaluation datasets
from sports broadcasting and egocentric human interaction domains, and
introduce a new metric, $\textbf{TRACE}$, to evaluate TGLG by jointly measuring
semantic similarity and temporal alignment. Finally, we present
$\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$,
a model that interleaves visual and linguistic tokens in a time-synchronized
manner, enabling real-time language generation without relying on turn-based
assumptions. Experimental results show that VLM-TSI significantly outperforms a
strong baseline, yet overall performance remains modest -- highlighting the
difficulty of TGLG and motivating further research in real-time VLMs. Code and
data available $\href{https://github.com/yukw777/tglg}{here}$.",http://arxiv.org/pdf/2505.11326v1,,False
Anomaly Detection for Non-stationary Time Series using Recurrent Wavelet Probabilistic Neural Network,16/05/2025,"Pu Yang, J. A. Barria","In this paper, an unsupervised Recurrent Wavelet Probabilistic Neural Network
(RWPNN) is proposed, which aims at detecting anomalies in non-stationary
environments by modelling the temporal features using a nonparametric density
estimation network. The novel framework consists of two components, a Stacked
Recurrent Encoder-Decoder (SREnc-Dec) module that captures temporal features in
a latent space, and a Multi-Receptive-field Wavelet Probabilistic Network
(MRWPN) that creates an ensemble probabilistic model to characterise the latent
space. This formulation extends the standard wavelet probabilistic networks to
wavelet deep probabilistic networks, which can handle higher data
dimensionality. The MRWPN module can adapt to different rates of data variation
in different datasets without imposing strong distribution assumptions,
resulting in a more robust and accurate detection for Time Series Anomaly
Detection (TSAD) tasks in the non-stationary environment. We carry out the
assessment on 45 real-world time series datasets from various domains, verify
the performance of RWPNN in TSAD tasks with several constraints, and show its
ability to provide early warnings for anomalous events.",http://arxiv.org/pdf/2505.11321v1,,False
Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for Aerial Combat Tactics,16/05/2025,"Ardian Selmonaj, Alessandro Antonucci, Adrian Schneider, Michael R√ºegsegger, Matthias Sommer","Artificial intelligence (AI) is reshaping strategic planning, with
Multi-Agent Reinforcement Learning (MARL) enabling coordination among
autonomous agents in complex scenarios. However, its practical deployment in
sensitive military contexts is constrained by the lack of explainability, which
is an essential factor for trust, safety, and alignment with human strategies.
This work reviews and assesses current advances in explainability methods for
MARL with a focus on simulated air combat scenarios. We proceed by adapting
various explainability techniques to different aerial combat scenarios to gain
explanatory insights about the model behavior. By linking AI-generated tactics
with human-understandable reasoning, we emphasize the need for transparency to
ensure reliable deployment and meaningful human-machine interaction. By
illuminating the crucial importance of explainability in advancing MARL for
operational defense, our work supports not only strategic planning but also the
training of military personnel with insightful and comprehensible analyses.",http://arxiv.org/pdf/2505.11311v1,,False
Rethinking Irregular Time Series Forecasting: A Simple yet Effective Baseline,16/05/2025,"Xvyuan Liu, Xiangfei Qiu, Xingjian Wu, Zhengyu Li, Chenjuan Guo, Jilin Hu, Bin Yang","The forecasting of irregular multivariate time series (IMTS) is crucial in
key areas such as healthcare, biomechanics, climate science, and astronomy.
However, achieving accurate and practical predictions is challenging due to two
main factors. First, the inherent irregularity and data missingness in
irregular time series make modeling difficult. Second, most existing methods
are typically complex and resource-intensive. In this study, we propose a
general framework called APN to address these challenges. Specifically, we
design a novel Time-Aware Patch Aggregation (TAPA) module that achieves
adaptive patching. By learning dynamically adjustable patch boundaries and a
time-aware weighted averaging strategy, TAPA transforms the original irregular
sequences into high-quality, regularized representations in a
channel-independent manner. Additionally, we use a simple query module to
effectively integrate historical information while maintaining the model's
efficiency. Finally, predictions are made by a shallow MLP. Experimental
results on multiple real-world datasets show that APN outperforms existing
state-of-the-art methods in both efficiency and accuracy.",http://arxiv.org/pdf/2505.11250v1,,False
A Set-Sequence Model for Time Series,16/05/2025,"Elliot L. Epstein, Apaar Sadhwani, Kay Giesecke","In many financial prediction problems, the behavior of individual units (such
as loans, bonds, or stocks) is influenced by observable unit-level factors and
macroeconomic variables, as well as by latent cross-sectional effects.
Traditional approaches attempt to capture these latent effects via handcrafted
summary features. We propose a Set-Sequence model that eliminates the need for
handcrafted features. The Set model first learns a shared cross-sectional
summary at each period. The Sequence model then ingests the summary-augmented
time series for each unit independently to predict its outcome. Both components
are learned jointly over arbitrary sets sampled during training. Our approach
harnesses the set nature of the cross-section and is computationally efficient,
generating set summaries in linear time relative to the number of units. It is
also flexible, allowing the use of existing sequence models and accommodating a
variable number of units at inference. Empirical evaluations demonstrate that
our Set-Sequence model significantly outperforms benchmarks on stock return
prediction and mortgage behavior tasks. Code will be released.",http://arxiv.org/pdf/2505.11243v1,,False
Massive-STEPS: Massive Semantic Trajectories for Understanding POI Check-ins -- Dataset and Benchmarks,16/05/2025,"Wilson Wongso, Hao Xue, Flora D. Salim","Understanding human mobility through Point-of-Interest (POI) recommendation
is increasingly important for applications such as urban planning, personalized
services, and generative agent simulation. However, progress in this field is
hindered by two key challenges: the over-reliance on older datasets from
2012-2013 and the lack of reproducible, city-level check-in datasets that
reflect diverse global regions. To address these gaps, we present Massive-STEPS
(Massive Semantic Trajectories for Understanding POI Check-ins), a large-scale,
publicly available benchmark dataset built upon the Semantic Trails dataset and
enriched with semantic POI metadata. Massive-STEPS spans 12 geographically and
culturally diverse cities and features more recent (2017-2018) and
longer-duration (24 months) check-in data than prior datasets. We benchmarked a
wide range of POI recommendation models on Massive-STEPS using both supervised
and zero-shot approaches, and evaluated their performance across multiple urban
contexts. By releasing Massive-STEPS, we aim to facilitate reproducible and
equitable research in human mobility and POI recommendation. The dataset and
benchmarking code are available at:
https://github.com/cruiseresearchgroup/Massive-STEPS",http://arxiv.org/pdf/2505.11239v1,,False
"Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization",16/05/2025,"Yanhao Jia, Ji Xie, S Jivaganesh, Hao Li, Xu Wu, Mengmi Zhang","Imagine hearing a dog bark and turning toward the sound only to see a parked
car, while the real, silent dog sits elsewhere. Such sensory conflicts test
perception, yet humans reliably resolve them by prioritizing sound over
misleading visuals. Despite advances in multimodal AI integrating vision and
audio, little is known about how these systems handle cross-modal conflicts or
whether they favor one modality. In this study, we systematically examine
modality bias and conflict resolution in AI sound localization. We assess
leading multimodal models and benchmark them against human performance in
psychophysics experiments across six audiovisual conditions, including
congruent, conflicting, and absent cues. Humans consistently outperform AI,
demonstrating superior resilience to conflicting or missing visuals by relying
on auditory information. In contrast, AI models often default to visual input,
degrading performance to near chance levels. To address this, we finetune a
state-of-the-art model using a stereo audio-image dataset generated via 3D
simulations. Even with limited training data, the refined model surpasses
existing benchmarks. Notably, it also mirrors human-like horizontal
localization bias favoring left-right precision-likely due to the stereo audio
structure reflecting human ear placement. These findings underscore how sensory
input quality and system architecture shape multimodal representation accuracy.",http://arxiv.org/pdf/2505.11217v1,,False
GLOVA: Global and Local Variation-Aware Analog Circuit Design with Risk-Sensitive Reinforcement Learning,16/05/2025,"Dongjun Kim, Junwoo Park, Chaehyeon Shin, Jaeheon Jung, Kyungho Shin, Seungheon Baek, Sanghyuk Heo, Woongrae Kim, Inchul Jeong, Joohwan Cho, Jongsun Park","Analog/mixed-signal circuit design encounters significant challenges due to
performance degradation from process, voltage, and temperature (PVT)
variations. To achieve commercial-grade reliability, iterative manual design
revisions and extensive statistical simulations are required. While several
studies have aimed to automate variation aware analog design to reduce
time-to-market, the substantial mismatches in real-world wafers have not been
thoroughly addressed. In this paper, we present GLOVA, an analog circuit sizing
framework that effectively manages the impact of diverse random mismatches to
improve robustness against PVT variations. In the proposed approach,
risk-sensitive reinforcement learning is leveraged to account for the
reliability bound affected by PVT variations, and ensemble-based critic is
introduced to achieve sample-efficient learning. For design verification, we
also propose $\mu$-$\sigma$ evaluation and simulation reordering method to
reduce simulation costs of identifying failed designs. GLOVA supports
verification through industrial-level PVT variation evaluation methods,
including corner simulation as well as global and local Monte Carlo (MC)
simulations. Compared to previous state-of-the-art variation-aware analog
sizing frameworks, GLOVA achieves up to 80.5$\times$ improvement in sample
efficiency and 76.0$\times$ reduction in time.",http://arxiv.org/pdf/2505.11208v1,,False
On Next-Token Prediction in LLMs: How End Goals Determine the Consistency of Decoding Algorithms,16/05/2025,"Jacob Trauger, Ambuj Tewari","Probabilistic next-token prediction trained using cross-entropy loss is the
basis of most large language models. Given a sequence of previous values,
next-token prediction assigns a probability to each possible next value in the
vocabulary. There are many ways to use next-token prediction to output token
sequences. This paper examines a few of these algorithms (greedy, lookahead,
random sampling, and temperature-scaled random sampling) and studies their
consistency with respect to various goals encoded as loss functions. Although
consistency of surrogate losses with respect to a target loss function is a
well researched topic, we are the first to study it in the context of LLMs (to
the best of our knowledge). We find that, so long as next-token prediction
converges to its true probability distribution, random sampling is consistent
with outputting sequences that mimic sampling from the true probability
distribution. For the other goals, such as minimizing the 0-1 loss on the
entire sequence, we show no polynomial-time algorithm is optimal for all
probability distributions and all decoding algorithms studied are only optimal
for a subset of probability distributions. When analyzing these results, we see
that there is a dichotomy created between the goals of information retrieval
and creative generation for the decoding algorithms. This shows that choosing
the correct decoding algorithm based on the desired goal is extremely important
and many of the ones used are lacking theoretical grounding in numerous
scenarios.",http://arxiv.org/pdf/2505.11183v1,,False
Attention on the Sphere,16/05/2025,"Boris Bonev, Max Rietmann, Andrea Paris, Alberto Carpentieri, Thorsten Kurth","We introduce a generalized attention mechanism for spherical domains,
enabling Transformer architectures to natively process data defined on the
two-dimensional sphere - a critical need in fields such as atmospheric physics,
cosmology, and robotics, where preserving spherical symmetries and topology is
essential for physical accuracy. By integrating numerical quadrature weights
into the attention mechanism, we obtain a geometrically faithful spherical
attention that is approximately rotationally equivariant, providing strong
inductive biases and leading to better performance than Cartesian approaches.
To further enhance both scalability and model performance, we propose
neighborhood attention on the sphere, which confines interactions to geodesic
neighborhoods. This approach reduces computational complexity and introduces
the additional inductive bias for locality, while retaining the symmetry
properties of our method. We provide optimized CUDA kernels and
memory-efficient implementations to ensure practical applicability. The method
is validated on three diverse tasks: simulating shallow water equations on the
rotating sphere, spherical image segmentation, and spherical depth estimation.
Across all tasks, our spherical Transformers consistently outperform their
planar counterparts, highlighting the advantage of geometric priors for
learning on spherical domains.",http://arxiv.org/pdf/2505.11157v1,,False
Reinforcement Learning for AMR Charging Decisions: The Impact of Reward and Action Space Design,16/05/2025,"Janik Bischoff, Alexandru Rinciog, Anne Meyer","We propose a novel reinforcement learning (RL) design to optimize the
charging strategy for autonomous mobile robots in large-scale block stacking
warehouses. RL design involves a wide array of choices that can mostly only be
evaluated through lengthy experimentation. Our study focuses on how different
reward and action space configurations, ranging from flexible setups to more
guided, domain-informed design configurations, affect the agent performance.
Using heuristic charging strategies as a baseline, we demonstrate the
superiority of flexible, RL-based approaches in terms of service times.
Furthermore, our findings highlight a trade-off: While more open-ended designs
are able to discover well-performing strategies on their own, they may require
longer convergence times and are less stable, whereas guided configurations
lead to a more stable learning process but display a more limited
generalization potential. Our contributions are threefold. First, we extend
SLAPStack, an open-source, RL-compatible simulation-framework to accommodate
charging strategies. Second, we introduce a novel RL design for tackling the
charging strategy problem. Finally, we introduce several novel adaptive
baseline heuristics and reproducibly evaluate the design using a Proximal
Policy Optimization agent and varying different design configurations, with a
focus on reward.",http://arxiv.org/pdf/2505.11136v1,,False
Scalability of Reinforcement Learning Methods for Dispatching in Semiconductor Frontend Fabs: A Comparison of Open-Source Models with Real Industry Datasets,16/05/2025,"Patrick St√∂ckermann, Henning S√ºdfeld, Alessandro Immordino, Thomas Altenm√ºller, Marc Wegmann, Martin Gebser, Konstantin Schekotihin, Georg Seidel, Chew Wye Chan, Fei Fei Zhang","Benchmark datasets are crucial for evaluating approaches to scheduling or
dispatching in the semiconductor industry during the development and deployment
phases. However, commonly used benchmark datasets like the Minifab or SMT2020
lack the complex details and constraints found in real-world scenarios. To
mitigate this shortcoming, we compare open-source simulation models with a real
industry dataset to evaluate how optimization methods scale with different
levels of complexity. Specifically, we focus on Reinforcement Learning methods,
performing optimization based on policy-gradient and Evolution Strategies. Our
research provides insights into the effectiveness of these optimization methods
and their applicability to realistic semiconductor frontend fab simulations. We
show that our proposed Evolution Strategies-based method scales much better
than a comparable policy-gradient-based approach. Moreover, we identify the
selection and combination of relevant bottleneck tools to control by the agent
as crucial for an efficient optimization. For the generalization across
different loading scenarios and stochastic tool failure patterns, we achieve
advantages when utilizing a diverse training dataset. While the overall
approach is computationally expensive, it manages to scale well with the number
of CPU cores used for training. For the real industry dataset, we achieve an
improvement of up to 4% regarding tardiness and up to 1% regarding throughput.
For the less complex open-source models Minifab and SMT2020, we observe
double-digit percentage improvement in tardiness and single digit percentage
improvement in throughput by use of Evolution Strategies.",http://arxiv.org/pdf/2505.11135v1,,False
PhiNet v2: A Mask-Free Brain-Inspired Vision Foundation Model from Video,16/05/2025,"Makoto Yamada, Kian Ming A. Chai, Ayoub Rhim, Satoki Ishikawa, Mohammad Sabokrou, Yao-Hung Hubert Tsai","Recent advances in self-supervised learning (SSL) have revolutionized
computer vision through innovative architectures and learning objectives, yet
they have not fully leveraged insights from biological visual processing
systems. Recently, a brain-inspired SSL model named PhiNet was proposed; it is
based on a ResNet backbone and operates on static image inputs with strong
augmentation. In this paper, we introduce PhiNet v2, a novel Transformer-based
architecture that processes temporal visual input (that is, sequences of
images) without relying on strong augmentation. Our model leverages variational
inference to learn robust visual representations from continuous input streams,
similar to human visual processing. Through extensive experimentation, we
demonstrate that PhiNet v2 achieves competitive performance compared to
state-of-the-art vision foundation models, while maintaining the ability to
learn from sequential input without strong data augmentation. This work
represents a significant step toward more biologically plausible computer
vision systems that process visual information in a manner more closely aligned
with human cognitive processes.",http://arxiv.org/pdf/2505.11129v1,,False
Conditioning Matters: Training Diffusion Policies is Faster Than You Think,16/05/2025,"Zibin Dong, Yicheng Liu, Yinchuan Li, Hang Zhao, Jianye Hao","Diffusion policies have emerged as a mainstream paradigm for building
vision-language-action (VLA) models. Although they demonstrate strong robot
control capabilities, their training efficiency remains suboptimal. In this
work, we identify a fundamental challenge in conditional diffusion policy
training: when generative conditions are hard to distinguish, the training
objective degenerates into modeling the marginal action distribution, a
phenomenon we term loss collapse. To overcome this, we propose Cocos, a simple
yet general solution that modifies the source distribution in the conditional
flow matching to be condition-dependent. By anchoring the source distribution
around semantics extracted from condition inputs, Cocos encourages stronger
condition integration and prevents the loss collapse. We provide theoretical
justification and extensive empirical results across simulation and real-world
benchmarks. Our method achieves faster convergence and higher success rates
than existing approaches, matching the performance of large-scale pre-trained
VLAs using significantly fewer gradient steps and parameters. Cocos is
lightweight, easy to implement, and compatible with diverse policy
architectures, offering a general-purpose improvement to diffusion policy
training.",http://arxiv.org/pdf/2505.11123v1,,False
Think Twice Before You Act: Enhancing Agent Behavioral Safety with Thought Correction,16/05/2025,"Changyue Jiang, Xudong Pan, Min Yang","LLM-based autonomous agents possess capabilities such as reasoning, tool
invocation, and environment interaction, enabling the execution of complex
multi-step tasks. The internal reasoning process, i.e., thought, of behavioral
trajectory significantly influences tool usage and subsequent actions but can
introduce potential risks. Even minor deviations in the agent's thought may
trigger cascading effects leading to irreversible safety incidents. To address
the safety alignment challenges in long-horizon behavioral trajectories, we
propose Thought-Aligner, a plug-in dynamic thought correction module. Utilizing
a lightweight and resource-efficient model, Thought-Aligner corrects each
high-risk thought on the fly before each action execution. The corrected
thought is then reintroduced to the agent, ensuring safer subsequent decisions
and tool interactions. Importantly, Thought-Aligner modifies only the reasoning
phase without altering the underlying agent framework, making it easy to deploy
and widely applicable to various agent frameworks. To train the Thought-Aligner
model, we construct an instruction dataset across ten representative scenarios
and simulate ReAct execution trajectories, generating 5,000 diverse
instructions and more than 11,400 safe and unsafe thought pairs. The model is
fine-tuned using contrastive learning techniques. Experiments across three
agent safety benchmarks involving 12 different LLMs demonstrate that
Thought-Aligner raises agent behavioral safety from approximately 50% in the
unprotected setting to 90% on average. Additionally, Thought-Aligner maintains
response latency below 100ms with minimal resource usage, demonstrating its
capability for efficient deployment, broad applicability, and timely
responsiveness. This method thus provides a practical dynamic safety solution
for the LLM-based agents.",http://arxiv.org/pdf/2505.11063v1,,False
Conceptual framework for the application of deep neural networks to surface composition reconstruction from Mercury's exospheric data,16/05/2025,"Adrian Kazakov, Anna Milillo, Alessandro Mura, Stavro Ivanovski, Valeria Mangano, Alessandro Aronica, Elisabetta De Angelis, Pier Paolo Di Bartolomeo, Alessandro Brin, Luca Colasanti, Miguel Escalona-Moran, Francesco Lazzarotto, Stefano Massetti, Martina Moroni, Raffaella Noschese, Fabrizio Nuccilli, Stefano Orsini, Christina Plainaki, Rosanna Rispoli, Roberto Sordini, Mirko Stumpo, Nello Vertolli","Surface information derived from exospheric measurements at planetary bodies
complements surface mapping provided by dedicated imagers, offering critical
insights into surface release processes, interactions within the planetary
environment, space weathering, and planetary evolution. This study explores the
feasibility of deriving Mercury's regolith elemental composition from in-situ
measurements of its neutral exosphere using deep neural networks (DNNs). We
present a supervised feed-forward DNN architecture - a multilayer perceptron
(MLP) - that, starting from exospheric densities and proton precipitation
fluxes, predicts the chemical elements of the surface regolith below. It serves
as an estimator for the surface-exosphere interaction and the processes leading
to exosphere formation. Because the DNN requires a comprehensive exospheric
dataset not available from previous missions, this study uses simulated
exosphere components and simulated drivers. Extensive training and testing
campaigns demonstrate the MLP's ability to accurately predict and reconstruct
surface composition maps from these simulated measurements. Although this
initial version does not aim to reproduce Mercury's actual surface composition,
it provides a proof of concept, showcasing the algorithm's robustness and
capacity for handling complex datasets to create estimators for exospheric
generation models. Moreover, our tests reveal substantial potential for further
development, suggesting that this method could significantly enhance the
analysis of complex surface-exosphere interactions and complement planetary
exosphere models. This work anticipates applying the approach to data from the
BepiColombo mission, specifically the SERENA package, whose nominal phase
begins in 2027.",http://arxiv.org/pdf/2505.11053v1,10.5281/zenodo.15394849 10.5281/zenodo.15425584,False
DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy,16/05/2025,"Yuran Wang, Ruihai Wu, Yue Chen, Jiarui Wang, Jiaqi Liang, Ziyu Zhu, Haoran Geng, Jitendra Malik, Pieter Abbeel, Hao Dong","Garment manipulation is a critical challenge due to the diversity in garment
categories, geometries, and deformations. Despite this, humans can effortlessly
handle garments, thanks to the dexterity of our hands. However, existing
research in the field has struggled to replicate this level of dexterity,
primarily hindered by the lack of realistic simulations of dexterous garment
manipulation. Therefore, we propose DexGarmentLab, the first environment
specifically designed for dexterous (especially bimanual) garment manipulation,
which features large-scale high-quality 3D assets for 15 task scenarios, and
refines simulation techniques tailored for garment modeling to reduce the
sim-to-real gap. Previous data collection typically relies on teleoperation or
training expert reinforcement learning (RL) policies, which are labor-intensive
and inefficient. In this paper, we leverage garment structural correspondence
to automatically generate a dataset with diverse trajectories using only a
single expert demonstration, significantly reducing manual intervention.
However, even extensive demonstrations cannot cover the infinite states of
garments, which necessitates the exploration of new algorithms. To improve
generalization across diverse garment shapes and deformations, we propose a
Hierarchical gArment-manipuLation pOlicy (HALO). It first identifies
transferable affordance points to accurately locate the manipulation area, then
generates generalizable trajectories to complete the task. Through extensive
experiments and detailed analysis of our method and baseline, we demonstrate
that HALO consistently outperforms existing methods, successfully generalizing
to previously unseen instances even with significant variations in shape and
deformation where others fail. Our project page is available at:
https://wayrise.github.io/DexGarmentLab/.",http://arxiv.org/pdf/2505.11032v1,,False
Logo-LLM: Local and Global Modeling with Large Language Models for Time Series Forecasting,16/05/2025,"Wenjie Ou, Zhishuo Zhao, Dongyue Guo, Yi Lin","Time series forecasting is critical across multiple domains, where time
series data exhibits both local patterns and global dependencies. While
Transformer-based methods effectively capture global dependencies, they often
overlook short-term local variations in time series. Recent methods that adapt
large language models (LLMs) into time series forecasting inherit this
limitation by treating LLMs as black-box encoders, relying solely on the
final-layer output and underutilizing hierarchical representations. To address
this limitation, we propose Logo-LLM, a novel LLM-based framework that
explicitly extracts and models multi-scale temporal features from different
layers of a pre-trained LLM. Through empirical analysis, we show that shallow
layers of LLMs capture local dynamics in time series, while deeper layers
encode global trends. Moreover, Logo-LLM introduces lightweight Local-Mixer and
Global-Mixer modules to align and integrate features with the temporal input
across layers. Extensive experiments demonstrate that Logo-LLM achieves
superior performance across diverse benchmarks, with strong generalization in
few-shot and zero-shot settings while maintaining low computational overhead.",http://arxiv.org/pdf/2505.11017v1,,False
GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on Genomic Foundation Models,16/05/2025,"Haozheng Luo, Chenghao Qiu, Yimin Wang, Shang Wu, Jiahao Yu, Han Liu, Binghui Wang, Yan Chen","We propose the first unified adversarial attack benchmark for Genomic
Foundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks,
GenoArmory offers the first comprehensive evaluation framework to
systematically assess the vulnerability of GFMs to adversarial attacks.
Methodologically, we evaluate the adversarial robustness of five
state-of-the-art GFMs using four widely adopted attack algorithms and three
defense strategies. Importantly, our benchmark provides an accessible and
comprehensive framework to analyze GFM vulnerabilities with respect to model
architecture, quantization schemes, and training datasets. Additionally, we
introduce GenoAdv, a new adversarial sample dataset designed to improve GFM
safety. Empirically, classification models exhibit greater robustness to
adversarial perturbations compared to generative models, highlighting the
impact of task type on model vulnerability. Moreover, adversarial attacks
frequently target biologically significant genomic regions, suggesting that
these models effectively capture meaningful sequence features.",http://arxiv.org/pdf/2505.10983v1,,False
A Dataset for Spatiotemporal-Sensitive POI Question Answering,16/05/2025,"Xiao Han, Dayan Pan, Xiangyu Zhao, Xuyuan Hu, Zhaolin Deng, Xiangjie Kong, Guojiang Shen","Spatiotemporal relationships are critical in data science, as many prediction
and reasoning tasks require analysis across both spatial and temporal
dimensions--for instance, navigating an unfamiliar city involves planning
itineraries that sequence locations and timing cultural experiences. However,
existing Question-Answering (QA) datasets lack sufficient
spatiotemporal-sensitive questions, making them inadequate benchmarks for
evaluating models' spatiotemporal reasoning capabilities. To address this gap,
we introduce POI-QA, a novel spatiotemporal-sensitive QA dataset centered on
Point of Interest (POI), constructed through three key steps: mining and
aligning open-source vehicle trajectory data from GAIA with high-precision
geographic POI data, rigorous manual validation of noisy spatiotemporal facts,
and generating bilingual (Chinese/English) QA pairs that reflect
human-understandable spatiotemporal reasoning tasks. Our dataset challenges
models to parse complex spatiotemporal dependencies, and evaluations of
state-of-the-art multilingual LLMs (e.g., Qwen2.5-7B, Llama3.1-8B) reveal stark
limitations: even the top-performing model (Qwen2.5-7B fine-tuned with
RAG+LoRA) achieves a top 10 Hit Ratio (HR@10) of only 0.41 on the easiest task,
far below human performance at 0.56. This underscores persistent weaknesses in
LLMs' ability to perform consistent spatiotemporal reasoning, while
highlighting POI-QA as a robust benchmark to advance algorithms sensitive to
spatiotemporal dynamics. The dataset is publicly available at
https://www.kaggle.com/ds/7394666.",http://arxiv.org/pdf/2505.10928v1,,False
A Physics-Informed Convolutional Long Short Term Memory Statistical Model for Fluid Thermodynamics Simulations,16/05/2025,"Luca Menicali, Andrew Grace, David H. Richter, Stefano Castruccio","Fluid thermodynamics underpins atmospheric dynamics, climate science,
industrial applications, and energy systems. However, direct numerical
simulations (DNS) of such systems are computationally prohibitive. To address
this, we present a novel physics-informed spatio-temporal surrogate model for
Rayleigh-B\'enard convection (RBC), a canonical example of convective fluid
flow. Our approach combines convolutional neural networks for spatial feature
extraction with an innovative recurrent architecture inspired by large language
models, comprising a context builder and a sequence generator to capture
temporal dynamics. Inference is penalized with respect to the governing partial
differential equations to ensure physical interpretability. Given the
sensitivity of turbulent convection to initial conditions, we quantify
uncertainty using a conformal prediction framework. This model replicates key
features of RBC dynamics while significantly reducing computational cost,
offering a scalable alternative to DNS for long-term simulations.",http://arxiv.org/pdf/2505.10919v1,,False
REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?,16/05/2025,"Chenxi Jiang, Chuhao Zhou, Jianfei Yang","Robot task planning decomposes human instructions into executable action
sequences that enable robots to complete a series of complex tasks. Although
recent large language model (LLM)-based task planners achieve amazing
performance, they assume that human instructions are clear and straightforward.
However, real-world users are not experts, and their instructions to robots
often contain significant vagueness. Linguists suggest that such vagueness
frequently arises from referring expressions (REs), whose meanings depend
heavily on dialogue context and environment. This vagueness is even more
prevalent among the elderly and children, who robots should serve more. This
paper studies how such vagueness in REs within human instructions affects
LLM-based robot task planning and how to overcome this issue. To this end, we
propose the first robot task planning benchmark with vague REs (REI-Bench),
where we discover that the vagueness of REs can severely degrade robot planning
performance, leading to success rate drops of up to 77.9%. We also observe that
most failure cases stem from missing objects in planners. To mitigate the REs
issue, we propose a simple yet effective approach: task-oriented context
cognition, which generates clear instructions for robots, achieving
state-of-the-art performance compared to aware prompt and chains of thought.
This work contributes to the research community of human-robot interaction
(HRI) by making robot task planning more practical, particularly for non-expert
users, e.g., the elderly and children.",http://arxiv.org/pdf/2505.10872v1,,False
ImputeINR: Time Series Imputation via Implicit Neural Representations for Disease Diagnosis with Missing Data,16/05/2025,"Mengxuan Li, Ke Liu, Jialong Guo, Jiajun Bu, Hongwei Wang, Haishuai Wang","Healthcare data frequently contain a substantial proportion of missing
values, necessitating effective time series imputation to support downstream
disease diagnosis tasks. However, existing imputation methods focus on discrete
data points and are unable to effectively model sparse data, resulting in
particularly poor performance for imputing substantial missing values. In this
paper, we propose a novel approach, ImputeINR, for time series imputation by
employing implicit neural representations (INR) to learn continuous functions
for time series. ImputeINR leverages the merits of INR in that the continuous
functions are not coupled to sampling frequency and have infinite sampling
frequency, allowing ImputeINR to generate fine-grained imputations even on
extremely sparse observed values. Extensive experiments conducted on eight
datasets with five ratios of masked values show the superior imputation
performance of ImputeINR, especially for high missing ratios in time series
data. Furthermore, we validate that applying ImputeINR to impute missing values
in healthcare data enhances the performance of downstream disease diagnosis
tasks. Codes are available.",http://arxiv.org/pdf/2505.10856v1,,False
AutoRAN: Weak-to-Strong Jailbreaking of Large Reasoning Models,16/05/2025,"Jiacheng Liang, Tanqiu Jiang, Yuhui Wang, Rongyi Zhu, Fenglong Ma, Ting Wang","This paper presents AutoRAN, the first automated, weak-to-strong jailbreak
attack framework targeting large reasoning models (LRMs). At its core, AutoRAN
leverages a weak, less-aligned reasoning model to simulate the target model's
high-level reasoning structures, generates narrative prompts, and iteratively
refines candidate prompts by incorporating the target model's intermediate
reasoning steps. We evaluate AutoRAN against state-of-the-art LRMs including
GPT-o3/o4-mini and Gemini-2.5-Flash across multiple benchmark datasets
(AdvBench, HarmBench, and StrongReject). Results demonstrate that AutoRAN
achieves remarkable success rates (approaching 100%) within one or a few turns
across different LRMs, even when judged by a robustly aligned external model.
This work reveals that leveraging weak reasoning models can effectively exploit
the critical vulnerabilities of much more capable reasoning models,
highlighting the need for improved safety measures specifically designed for
reasoning-based models. The code for replicating AutoRAN and running records
are available at: (https://github.com/JACKPURCELL/AutoRAN-public). (warning:
this paper contains potentially harmful content generated by LRMs.)",http://arxiv.org/pdf/2505.10846v1,,False
Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL,16/05/2025,"Songjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, Dongbin Zhao","Large reasoning models (LRMs) are proficient at generating explicit,
step-by-step reasoning sequences before producing final answers. However, such
detailed reasoning can introduce substantial computational overhead and
latency, particularly for simple problems. To address this over-thinking
problem, we explore how to equip LRMs with adaptive thinking capabilities:
enabling them to dynamically decide whether or not to engage in explicit
reasoning based on problem complexity. Building on R1-style distilled models,
we observe that inserting a simple ellipsis (""..."") into the prompt can
stochastically trigger either a thinking or no-thinking mode, revealing a
latent controllability in the reasoning behavior. Leveraging this property, we
propose AutoThink, a multi-stage reinforcement learning (RL) framework that
progressively optimizes reasoning policies via stage-wise reward shaping.
AutoThink learns to invoke explicit reasoning only when necessary, while
defaulting to succinct responses for simpler tasks. Experiments on five
mainstream mathematical benchmarks demonstrate that AutoThink achieves
favorable accuracy-efficiency trade-offs compared to recent prompting and
RL-based pruning methods. It can be seamlessly integrated into any R1-style
model, including both distilled and further fine-tuned variants. Notably,
AutoThink improves relative accuracy by 6.4 percent while reducing token usage
by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and
adaptive reasoning paradigm for LRMs.",http://arxiv.org/pdf/2505.10832v1,,False
Completely Weakly Supervised Class-Incremental Learning for Semantic Segmentation,16/05/2025,"David Minkwan Kim, Soeun Lee, Byeongkeun Kang","This work addresses the task of completely weakly supervised
class-incremental learning for semantic segmentation to learn segmentation for
both base and additional novel classes using only image-level labels. While
class-incremental semantic segmentation (CISS) is crucial for handling diverse
and newly emerging objects in the real world, traditional CISS methods require
expensive pixel-level annotations for training. To overcome this limitation,
partially weakly-supervised approaches have recently been proposed. However, to
the best of our knowledge, this is the first work to introduce a completely
weakly-supervised method for CISS. To achieve this, we propose to generate
robust pseudo-labels by combining pseudo-labels from a localizer and a sequence
of foundation models based on their uncertainty. Moreover, to mitigate
catastrophic forgetting, we introduce an exemplar-guided data augmentation
method that generates diverse images containing both previous and novel classes
with guidance. Finally, we conduct experiments in three common experimental
settings: 15-5 VOC, 10-10 VOC, and COCO-to-VOC, and in two scenarios: disjoint
and overlap. The experimental results demonstrate that our completely weakly
supervised method outperforms even partially weakly supervised methods in the
15-5 VOC and 10-10 VOC settings while achieving competitive accuracy in the
COCO-to-VOC setting.",http://arxiv.org/pdf/2505.10781v1,,False
Context-Aware Probabilistic Modeling with LLM for Multimodal Time Series Forecasting,16/05/2025,"Yueyang Yao, Jiajun Li, Xingyuan Dai, MengMeng Zhang, Xiaoyan Gong, Fei-Yue Wang, Yisheng Lv","Time series forecasting is important for applications spanning energy
markets, climate analysis, and traffic management. However, existing methods
struggle to effectively integrate exogenous texts and align them with the
probabilistic nature of large language models (LLMs). Current approaches either
employ shallow text-time series fusion via basic prompts or rely on
deterministic numerical decoding that conflict with LLMs' token-generation
paradigm, which limits contextual awareness and distribution modeling. To
address these limitations, we propose CAPTime, a context-aware probabilistic
multimodal time series forecasting method that leverages text-informed
abstraction and autoregressive LLM decoding. Our method first encodes temporal
patterns using a pretrained time series encoder, then aligns them with textual
contexts via learnable interactions to produce joint multimodal
representations. By combining a mixture of distribution experts with frozen
LLMs, we enable context-aware probabilistic forecasting while preserving LLMs'
inherent distribution modeling capabilities. Experiments on diverse time series
forecasting tasks demonstrate the superior accuracy and generalization of
CAPTime, particularly in multimodal scenarios. Additional analysis highlights
its robustness in data-scarce scenarios through hybrid probabilistic decoding.",http://arxiv.org/pdf/2505.10774v1,,False
