Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
Implicit Regularization for Tubal Tensor Factorizations via Gradient Descent,21/10/2024,"Santhosh Karnik, Anna Veselovska, Mark Iwen, Felix Krahmer","We provide a rigorous analysis of implicit regularization in an
overparametrized tensor factorization problem beyond the lazy training regime.
For matrix factorization problems, this phenomenon has been studied in a number
of works. A particular challenge has been to design universal initialization
strategies which provably lead to implicit regularization in gradient-descent
methods. At the same time, it has been argued by Cohen et. al. 2016 that more
general classes of neural networks can be captured by considering tensor
factorizations. However, in the tensor case, implicit regularization has only
been rigorously established for gradient flow or in the lazy training regime.
In this paper, we prove the first tensor result of its kind for gradient
descent rather than gradient flow. We focus on the tubal tensor product and the
associated notion of low tubal rank, encouraged by the relevance of this model
for image data. We establish that gradient descent in an overparametrized
tensor factorization model with a small random initialization exhibits an
implicit bias towards solutions of low tubal rank. Our theoretical findings are
illustrated in an extensive set of numerical simulations show-casing the
dynamics predicted by our theory as well as the crucial role of using a small
random initialization.",http://arxiv.org/pdf/2410.16247v1,,False
Sketch2Code: Evaluating Vision-Language Models for Interactive Web Design Prototyping,21/10/2024,"Ryan Li, Yanzhe Zhang, Diyi Yang","Sketches are a natural and accessible medium for UI designers to
conceptualize early-stage ideas. However, existing research on UI/UX automation
often requires high-fidelity inputs like Figma designs or detailed screenshots,
limiting accessibility and impeding efficient design iteration. To bridge this
gap, we introduce Sketch2Code, a benchmark that evaluates state-of-the-art
Vision Language Models (VLMs) on automating the conversion of rudimentary
sketches into webpage prototypes. Beyond end-to-end benchmarking, Sketch2Code
supports interactive agent evaluation that mimics real-world design workflows,
where a VLM-based agent iteratively refines its generations by communicating
with a simulated user, either passively receiving feedback instructions or
proactively asking clarification questions. We comprehensively analyze ten
commercial and open-source models, showing that Sketch2Code is challenging for
existing VLMs; even the most capable models struggle to accurately interpret
sketches and formulate effective questions that lead to steady improvement.
Nevertheless, a user study with UI/UX experts reveals a significant preference
for proactive question-asking over passive feedback reception, highlighting the
need to develop more effective paradigms for multi-turn conversational agents.",http://arxiv.org/pdf/2410.16232v1,,False
Comprehensive benchmarking of large language models for RNA secondary structure prediction,21/10/2024,"L. I. Zablocki, L. A. Bugnon, M. Gerard, L. Di Persia, G. Stegmayer, D. H. Milone","Inspired by the success of large language models (LLM) for DNA and proteins,
several LLM for RNA have been developed recently. RNA-LLM uses large datasets
of RNA sequences to learn, in a self-supervised way, how to represent each RNA
base with a semantically rich numerical vector. This is done under the
hypothesis that obtaining high-quality RNA representations can enhance
data-costly downstream tasks. Among them, predicting the secondary structure is
a fundamental task for uncovering RNA functional mechanisms. In this work we
present a comprehensive experimental analysis of several pre-trained RNA-LLM,
comparing them for the RNA secondary structure prediction task in an unified
deep learning framework. The RNA-LLM were assessed with increasing
generalization difficulty on benchmark datasets. Results showed that two LLM
clearly outperform the other models, and revealed significant challenges for
generalization in low-homology scenarios.",http://arxiv.org/pdf/2410.16212v1,,False
Warped Diffusion: Solving Video Inverse Problems with Image Diffusion Models,21/10/2024,"Giannis Daras, Weili Nie, Karsten Kreis, Alex Dimakis, Morteza Mardani, Nikola Borislavov Kovachki, Arash Vahdat","Using image models naively for solving inverse video problems often suffers
from flickering, texture-sticking, and temporal inconsistency in generated
videos. To tackle these problems, in this paper, we view frames as continuous
functions in the 2D space, and videos as a sequence of continuous warping
transformations between different frames. This perspective allows us to train
function space diffusion models only on images and utilize them to solve
temporally correlated inverse problems. The function space diffusion models
need to be equivariant with respect to the underlying spatial transformations.
To ensure temporal consistency, we introduce a simple post-hoc test-time
guidance towards (self)-equivariant solutions. Our method allows us to deploy
state-of-the-art latent diffusion models such as Stable Diffusion XL to solve
video inverse problems. We demonstrate the effectiveness of our method for
video inpainting and $8\times$ video super-resolution, outperforming existing
techniques based on noise transformations. We provide generated video results:
https://giannisdaras.github.io/warped\_diffusion.github.io/.",http://arxiv.org/pdf/2410.16152v1,,False
A Data-driven Crowd Simulation Framework Integrating Physics-informed Machine Learning with Navigation Potential Fields,21/10/2024,"Runkang Guo, Bin Chen, Qi Zhang, Yong Zhao, Xiao Wang, Zhengqiu Zhu","Traditional rule-based physical models are limited by their reliance on
singular physical formulas and parameters, making it difficult to effectively
tackle the intricate tasks associated with crowd simulation. Recent research
has introduced deep learning methods to tackle these issues, but most current
approaches focus primarily on generating pedestrian trajectories, often lacking
interpretability and failing to provide real-time dynamic simulations.To
address the aforementioned issues, we propose a novel data-driven crowd
simulation framework that integrates Physics-informed Machine Learning (PIML)
with navigation potential fields. Our approach leverages the strengths of both
physical models and PIML. Specifically, we design an innovative
Physics-informed Spatio-temporal Graph Convolutional Network (PI-STGCN) as a
data-driven module to predict pedestrian movement trends based on crowd
spatio-temporal data. Additionally, we construct a physical model of navigation
potential fields based on flow field theory to guide pedestrian movements,
thereby reinforcing physical constraints during the simulation. In our
framework, navigation potential fields are dynamically computed and updated
based on the movement trends predicted by the PI-STGCN, while the updated crowd
dynamics, guided by these fields, subsequently feed back into the PI-STGCN.
Comparative experiments on two publicly available large-scale real-world
datasets across five scenes demonstrate that our proposed framework outperforms
existing rule-based methods in accuracy and fidelity. The similarity between
simulated and actual pedestrian trajectories increases by 10.8%, while the
average error is reduced by 4%. Moreover, our framework exhibits greater
adaptability and better interpretability compared to methods that rely solely
on deep learning for trajectory generation.",http://arxiv.org/pdf/2410.16132v1,,False
SeaDAG: Semi-autoregressive Diffusion for Conditional Directed Acyclic Graph Generation,21/10/2024,"Xinyi Zhou, Xing Li, Yingzhao Lian, Yiwen Wang, Lei Chen, Mingxuan Yuan, Jianye Hao, Guangyong Chen, Pheng Ann Heng","We introduce SeaDAG, a semi-autoregressive diffusion model for conditional
generation of Directed Acyclic Graphs (DAGs). Considering their inherent
layer-wise structure, we simulate layer-wise autoregressive generation by
designing different denoising speed for different layers. Unlike conventional
autoregressive generation that lacks a global graph structure view, our method
maintains a complete graph structure at each diffusion step, enabling
operations such as property control that require the full graph structure.
Leveraging this capability, we evaluate the DAG properties during training by
employing a graph property decoder. We explicitly train the model to learn
graph conditioning with a condition loss, which enhances the diffusion model's
capacity to generate graphs that are both realistic and aligned with specified
properties. We evaluate our method on two representative conditional DAG
generation tasks: (1) circuit generation from truth tables, where precise DAG
structures are crucial for realizing circuit functionality, and (2) molecule
generation based on quantum properties. Our approach demonstrates promising
results, generating high-quality and realistic DAGs that closely align with
given conditions.",http://arxiv.org/pdf/2410.16119v1,,False
ExDBN: Exact learning of Dynamic Bayesian Networks,21/10/2024,"Pavel Rytíř, Aleš Wodecki, Georgios Korpas, Jakub Mareček","Causal learning from data has received much attention in recent years. One
way of capturing causal relationships is by utilizing Bayesian networks. There,
one recovers a weighted directed acyclic graph, in which random variables are
represented by vertices, and the weights associated with each edge represent
the strengths of the causal relationships between them. This concept is
extended to capture dynamic effects by introducing a dependency on past data,
which may be captured by the structural equation model, which is utilized in
the present contribution to formulate a score-based learning approach. A
mixed-integer quadratic program is formulated and an algorithmic solution
proposed, in which the pre-generation of exponentially many acyclicity
constraints is avoided by utilizing the so-called branch-and-cut (""lazy
constraint"") method. Comparing the novel approach to the state of the art, we
show that the proposed approach turns out to produce excellent results when
applied to small and medium-sized synthetic instances of up to 25 time-series.
Lastly, two interesting applications in bio-science and finance, to which the
method is directly applied, further stress the opportunities in developing
highly accurate, globally convergent solvers that can handle modest instances.",http://arxiv.org/pdf/2410.16100v1,,False
TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis,21/10/2024,"Shiyu Wang, Jiawei Li, Xiaoming Shi, Zhou Ye, Baichuan Mo, Wenze Lin, Shengtong Ju, Zhixuan Chu, Ming Jin","Time series analysis plays a critical role in numerous applications,
supporting tasks such as forecasting, classification, anomaly detection, and
imputation. In this work, we present the time series pattern machine (TSPM), a
model designed to excel in a broad range of time series tasks through powerful
representation and pattern extraction capabilities. Traditional time series
models often struggle to capture universal patterns, limiting their
effectiveness across diverse tasks. To address this, we define multiple scales
in the time domain and various resolutions in the frequency domain, employing
various mixing strategies to extract intricate, task-adaptive time series
patterns. Specifically, we introduce a general-purpose TSPM that processes
multi-scale time series using (1) multi-resolution time imaging (MRTI), (2)
time image decomposition (TID), (3) multi-scale mixing (MCM), and (4)
multi-resolution mixing (MRM) to extract comprehensive temporal patterns. MRTI
transforms multi-scale time series into multi-resolution time images, capturing
patterns across both temporal and frequency domains. TID leverages dual-axis
attention to extract seasonal and trend patterns, while MCM hierarchically
aggregates these patterns across scales. MRM adaptively integrates all
representations across resolutions. This method achieves state-of-the-art
performance across 8 time series analytical tasks, consistently surpassing both
general-purpose and task-specific models. Our work marks a promising step
toward the next generation of TSPMs, paving the way for further advancements in
time series analysis.",http://arxiv.org/pdf/2410.16032v1,,False
MultiRC: Joint Learning for Time Series Anomaly Prediction and Detection with Multi-scale Reconstructive Contrast,21/10/2024,"Shiyan Hu, Kai Zhao, Xiangfei Qiu, Yang Shu, Jilin Hu, Bin Yang, Chenjuan Guo","Many methods have been proposed for unsupervised time series anomaly
detection. Despite some progress, research on predicting future anomalies is
still relatively scarce. Predicting anomalies is particularly challenging due
to the diverse reaction time and the lack of labeled data. To address these
challenges, we propose MultiRC to integrate reconstructive and contrastive
learning for joint learning of anomaly prediction and detection, with
multi-scale structure and adaptive dominant period mask to deal with the
diverse reaction time. MultiRC also generates negative samples to provide
essential training momentum for the anomaly prediction tasks and prevent model
degradation. We evaluate seven benchmark datasets from different fields. For
both anomaly prediction and detection tasks, MultiRC outperforms existing
state-of-the-art methods.",http://arxiv.org/pdf/2410.15997v1,,False
LLM4GRN: Discovering Causal Gene Regulatory Networks with LLMs -- Evaluation through Synthetic Data Generation,21/10/2024,"Tejumade Afonja, Ivaxi Sheth, Ruta Binkyte, Waqar Hanif, Thomas Ulas, Matthias Becker, Mario Fritz","Gene regulatory networks (GRNs) represent the causal relationships between
transcription factors (TFs) and target genes in single-cell RNA sequencing
(scRNA-seq) data. Understanding these networks is crucial for uncovering
disease mechanisms and identifying therapeutic targets. In this work, we
investigate the potential of large language models (LLMs) for GRN discovery,
leveraging their learned biological knowledge alone or in combination with
traditional statistical methods. We develop a task-based evaluation strategy to
address the challenge of unavailable ground truth causal graphs. Specifically,
we use the GRNs suggested by LLMs to guide causal synthetic data generation and
compare the resulting data against the original dataset. Our statistical and
biological assessments show that LLMs can support statistical modeling and data
synthesis for biological research.",http://arxiv.org/pdf/2410.15828v1,,False
Arithmetic Transformers Can Length-Generalize in Both Operand Length and Count,21/10/2024,"Hanseul Cho, Jaeyoung Cha, Srinadh Bhojanapalli, Chulhee Yun","Transformers often struggle with length generalization, meaning they fail to
generalize to sequences longer than those encountered during training. While
arithmetic tasks are commonly used to study length generalization, certain
tasks are considered notoriously difficult, e.g., multi-operand addition
(requiring generalization over both the number of operands and their lengths)
and multiplication (requiring generalization over both operand lengths). In
this work, we achieve approximately 2-3x length generalization on both tasks,
which is the first such achievement in arithmetic Transformers. We design
task-specific scratchpads enabling the model to focus on a fixed number of
tokens per each next-token prediction step, and apply multi-level versions of
Position Coupling (Cho et al., 2024; McLeish et al., 2024) to let Transformers
know the right position to attend to. On the theory side, we prove that a
1-layer Transformer using our method can solve multi-operand addition, up to
operand length and operand count that are exponential in embedding dimension.",http://arxiv.org/pdf/2410.15787v1,,False
Learning to Synthesize Graphics Programs for Geometric Artworks,21/10/2024,"Qi Bing, Chaoyi Zhang, Weidong Cai","Creating and understanding art has long been a hallmark of human ability.
When presented with finished digital artwork, professional graphic artists can
intuitively deconstruct and replicate it using various drawing tools, such as
the line tool, paint bucket, and layer features, including opacity and blending
modes. While most recent research in this field has focused on art generation,
proposing a range of methods, these often rely on the concept of artwork being
represented as a final image. To bridge the gap between pixel-level results and
the actual drawing process, we present an approach that treats a set of drawing
tools as executable programs. This method predicts a sequence of steps to
achieve the final image, allowing for understandable and resolution-independent
reproductions under the usage of a set of drawing commands. Our experiments
demonstrate that our program synthesizer, Art2Prog, can comprehensively
understand complex input images and reproduce them using high-quality
executable programs. The experimental results evidence the potential of
machines to grasp higher-level information from images and generate compact
program-level descriptions.",http://arxiv.org/pdf/2410.15768v1,,False
AutoTrain: No-code training for state-of-the-art models,21/10/2024,Abhishek Thakur,"With the advancements in open-source models, training (or finetuning) models
on custom datasets has become a crucial part of developing solutions which are
tailored to specific industrial or open-source applications. Yet, there is no
single tool which simplifies the process of training across different types of
modalities or tasks. We introduce AutoTrain (aka AutoTrain Advanced) -- an
open-source, no code tool/library which can be used to train (or finetune)
models for different kinds of tasks such as: large language model (LLM)
finetuning, text classification/regression, token classification,
sequence-to-sequence task, finetuning of sentence transformers, visual language
model (VLM) finetuning, image classification/regression and even classification
and regression tasks on tabular data. AutoTrain Advanced is an open-source
library providing best practices for training models on custom datasets. The
library is available at https://github.com/huggingface/autotrain-advanced.
AutoTrain can be used in fully local mode or on cloud machines and works with
tens of thousands of models shared on Hugging Face Hub and their variations.",http://arxiv.org/pdf/2410.15735v1,,False
Distributionally Robust Instrumental Variables Estimation,21/10/2024,"Zhaonan Qu, Yongchan Kwon","Instrumental variables (IV) estimation is a fundamental method in
econometrics and statistics for estimating causal effects in the presence of
unobserved confounding. However, challenges such as untestable model
assumptions and poor finite sample properties have undermined its reliability
in practice. Viewing common issues in IV estimation as distributional
uncertainties, we propose DRIVE, a distributionally robust framework of the
classical IV estimation method. When the ambiguity set is based on a
Wasserstein distance, DRIVE minimizes a square root ridge regularized variant
of the two stage least squares (TSLS) objective. We develop a novel asymptotic
theory for this regularized regression estimator based on the square root
ridge, showing that it achieves consistency without requiring the
regularization parameter to vanish. This result follows from a fundamental
property of the square root ridge, which we call ``delayed shrinkage''. This
novel property, which also holds for a class of generalized method of moments
(GMM) estimators, ensures that the estimator is robust to distributional
uncertainties that persist in large samples. We further derive the asymptotic
distribution of Wasserstein DRIVE and propose data-driven procedures to select
the regularization parameter based on theoretical results. Simulation studies
confirm the superior finite sample performance of Wasserstein DRIVE. Thanks to
its regularization and robustness properties, Wasserstein DRIVE could be
preferable in practice, particularly when the practitioner is uncertain about
model assumptions or distributional shifts in data.",http://arxiv.org/pdf/2410.15634v1,,False
All You Need is an Improving Column: Enhancing Column Generation for Parallel Machine Scheduling via Transformers,21/10/2024,"Amira Hijazi, Osman Ozaltin, Reha Uzsoy","We present a neural network-enhanced column generation (CG) approach for a
parallel machine scheduling problem. The proposed approach utilizes an
encoder-decoder attention model, namely the transformer and pointer
architectures, to develop job sequences with negative reduced cost and thus
generate columns to add to the master problem. By training the neural network
offline and using it in inference mode to predict negative reduced costs
columns, we achieve significant computational time savings compared to dynamic
programming (DP). Since the exact DP procedure is used to verify that no
further columns with negative reduced cost can be identified at termination,
the optimality guarantee of the original CG procedure is preserved. For small
to medium-sized instances, our approach achieves an average 45% reduction in
computation time compared to solving the subproblems with DP. Furthermore, the
model generalizes not only to unseen, larger problem instances from the same
probability distribution but also to instances from different probability
distributions than those presented at training time. For large-sized instances,
the proposed approach achieves an 80% improvement in the objective value in
under 500 seconds, demonstrating both its scalability and efficiency.",http://arxiv.org/pdf/2410.15601v1,,False
SSMT: Few-Shot Traffic Forecasting with Single Source Meta-Transfer,21/10/2024,"Kishor Kumar Bhaumik, Minha Kim, Fahim Faisal Niloy, Amin Ahsan Ali, Simon S. Woo","Traffic forecasting in Intelligent Transportation Systems (ITS) is vital for
intelligent traffic prediction. Yet, ITS often relies on data from traffic
sensors or vehicle devices, where certain cities might not have all those smart
devices or enabling infrastructures. Also, recent studies have employed
meta-learning to generalize spatial-temporal traffic networks, utilizing data
from multiple cities for effective traffic forecasting for data-scarce target
cities. However, collecting data from multiple cities can be costly and
time-consuming. To tackle this challenge, we introduce Single Source
Meta-Transfer Learning (SSMT) which relies only on a single source city for
traffic prediction. Our method harnesses this transferred knowledge to enable
few-shot traffic forecasting, particularly when the target city possesses
limited data. Specifically, we use memory-augmented attention to store the
heterogeneous spatial knowledge from the source city and selectively recall
them for the data-scarce target city. We extend the idea of sinusoidal
positional encoding to establish meta-learning tasks by leveraging diverse
temporal traffic patterns from the source city. Moreover, to capture a more
generalized representation of the positions we introduced a meta-positional
encoding that learns the most optimal representation of the temporal pattern
across all the tasks. We experiment on five real-world benchmark datasets to
demonstrate that our method outperforms several existing methods in time series
traffic prediction.",http://arxiv.org/pdf/2410.15589v1,,False
