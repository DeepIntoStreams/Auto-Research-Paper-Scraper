Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward,05/08/2025,"Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek F. Wong, Songyang Zhang, Kai Chen","Answer verification is crucial not only for evaluating large language models
(LLMs) by matching their unstructured outputs against standard answers, but
also serves as the reward model to guide LLM optimization. Most evaluation
frameworks rely on regularized matching or employ general LLMs for answer
verification, which demands extensive, repetitive customization for regex rules
or evaluation prompts. Two fundamental limitations persist in current
methodologies: 1) the absence of comprehensive benchmarks that systematically
evaluate verification capabilities across different LLMs; and 2) the nascent
stage of verifier development, where existing approaches lack both the
robustness to handle complex edge cases and the generalizability across
different domains. In this work, we develop CompassVerifier, an accurate and
robust lightweight verifier model for evaluation and outcome reward. It
demonstrates multi-domain competency spanning math, knowledge, and diverse
reasoning tasks, with the capability to process various answer types, including
multi-subproblems, formulas, and sequence answers, while effectively
identifying abnormal/invalid responses. We introduce VerifierBench benchmark
comprising model outputs collected from multiple data sources, augmented
through manual analysis of metaerror patterns to enhance CompassVerifier. We
anticipate that CompassVerifier and VerifierBench will facilitate answer
verification, evaluation protocols, and reinforcement learning research. Code
and dataset are available at https://github.com/open-compass/CompassVerifier.",http://arxiv.org/pdf/2508.03686v1,,False
Agent Lightning: Train ANY AI Agents with Reinforcement Learning,05/08/2025,"Xufang Luo, Yuge Zhang, Zhiyuan He, Zilong Wang, Siyun Zhao, Dongsheng Li, Luna K. Qiu, Yuqing Yang","We present Agent Lightning, a flexible and extensible framework that enables
Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for
any AI agent. Unlike existing methods that tightly couple RL training with
agent or rely on sequence concatenation with masking, Agent Lightning achieves
complete decoupling between agent execution and training, allowing seamless
integration with existing agents developed via diverse ways (e.g., using
frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from
scratch) with almost ZERO code modifications. By formulating agent execution as
Markov decision process, we define an unified data interface and propose a
hierarchical RL algorithm, LightningRL, which contains a credit assignment
module, allowing us to decompose trajectories generated by ANY agents into
training transition. This enables RL to handle complex interaction logic, such
as multi-agent scenarios and dynamic workflows. For the system design, we
introduce a Training-Agent Disaggregation architecture, and brings agent
observability frameworks into agent runtime, providing a standardized agent
finetuning interface. Experiments across text-to-SQL, retrieval-augmented
generation, and math tool-use tasks demonstrate stable, continuous
improvements, showcasing the framework's potential for real-world agent
training and deployment.",http://arxiv.org/pdf/2508.03680v1,,False
Streaming Generated Gaussian Process Experts for Online Learning and Control,05/08/2025,"Zewen Yang, Dongfa Zhang, Xiaobing Dai, Fengyi Yu, Chi Zhang, Bingkun Huang, Hamid Sadeghian, Sami Haddadin","Gaussian Processes (GPs), as a nonparametric learning method, offer flexible
modeling capabilities and calibrated uncertainty quantification for function
approximations. Additionally, GPs support online learning by efficiently
incorporating new data with polynomial-time computation, making them
well-suited for safety-critical dynamical systems that require rapid
adaptation. However, the inference and online updates of exact GPs, when
processing streaming data, incur cubic computation time and quadratic storage
memory complexity, limiting their scalability to large datasets in real-time
settings. In this paper, we propose a \underline{s}treaming
\underline{k}ernel-induced progressivel\underline{y} generated expert framework
of \underline{G}aussian \underline{p}rocesses (SkyGP) that addresses both
computational and memory constraints by maintaining a bounded set of experts,
while inheriting the learning performance guarantees from exact Gaussian
processes. Furthermore, two SkyGP variants are introduced, each tailored to a
specific objective, either maximizing prediction accuracy (SkyGP-Dense) or
improving computational efficiency (SkyGP-Fast). The effectiveness of SkyGP is
validated through extensive benchmarks and real-time control experiments
demonstrating its superior performance compared to state-of-the-art approaches.",http://arxiv.org/pdf/2508.03679v1,,False
Likelihood Matching for Diffusion Models,05/08/2025,"Lei Qian, Wu Su, Yanqi Huang, Song Xi Chen","We propose a Likelihood Matching approach for training diffusion models by
first establishing an equivalence between the likelihood of the target data
distribution and a likelihood along the sample path of the reverse diffusion.
To efficiently compute the reverse sample likelihood, a quasi-likelihood is
considered to approximate each reverse transition density by a Gaussian
distribution with matched conditional mean and covariance, respectively. The
score and Hessian functions for the diffusion generation are estimated by
maximizing the quasi-likelihood, ensuring a consistent matching of both the
first two transitional moments between every two time points. A stochastic
sampler is introduced to facilitate computation that leverages on both the
estimated score and Hessian information. We establish consistency of the
quasi-maximum likelihood estimation, and provide non-asymptotic convergence
guarantees for the proposed sampler, quantifying the rates of the approximation
errors due to the score and Hessian estimation, dimensionality, and the number
of diffusion steps. Empirical and simulation evaluations demonstrate the
effectiveness of the proposed Likelihood Matching and validate the theoretical
results.",http://arxiv.org/pdf/2508.03636v1,,False
Minimal Convolutional RNNs Accelerate Spatiotemporal Learning,05/08/2025,"Coşku Can Horuz, Sebastian Otte, Martin V. Butz, Matthias Karlbauer","We introduce MinConvLSTM and MinConvGRU, two novel spatiotemporal models that
combine the spatial inductive biases of convolutional recurrent networks with
the training efficiency of minimal, parallelizable RNNs. Our approach extends
the log-domain prefix-sum formulation of MinLSTM and MinGRU to convolutional
architectures, enabling fully parallel training while retaining localized
spatial modeling. This eliminates the need for sequential hidden state updates
during teacher forcing - a major bottleneck in conventional ConvRNN models. In
addition, we incorporate an exponential gating mechanism inspired by the xLSTM
architecture into the MinConvLSTM, which further simplifies the log-domain
computation. Our models are structurally minimal and computationally efficient,
with reduced parameter count and improved scalability. We evaluate our models
on two spatiotemporal forecasting tasks: Navier-Stokes dynamics and real-world
geopotential data. In terms of training speed, our architectures significantly
outperform standard ConvLSTMs and ConvGRUs. Moreover, our models also achieve
lower prediction errors in both domains, even in closed-loop autoregressive
mode. These findings demonstrate that minimal recurrent structures, when
combined with convolutional input aggregation, offer a compelling and efficient
alternative for spatiotemporal sequence modeling, bridging the gap between
recurrent simplicity and spatial complexity.",http://arxiv.org/pdf/2508.03614v1,,False
MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy,05/08/2025,"Wuyang Li, Wentao Pan, Xiaoyuan Liu, Zhendong Luo, Chenxin Li, Hengyu Liu, Din Ping Tsai, Mu Ku Chen, Yixuan Yuan","Miniaturized endoscopy has advanced accurate visual perception within the
human body. Prevailing research remains limited to conventional cameras
employing convex lenses, where the physical constraints with millimetre-scale
thickness impose serious impediments on the micro-level clinical. Recently,
with the emergence of meta-optics, ultra-micro imaging based on metalenses
(micron-scale) has garnered great attention, serving as a promising solution.
However, due to the physical difference of metalens, there is a large gap in
data acquisition and algorithm research. In light of this, we aim to bridge
this unexplored gap, advancing the novel metalens endoscopy. First, we
establish datasets for metalens endoscopy and conduct preliminary optical
simulation, identifying two derived optical issues that physically adhere to
strong optical priors. Second, we propose MetaScope, a novel optics-driven
neural network tailored for metalens endoscopy driven by physical optics.
MetaScope comprises two novel designs: Optics-informed Intensity Adjustment
(OIA), rectifying intensity decay by learning optical embeddings, and
Optics-informed Chromatic Correction (OCC), mitigating chromatic aberration by
learning spatial deformations informed by learned Point Spread Function (PSF)
distributions. To enhance joint learning, we further deploy a gradient-guided
distillation to transfer knowledge from the foundational model adaptively.
Extensive experiments demonstrate that MetaScope not only outperforms
state-of-the-art methods in both metalens segmentation and restoration but also
achieves impressive generalized ability in real biomedical scenes.",http://arxiv.org/pdf/2508.03596v1,,False
SolarSeer: Ultrafast and accurate 24-hour solar irradiance forecasts outperforming numerical weather prediction across the USA,05/08/2025,"Mingliang Bai, Zuliang Fang, Shengyu Tao, Siqi Xiang, Jiang Bian, Yanfei Xiang, Pengcheng Zhao, Weixin Jin, Jonathan A. Weyn, Haiyu Dong, Bin Zhang, Hongyu Sun, Kit Thambiratnam, Qi Zhang, Hongbin Sun, Xuan Zhang, Qiuwei Wu","Accurate 24-hour solar irradiance forecasting is essential for the safe and
economic operation of solar photovoltaic systems. Traditional numerical weather
prediction (NWP) models represent the state-of-the-art in forecasting
performance but rely on computationally costly data assimilation and solving
complicated partial differential equations (PDEs) that simulate atmospheric
physics. Here, we introduce SolarSeer, an end-to-end large artificial
intelligence (AI) model for solar irradiance forecasting across the Contiguous
United States (CONUS). SolarSeer is designed to directly map the historical
satellite observations to future forecasts, eliminating the computational
overhead of data assimilation and PDEs solving. This efficiency allows
SolarSeer to operate over 1,500 times faster than traditional NWP, generating
24-hour cloud cover and solar irradiance forecasts for the CONUS at 5-kilometer
resolution in under 3 seconds. Compared with the state-of-the-art NWP in the
CONUS, i.e., High-Resolution Rapid Refresh (HRRR), SolarSeer significantly
reduces the root mean squared error of solar irradiance forecasting by 27.28%
in reanalysis data and 15.35% across 1,800 stations. SolarSeer also effectively
captures solar irradiance fluctuations and significantly enhances the
first-order irradiance difference forecasting accuracy. SolarSeer's ultrafast,
accurate 24-hour solar irradiance forecasts provide strong support for the
transition to sustainable, net-zero energy systems.",http://arxiv.org/pdf/2508.03590v1,,False
VITA: Variational Pretraining of Transformers for Climate-Robust Crop Yield Forecasting,05/08/2025,"Adib Hasan, Mardavij Roozbehani, Munther Dahleh","Accurate crop yield forecasting is essential for global food security.
However, current AI models systematically underperform when yields deviate from
historical trends. This issue arises from key data challenges, including a
major asymmetry between rich pretraining weather datasets and the limited data
available for fine-tuning. We introduce VITA (Variational Inference Transformer
for Asymmetric data), a variational pretraining framework that addresses this
asymmetry. Instead of relying on input reconstruction, VITA uses detailed
weather variables as proxy targets during pretraining and learns to predict
rich atmospheric states through self-supervised feature masking. This allows
the model to be fine-tuned using only basic weather statistics during
deployment. Applied to 763 counties in the U.S. Corn Belt, VITA achieves
state-of-the-art performance in predicting corn and soybean yields across all
evaluation scenarios. While it consistently delivers superior performance under
normal conditions, its advantages are particularly pronounced during extreme
weather years, with statistically significant improvements (paired t-test, $p
\approx 0.01$). Importantly, VITA outperforms prior frameworks like GNN-RNN
using less data, making it more practical for real-world use--particularly in
data-scarce regions. This work highlights how domain-aware AI design can
overcome data limitations and support resilient agricultural forecasting in a
changing climate.",http://arxiv.org/pdf/2508.03589v1,,False
Semantic-aware Graph-guided Behavior Sequences Generation with Large Language Models for Smart Homes,05/08/2025,"Zhiyao Xu, Dan Zhao, Qingsong Zou, Qing Li, Yong Jiang, Yuhang Wang, Jingyu Xiao","As smart homes become increasingly prevalent, intelligent models are widely
used for tasks such as anomaly detection and behavior prediction. These models
are typically trained on static datasets, making them brittle to behavioral
drift caused by seasonal changes, lifestyle shifts, or evolving routines.
However, collecting new behavior data for retraining is often impractical due
to its slow pace, high cost, and privacy concerns. In this paper, we propose
SmartGen, an LLM-based framework that synthesizes context-aware user behavior
data to support continual adaptation of downstream smart home models. SmartGen
consists of four key components. First, we design a Time and Semantic-aware
Split module to divide long behavior sequences into manageable, semantically
coherent subsequences under dual time-span constraints. Second, we propose
Semantic-aware Sequence Compression to reduce input length while preserving
representative semantics by clustering behavior mapping in latent space. Third,
we introduce Graph-guided Sequence Synthesis, which constructs a behavior
relationship graph and encodes frequent transitions into prompts, guiding the
LLM to generate data aligned with contextual changes while retaining core
behavior patterns. Finally, we design a Two-stage Outlier Filter to identify
and remove implausible or semantically inconsistent outputs, aiming to improve
the factual coherence and behavioral validity of the generated sequences.
Experiments on three real-world datasets demonstrate that SmartGen
significantly enhances model performance on anomaly detection and behavior
prediction tasks under behavioral drift, with anomaly detection improving by
85.43% and behavior prediction by 70.51% on average. The code is available at
https://github.com/horizonsinzqs/SmartGen.",http://arxiv.org/pdf/2508.03484v1,,False
SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering,05/08/2025,"Jan Melechovsky, Ambuj Mehrish, Dorien Herremans","Music recordings often suffer from audio quality issues such as excessive
reverberation, distortion, clipping, tonal imbalances, and a narrowed stereo
image, especially when created in non-professional settings without specialized
equipment or expertise. These problems are typically corrected using separate
specialized tools and manual adjustments. In this paper, we introduce
SonicMaster, the first unified generative model for music restoration and
mastering that addresses a broad spectrum of audio artifacts with text-based
control. SonicMaster is conditioned on natural language instructions to apply
targeted enhancements, or can operate in an automatic mode for general
restoration. To train this model, we construct the SonicMaster dataset, a large
dataset of paired degraded and high-quality tracks by simulating common
degradation types with nineteen degradation functions belonging to five
enhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our
approach leverages a flow-matching generative training paradigm to learn an
audio transformation that maps degraded inputs to their cleaned, mastered
versions guided by text prompts. Objective audio quality metrics demonstrate
that SonicMaster significantly improves sound quality across all artifact
categories. Furthermore, subjective listening tests confirm that listeners
prefer SonicMaster's enhanced outputs over the original degraded audio,
highlighting the effectiveness of our unified approach.",http://arxiv.org/pdf/2508.03448v1,,False
An Auditable Agent Platform For Automated Molecular Optimisation,05/08/2025,"Atabey Ünlü, Phil Rohr, Ahmet Celebi","Drug discovery frequently loses momentum when data, expertise, and tools are
scattered, slowing design cycles. To shorten this loop we built a hierarchical,
tool using agent framework that automates molecular optimisation. A Principal
Researcher defines each objective, a Database agent retrieves target
information, an AI Expert generates de novo scaffolds with a sequence to
molecule deep learning model, a Medicinal Chemist edits them while invoking a
docking tool, a Ranking agent scores the candidates, and a Scientific Critic
polices the logic. Each tool call is summarised and stored causing the full
reasoning path to remain inspectable. The agents communicate through concise
provenance records that capture molecular lineage, to build auditable, molecule
centered reasoning trajectories and reuse successful transformations via in
context learning. Three cycle research loops were run against AKT1 protein
using five large language models. After ranking the models by mean docking
score, we ran 20 independent scale ups on the two top performers. We then
compared the leading LLMs' binding affinity results across three
configurations, LLM only, single agent, and multi agent. Our results reveal an
architectural trade off, the multi agent setting excelled at focused binding
optimization, improving average predicted binding affinity by 31%. In contrast,
single agent runs generated molecules with superior drug like properties at the
cost of less potent binding scores. Unguided LLM runs finished fastest, yet
their lack of transparent tool signals left the validity of their reasoning
paths unverified. These results show that test time scaling, focused feedback
loops and provenance convert general purpose LLMs into auditable systems for
molecular design, and suggest that extending the toolset to ADMET and
selectivity predictors could push research workflows further along the
discovery pipeline.",http://arxiv.org/pdf/2508.03444v1,,False
Spatial Imputation Drives Cross-Domain Alignment for EEG Classification,05/08/2025,"Hongjun Liu, Chao Yao, Yalan Zhang, Xiaokun wang, Xiaojuan Ban","Electroencephalogram (EEG) signal classification faces significant challenges
due to data distribution shifts caused by heterogeneous electrode
configurations, acquisition protocols, and hardware discrepancies across
domains. This paper introduces IMAC, a novel channel-dependent mask and
imputation self-supervised framework that formulates the alignment of
cross-domain EEG data shifts as a spatial time series imputation task. To
address heterogeneous electrode configurations in cross-domain scenarios, IMAC
first standardizes different electrode layouts using a 3D-to-2D positional
unification mapping strategy, establishing unified spatial representations.
Unlike previous mask-based self-supervised representation learning methods,
IMAC introduces spatio-temporal signal alignment. This involves constructing a
channel-dependent mask and reconstruction task framed as a low-to-high
resolution EEG spatial imputation problem. Consequently, this approach
simulates cross-domain variations such as channel omissions and temporal
instabilities, thus enabling the model to leverage the proposed imputer for
robust signal alignment during inference. Furthermore, IMAC incorporates a
disentangled structure that separately models the temporal and spatial
information of the EEG signals separately, reducing computational complexity
while enhancing flexibility and adaptability. Comprehensive evaluations across
10 publicly available EEG datasets demonstrate IMAC's superior performance,
achieving state-of-the-art classification accuracy in both cross-subject and
cross-center validation scenarios. Notably, IMAC shows strong robustness under
both simulated and real-world distribution shifts, surpassing baseline methods
by up to $35$\% in integrity scores while maintaining consistent classification
accuracy.",http://arxiv.org/pdf/2508.03437v1,10.1145/3746027.3755582,False
The Science Fiction Science Method,05/08/2025,"Iyad Rahwan, Azim Shariff, Jean-François Bonnefon","Predicting the social and behavioral impact of future technologies, before
they are achieved, would allow us to guide their development and regulation
before these impacts get entrenched. Traditionally, this prediction has relied
on qualitative, narrative methods. Here we describe a method which uses
experimental methods to simulate future technologies, and collect quantitative
measures of the attitudes and behaviors of participants assigned to controlled
variations of the future. We call this method 'science fiction science'. We
suggest that the reason why this method has not been fully embraced yet,
despite its potential benefits, is that experimental scientists may be
reluctant to engage in work facing such serious validity threats as science
fiction science. To address these threats, we consider possible constraints on
the kind of technology that science fiction science may study, as well as the
unconventional, immersive methods that science fiction science may require. We
seek to provide perspective on the reasons why this method has been
marginalized for so long, what benefits it would bring if it could be built on
strong yet unusual methods, and how we can normalize these methods to help the
diverse community of science fiction scientists to engage in a virtuous cycle
of validity improvement.",http://arxiv.org/pdf/2508.03430v1,10.1038/s41586-025-09194-6,False
Residual Neural Terminal Constraint for MPC-based Collision Avoidance in Dynamic Environments,05/08/2025,"Bojan Derajić, Mohamed-Khalil Bouzidi, Sebastian Bernhard, Wolfgang Hönig","In this paper, we propose a hybrid MPC local planner that uses a
learning-based approximation of a time-varying safe set, derived from local
observations and applied as the MPC terminal constraint. This set can be
represented as a zero-superlevel set of the value function computed via
Hamilton-Jacobi (HJ) reachability analysis, which is infeasible in real-time.
We exploit the property that the HJ value function can be expressed as a
difference of the corresponding signed distance function (SDF) and a
non-negative residual function. The residual component is modeled as a neural
network with non-negative output and subtracted from the computed SDF,
resulting in a real-time value function estimate that is at least as safe as
the SDF by design. Additionally, we parametrize the neural residual by a
hypernetwork to improve real-time performance and generalization properties.
The proposed method is compared with three state-of-the-art methods in
simulations and hardware experiments, achieving up to 30\% higher success rates
compared to the best baseline while requiring a similar computational effort
and producing high-quality (low travel-time) solutions.",http://arxiv.org/pdf/2508.03428v1,,False
NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification,05/08/2025,"Wenshuo Zhang, Leixian Shen, Shuchang Xu, Jindu Wang, Jian Zhao, Huamin Qu, Linping Yuan","Conversational LLMs have been widely adopted by domain users with limited
programming experience to solve domain problems. However, these users often
face misalignment between their intent and generated code, resulting in
frustration and rounds of clarification. This work first investigates the cause
of this misalignment, which dues to bidirectional ambiguity: both user intents
and coding tasks are inherently nonlinear, yet must be expressed and
interpreted through linear prompts and code sequences. To address this, we
propose direct intent-task matching, a new human-LLM interaction paradigm that
externalizes and enables direct manipulation of the LLM understanding, i.e.,
the coding tasks and their relationships inferred by the LLM prior to code
generation. As a proof-of-concept, this paradigm is then implemented in
NeuroSync, which employs a knowledge distillation pipeline to extract LLM
understanding, user intents, and their mappings, and enhances the alignment by
allowing users to intuitively inspect and edit them via visualizations. We
evaluate the algorithmic components of NeuroSync via technical experiments, and
assess its overall usability and effectiveness via a user study (N=12). The
results show that it enhances intent-task alignment, lowers cognitive effort,
and improves coding efficiency.",http://arxiv.org/pdf/2508.02823v1,10.1145/3746059.3747668,False
SCFlow: Implicitly Learning Style and Content Disentanglement with Flow Models,05/08/2025,"Pingchuan Ma, Xiaopei Yang, Yusong Li, Ming Gui, Felix Krause, Johannes Schusterbauer, Björn Ommer","Explicitly disentangling style and content in vision models remains
challenging due to their semantic overlap and the subjectivity of human
perception. Existing methods propose separation through generative or
discriminative objectives, but they still face the inherent ambiguity of
disentangling intertwined concepts. Instead, we ask: Can we bypass explicit
disentanglement by learning to merge style and content invertibly, allowing
separation to emerge naturally? We propose SCFlow, a flow-matching framework
that learns bidirectional mappings between entangled and disentangled
representations. Our approach is built upon three key insights: 1) Training
solely to merge style and content, a well-defined task, enables invertible
disentanglement without explicit supervision; 2) flow matching bridges on
arbitrary distributions, avoiding the restrictive Gaussian priors of diffusion
models and normalizing flows; and 3) a synthetic dataset of 510,000 samples (51
styles $\times$ 10,000 content samples) was curated to simulate disentanglement
through systematic style-content pairing. Beyond controllable generation tasks,
we demonstrate that SCFlow generalizes to ImageNet-1k and WikiArt in zero-shot
settings and achieves competitive performance, highlighting that
disentanglement naturally emerges from the invertible merging process.",http://arxiv.org/pdf/2508.03402v1,,False
Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams,05/08/2025,"Wenxin Mao, Zhitao Wang Long Wang, Sirong Chen, Cuiyun Gao, Luyang Cao, Ziming Liu, Qiming Zhang, Jun Zhou, Zhi Jin","Large language models (LLMs) excel at generating code from natural language
(NL) descriptions. However, the plain textual descriptions are inherently
ambiguous and often fail to capture complex requirements like intricate system
behaviors, conditional logic, and architectural constraints; implicit data
dependencies in service-oriented architectures are difficult to infer and
handle correctly. To bridge this gap, we propose a novel step-by-step code
generation framework named UML2Dep by leveraging unambiguous formal
specifications of complex requirements. First, we introduce an enhanced Unified
Modeling Language (UML) sequence diagram tailored for service-oriented
architectures. This diagram extends traditional visual syntax by integrating
decision tables and API specifications, explicitly formalizing structural
relationships and business logic flows in service interactions to rigorously
eliminate linguistic ambiguity. Second, recognizing the critical role of data
flow, we introduce a dedicated data dependency inference (DDI) task. DDI
systematically constructs an explicit data dependency graph prior to actual
code synthesis. To ensure reliability, we formalize DDI as a constrained
mathematical reasoning task through novel prompting strategies, aligning with
LLMs' excellent mathematical strengths. Additional static parsing and
dependency pruning further reduce context complexity and cognitive load
associated with intricate specifications, thereby enhancing reasoning accuracy
and efficiency.",http://arxiv.org/pdf/2508.03379v1,,False
Bridging ocean wave physics and deep learning: Physics-informed neural operators for nonlinear wavefield reconstruction in real-time,05/08/2025,"Svenja Ehlers, Merten Stender, Norbert Hoffmann","Accurate real-time prediction of phase-resolved ocean wave fields remains a
critical yet largely unsolved problem, primarily due to the absence of
practical data assimilation methods for reconstructing initial conditions from
sparse or indirect wave measurements. While recent advances in supervised deep
learning have shown potential for this purpose, they require large labelled
datasets of ground truth wave data, which are infeasible to obtain in
real-world scenarios. To overcome this limitation, we propose a
Physics-Informed Neural Operator (PINO) framework for reconstructing spatially
and temporally phase-resolved, nonlinear ocean wave fields from sparse
measurements, without the need for ground truth data during training. This is
achieved by embedding residuals of the free surface boundary conditions of
ocean gravity waves into the loss function of the PINO, constraining the
solution space in a soft manner. After training, we validate our approach using
highly realistic synthetic wave data and demonstrate the accurate
reconstruction of nonlinear wave fields from both buoy time series and radar
snapshots. Our results indicate that PINOs enable accurate, real-time
reconstruction and generalize robustly across a wide range of wave conditions,
thereby paving the way for operational, data-driven wave reconstruction and
prediction in realistic marine environments.",http://arxiv.org/pdf/2508.03315v1,,False
ToolVQA: A Dataset for Multi-step Reasoning VQA with External Tools,05/08/2025,"Shaofeng Yin, Ting Lei, Yang Liu","Integrating external tools into Large Foundation Models (LFMs) has emerged as
a promising approach to enhance their problem-solving capabilities. While
existing studies have demonstrated strong performance in tool-augmented Visual
Question Answering (VQA), recent benchmarks reveal significant gaps in
real-world tool-use proficiency, particularly in functionally diverse
multimodal settings requiring multi-step reasoning. In this work, we introduce
ToolVQA, a large-scale multimodal dataset comprising 23K instances, designed to
bridge this gap. Unlike previous datasets that rely on synthetic scenarios and
simplified queries, ToolVQA features real-world visual contexts and challenging
implicit multi-step reasoning tasks, better aligning with real user
interactions. To construct this dataset, we propose ToolEngine, a novel data
generation pipeline that employs Depth-First Search (DFS) with a dynamic
in-context example matching mechanism to simulate human-like tool-use
reasoning. ToolVQA encompasses 10 multimodal tools across 7 diverse task
domains, with an average inference length of 2.78 reasoning steps per instance.
The fine-tuned 7B LFMs on ToolVQA not only achieve impressive performance on
our test set but also surpass the large close-sourced model GPT-3.5-turbo on
various out-of-distribution (OOD) datasets, demonstrating strong
generalizability to real-world tool-use scenarios.",http://arxiv.org/pdf/2508.03284v1,,False
Online Continual Graph Learning,05/08/2025,"Giovanni Donghi, Luca Pasa, Daniele Zambon, Cesare Alippi, Nicolò Navarin","The aim of Continual Learning (CL) is to learn new tasks incrementally while
avoiding catastrophic forgetting. Online Continual Learning (OCL) specifically
focuses on learning efficiently from a continuous stream of data with shifting
distribution. While recent studies explore Continual Learning on graphs
exploiting Graph Neural Networks (GNNs), only few of them focus on a streaming
setting. Yet, many real-world graphs evolve over time, often requiring timely
and online predictions. Current approaches, however, are not well aligned with
the standard OCL setting, partly due to the lack of a clear definition of
online Continual Learning on graphs. In this work, we propose a general
formulation for online Continual Learning on graphs, emphasizing the efficiency
requirements on batch processing over the graph topology, and providing a
well-defined setting for systematic model evaluation. Finally, we introduce a
set of benchmarks and report the performance of several methods in the CL
literature, adapted to our setting.",http://arxiv.org/pdf/2508.03283v1,,False
Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?,05/08/2025,"Junhyuk Choi, Hyeonchu Park, Haemin Lee, Hyebeen Shin, Hyun Joung Jin, Bugeun Kim","Recent advances in Large Language Models (LLMs) have generated significant
interest in their capacity to simulate human-like behaviors, yet most studies
rely on fictional personas rather than actual human data. We address this
limitation by evaluating LLMs' ability to predict individual economic
decision-making using Pay-What-You-Want (PWYW) pricing experiments with real
522 human personas. Our study systematically compares three state-of-the-art
multimodal LLMs using detailed persona information from 522 Korean participants
in cultural consumption scenarios. We investigate whether LLMs can accurately
replicate individual human choices and how persona injection methods affect
prediction performance. Results reveal that while LLMs struggle with precise
individual-level predictions, they demonstrate reasonable group-level
behavioral tendencies. Also, we found that commonly adopted prompting
techniques are not much better than naive prompting methods; reconstruction of
personal narrative nor retrieval augmented generation have no significant gain
against simple prompting method. We believe that these findings can provide the
first comprehensive evaluation of LLMs' capabilities on simulating economic
behavior using real human data, offering empirical guidance for persona-based
simulation in computational social science.",http://arxiv.org/pdf/2508.03262v1,,False
InqEduAgent: Adaptive AI Learning Partners with Gaussian Process Augmentation,05/08/2025,"Tian-Fang Zhao, Wen-Xi Yang","Collaborative partnership matters in inquiry-oriented education. However,
most study partners are selected either rely on experience-based assignments
with little scientific planning or build on rule-based machine assistants,
encountering difficulties in knowledge expansion and inadequate flexibility.
This paper proposes an LLM-empowered agent model for simulating and selecting
learning partners tailored to inquiry-oriented learning, named InqEduAgent.
Generative agents are designed to capture cognitive and evaluative features of
learners in real-world scenarios. Then, an adaptive matching algorithm with
Gaussian process augmentation is formulated to identify patterns within prior
knowledge. Optimal learning-partner matches are provided for learners facing
different exercises. The experimental results show the optimal performance of
InqEduAgent in most knowledge-learning scenarios and LLM environment with
different levels of capabilities. This study promotes the intelligent
allocation of human-based learning partners and the formulation of AI-based
learning partners. The code, data, and appendix are publicly available at
https://github.com/InqEduAgent/InqEduAgent.",http://arxiv.org/pdf/2508.03174v1,,False
CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction,05/08/2025,"Jueon Park, Yein Park, Minju Song, Soyon Park, Donghyeon Lee, Seungheun Baek, Jaewoo Kang","Drug toxicity remains a major challenge in pharmaceutical development. Recent
machine learning models have improved in silico toxicity prediction, but their
reliance on annotated data and lack of interpretability limit their
applicability. This limits their ability to capture organ-specific toxicities
driven by complex biological mechanisms. Large language models (LLMs) offer a
promising alternative through step-by-step reasoning and integration of textual
data, yet prior approaches lack biological context and transparent rationale.
To address this issue, we propose CoTox, a novel framework that integrates LLM
with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox
combines chemical structure data, biological pathways, and gene ontology (GO)
terms to generate interpretable toxicity predictions through step-by-step
reasoning. Using GPT-4o, we show that CoTox outperforms both traditional
machine learning and deep learning model. We further examine its performance
across various LLMs to identify where CoTox is most effective. Additionally, we
find that representing chemical structures with IUPAC names, which are easier
for LLMs to understand than SMILES, enhances the model's reasoning ability and
improves predictive performance. To demonstrate its practical utility in drug
development, we simulate the treatment of relevant cell types with drug and
incorporated the resulting biological context into the CoTox framework. This
approach allow CoTox to generate toxicity predictions aligned with
physiological responses, as shown in case study. This result highlights the
potential of LLM-based frameworks to improve interpretability and support
early-stage drug safety assessment. The code and prompt used in this work are
available at https://github.com/dmis-lab/CoTox.",http://arxiv.org/pdf/2508.03159v1,,False
Rethinking Selectivity in State Space Models: A Minimal Predictive Sufficiency Approach,05/08/2025,"Yiyi Wang, Jian'an Zhang, Hongyi Duan, Haoyang Liu, Qingyang Li","State Space Models (SSMs), particularly recent selective variants like Mamba,
have emerged as a leading architecture for sequence modeling, challenging the
dominance of Transformers. However, the success of these state-of-the-art
models largely relies on heuristically designed selective mechanisms, which
lack a rigorous first-principle derivation. This theoretical gap raises
questions about their optimality and robustness against spurious correlations.
To address this, we introduce the Principle of Predictive Sufficiency, a novel
information-theoretic criterion stipulating that an ideal hidden state should
be a minimal sufficient statistic of the past for predicting the future. Based
on this principle, we propose the Minimal Predictive Sufficiency State Space
Model (MPS-SSM), a new framework where the selective mechanism is guided by
optimizing an objective function derived from our principle. This approach
encourages the model to maximally compress historical information without
losing predictive power, thereby learning to ignore non-causal noise and
spurious patterns. Extensive experiments on a wide range of benchmark datasets
demonstrate that MPS-SSM not only achieves state-of-the-art performance,
significantly outperforming existing models in long-term forecasting and noisy
scenarios, but also exhibits superior robustness. Furthermore, we show that the
MPS principle can be extended as a general regularization framework to enhance
other popular architectures, highlighting its broad potential.",http://arxiv.org/pdf/2508.03158v1,,False
Frontier: Simulating the Next Generation of LLM Inference Systems,05/08/2025,"Yicheng Feng, Xin Tan, Kin Hang Sew, Yimin Jiang, Yibo Zhu, Hong Xu","Large Language Model (LLM) inference is growing increasingly complex with the
rise of Mixture-of-Experts (MoE) models and disaggregated architectures that
decouple components like prefill/decode (PD) or attention/FFN (AF) for
heterogeneous scaling. Existing simulators, architected for co-located, dense
models, are unable to capture the intricate system dynamics of these emerging
paradigms. We present Frontier, a high-fidelity simulator designed from the
ground up for this new landscape. Frontier introduces a unified framework to
model both co-located and disaggregated systems, providing native support for
MoE inference with expert parallelism (EP). It enables the simulation of
complex workflows like cross-cluster expert routing and advanced pipelining
strategies for latency hiding. To ensure fidelity and usability, Frontier
incorporates refined operator models for improved accuracy. Frontier empowers
the community to design and optimize the future of LLM inference at scale.",http://arxiv.org/pdf/2508.03148v1,,False
Long Story Generation via Knowledge Graph and Literary Theory,05/08/2025,"Ge Shi, Kaiyu Huang, Guochen Feng","The generation of a long story consisting of several thousand words is a
sub-task in the field of long text generation~(LTG). Previous research has
addressed this challenge through outline-based generation, which employs a
multi-stage method for generating outlines into stories. However, this approach
suffers from two common issues: almost inevitable theme drift caused by the
loss of memory of previous outlines, and tedious plots with incoherent logic
that are less appealing to human readers.
  In this paper, we propose the multi-agent Story Generator structure to
improve the multi-stage method, using large language models~(LLMs) as the core
components of agents. To avoid theme drift, we introduce a memory storage model
comprising two components: a long-term memory storage that identifies the most
important memories, thereby preventing theme drift; and a short-term memory
storage that retains the latest outlines from each generation round. To
incorporate engaging elements into the story, we design a story theme obstacle
framework based on literary narratology theory that introduces uncertain
factors and evaluation criteria to generate outline. This framework calculates
the similarity of the former storyline and enhances the appeal of the story by
building a knowledge graph and integrating new node content. Additionally, we
establish a multi-agent interaction stage to simulate writer-reader interaction
through dialogue and revise the story text according to feedback, to ensure it
remains consistent and logical. Evaluations against previous methods
demonstrate that our approach can generate higher-quality long stories.",http://arxiv.org/pdf/2508.03137v1,,False
AgentSME for Simulating Diverse Communication Modes in Smart Education,05/08/2025,"Wen-Xi Yang, Tian-Fang Zhao","Generative agent models specifically tailored for smart education are
critical, yet remain relatively underdeveloped. A key challenge stems from the
inherent complexity of educational contexts: learners are human beings with
various cognitive behaviors, and pedagogy is fundamentally centered on
personalized human-to-human communication. To address this issue, this paper
proposes AgentSME, a unified generative agent framework powered by LLM. Three
directional communication modes are considered in the models, namely Solo,
Mono, and Echo, reflecting different types of agency autonomy and communicative
reciprocity. Accuracy is adopted as the primary evaluation metric, complemented
by three diversity indices designed to assess the diversity of reasoning
contents. Six widely used LLMs are tested to validate the robustness of
communication modes across different model tiers, which are equally divided
into base-capacity and high-capacity configurations. The results show that
generative agents that employ the Echo communication mode achieve the highest
accuracy scores, while DeepSeek exhibits the greatest diversity. This study
provides valuable information to improve agent learning capabilities and
inspire smart education models.",http://arxiv.org/pdf/2508.03109v1,,False
VCNet: Recreating High-Level Visual Cortex Principles for Robust Artificial Vision,05/08/2025,"Brennen A. Hill, Zhang Xinyu, Timothy Putra Prasetio","Despite their success in image classification, modern convolutional neural
networks (CNNs) exhibit fundamental limitations, including data inefficiency,
poor out-of-distribution generalization, and vulnerability to adversarial
perturbations. The primate visual system, in contrast, demonstrates superior
efficiency and robustness, suggesting that its architectural principles may
offer a blueprint for more capable artificial vision systems. This paper
introduces Visual Cortex Network (VCNet), a novel neural network architecture
whose design is informed by the macro-scale organization of the primate visual
cortex. VCNet emulates key biological mechanisms, including hierarchical
processing across distinct cortical areas, dual-stream information segregation,
and top-down predictive feedback. We evaluate VCNet on two specialized
benchmarks: the Spots-10 animal pattern dataset and a light field image
classification task. Our results show that VCNet achieves a classification
accuracy of 92.1\% on Spots-10 and 74.4\% on the light field dataset,
surpassing contemporary models of comparable size. This work demonstrates that
integrating neuroscientific principles into network design can lead to more
efficient and robust models, providing a promising direction for addressing
long-standing challenges in machine learning.",http://arxiv.org/pdf/2508.02995v1,,False
