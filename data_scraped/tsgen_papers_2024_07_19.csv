Title,Publication Date,Author(s),Abstract,Link,DOI,Relevant
Temporal Representation Learning for Stock Similarities and Its Applications in Investment Management,18/07/2024,"Yoontae Hwang, Stefan Zohren, Yongjae Lee","In the era of rapid globalization and digitalization, accurate identification
of similar stocks has become increasingly challenging due to the non-stationary
nature of financial markets and the ambiguity in conventional regional and
sector classifications. To address these challenges, we examine SimStock, a
novel temporal self-supervised learning framework that combines techniques from
self-supervised learning (SSL) and temporal domain generalization to learn
robust and informative representations of financial time series data. The
primary focus of our study is to understand the similarities between stocks
from a broader perspective, considering the complex dynamics of the global
financial landscape. We conduct extensive experiments on four real-world
datasets with thousands of stocks and demonstrate the effectiveness of SimStock
in finding similar stocks, outperforming existing methods. The practical
utility of SimStock is showcased through its application to various investment
strategies, such as pairs trading, index tracking, and portfolio optimization,
where it leads to superior performance compared to conventional methods. Our
findings empirically examine the potential of data-driven approach to enhance
investment decision-making and risk management practices by leveraging the
power of temporal self-supervised learning in the face of the ever-changing
global financial landscape.",http://arxiv.org/pdf/2407.13751v1,,False
PASTA: Controllable Part-Aware Shape Generation with Autoregressive Transformers,18/07/2024,"Songlin Li, Despoina Paschalidou, Leonidas Guibas","The increased demand for tools that automate the 3D content creation process
led to tremendous progress in deep generative models that can generate diverse
3D objects of high fidelity. In this paper, we present PASTA, an autoregressive
transformer architecture for generating high quality 3D shapes. PASTA comprises
two main components: An autoregressive transformer that generates objects as a
sequence of cuboidal primitives and a blending network, implemented with a
transformer decoder that composes the sequences of cuboids and synthesizes high
quality meshes for each object. Our model is trained in two stages: First we
train our autoregressive generative model using only annotated cuboidal parts
as supervision and next, we train our blending network using explicit 3D
supervision, in the form of watertight meshes. Evaluations on various ShapeNet
objects showcase the ability of our model to perform shape generation from
diverse inputs \eg from scratch, from a partial object, from text and images,
as well size-guided generation, by explicitly conditioning on a bounding box
that defines the object's boundaries. Moreover, as our model considers the
underlying part-based structure of a 3D object, we are able to select a
specific part and produce shapes with meaningful variations of this part. As
evidenced by our experiments, our model generates 3D shapes that are both more
realistic and diverse than existing part-based and non part-based methods,
while at the same time is simpler to implement and train.",http://arxiv.org/pdf/2407.13677v1,,False
EnergyDiff: Universal Time-Series Energy Data Generation using Diffusion Models,18/07/2024,"Nan Lin, Peter Palensky, Pedro P. Vergara","High-resolution time series data are crucial for operation and planning in
energy systems such as electrical power systems and heating systems. However,
due to data collection costs and privacy concerns, such data is often
unavailable or insufficient for downstream tasks. Data synthesis is a potential
solution for this data scarcity. With the recent development of generative AI,
we propose EnergyDiff, a universal data generation framework for energy time
series data. EnergyDiff builds on state-of-the-art denoising diffusion
probabilistic models, utilizing a proposed denoising network dedicated to
high-resolution time series data and introducing a novel Marginal Calibration
technique. Our extensive experimental results demonstrate that EnergyDiff
achieves significant improvement in capturing temporal dependencies and
marginal distributions compared to baselines, particularly at the 1-minute
resolution. Additionally, EnergyDiff consistently generates high-quality time
series data across diverse energy domains, time resolutions, and at both
customer and transformer levels with reduced computational need.",http://arxiv.org/pdf/2407.13538v1,,False
Model-based Policy Optimization using Symbolic World Model,18/07/2024,"Andrey Gorodetskiy, Konstantin Mironov, Aleksandr Panov","The application of learning-based control methods in robotics presents
significant challenges. One is that model-free reinforcement learning
algorithms use observation data with low sample efficiency. To address this
challenge, a prevalent approach is model-based reinforcement learning, which
involves employing an environment dynamics model. We suggest approximating
transition dynamics with symbolic expressions, which are generated via symbolic
regression. Approximation of a mechanical system with a symbolic model has
fewer parameters than approximation with neural networks, which can potentially
lead to higher accuracy and quality of extrapolation. We use a symbolic
dynamics model to generate trajectories in model-based policy optimization to
improve the sample efficiency of the learning algorithm. We evaluate our
approach across various tasks within simulated environments. Our method
demonstrates superior sample efficiency in these tasks compared to model-free
and model-based baseline methods.",http://arxiv.org/pdf/2407.13518v1,,False
Instance Selection for Dynamic Algorithm Configuration with Reinforcement Learning: Improving Generalization,18/07/2024,"Carolin Benjamins, Gjorgjina Cenikj, Ana Nikolikj, Aditya Mohan, Tome Eftimov, Marius Lindauer","Dynamic Algorithm Configuration (DAC) addresses the challenge of dynamically
setting hyperparameters of an algorithm for a diverse set of instances rather
than focusing solely on individual tasks. Agents trained with Deep
Reinforcement Learning (RL) offer a pathway to solve such settings. However,
the limited generalization performance of these agents has significantly
hindered the application in DAC. Our hypothesis is that a potential bias in the
training instances limits generalization capabilities. We take a step towards
mitigating this by selecting a representative subset of training instances to
overcome overrepresentation and then retraining the agent on this subset to
improve its generalization performance. For constructing the meta-features for
the subset selection, we particularly account for the dynamic nature of the RL
agent by computing time series features on trajectories of actions and rewards
generated by the agent's interaction with the environment. Through empirical
evaluations on the Sigmoid and CMA-ES benchmarks from the standard benchmark
library for DAC, called DACBench, we discuss the potentials of our selection
technique compared to training on the entire instance set. Our results
highlight the efficacy of instance selection in refining DAC policies for
diverse instance spaces.",http://arxiv.org/pdf/2407.13513v1,,False
Spontaneous Style Text-to-Speech Synthesis with Controllable Spontaneous Behaviors Based on Language Models,18/07/2024,"Weiqin Li, Peiji Yang, Yicheng Zhong, Yixuan Zhou, Zhisheng Wang, Zhiyong Wu, Xixin Wu, Helen Meng","Spontaneous style speech synthesis, which aims to generate human-like speech,
often encounters challenges due to the scarcity of high-quality data and
limitations in model capabilities. Recent language model-based TTS systems can
be trained on large, diverse, and low-quality speech datasets, resulting in
highly natural synthesized speech. However, they are limited by the difficulty
of simulating various spontaneous behaviors and capturing prosody variations in
spontaneous speech. In this paper, we propose a novel spontaneous speech
synthesis system based on language models. We systematically categorize and
uniformly model diverse spontaneous behaviors. Moreover, fine-grained prosody
modeling is introduced to enhance the model's ability to capture subtle prosody
variations in spontaneous speech.Experimental results show that our proposed
method significantly outperforms the baseline methods in terms of prosody
naturalness and spontaneous behavior naturalness.",http://arxiv.org/pdf/2407.13509v1,,False
SA-DVAE: Improving Zero-Shot Skeleton-Based Action Recognition by Disentangled Variational Autoencoders,18/07/2024,"Sheng-Wei Li, Zi-Xiang Wei, Wei-Jie Chen, Yi-Hsin Yu, Chih-Yuan Yang, Jane Yung-jen Hsu","Existing zero-shot skeleton-based action recognition methods utilize
projection networks to learn a shared latent space of skeleton features and
semantic embeddings. The inherent imbalance in action recognition datasets,
characterized by variable skeleton sequences yet constant class labels,
presents significant challenges for alignment. To address the imbalance, we
propose SA-DVAE -- Semantic Alignment via Disentangled Variational
Autoencoders, a method that first adopts feature disentanglement to separate
skeleton features into two independent parts -- one is semantic-related and
another is irrelevant -- to better align skeleton and semantic features. We
implement this idea via a pair of modality-specific variational autoencoders
coupled with a total correction penalty. We conduct experiments on three
benchmark datasets: NTU RGB+D, NTU RGB+D 120 and PKU-MMD, and our experimental
results show that SA-DAVE produces improved performance over existing methods.
The code is available at https://github.com/pha123661/SA-DVAE.",http://arxiv.org/pdf/2407.13460v1,,False
The Art of Imitation: Learning Long-Horizon Manipulation Tasks from Few Demonstrations,18/07/2024,"Jan Ole von Hartz, Tim Welschehold, Abhinav Valada, Joschka Boedecker","Task Parametrized Gaussian Mixture Models (TP-GMM) are a sample-efficient
method for learning object-centric robot manipulation tasks. However, there are
several open challenges to applying TP-GMMs in the wild. In this work, we
tackle three crucial challenges synergistically. First, end-effector velocities
are non-Euclidean and thus hard to model using standard GMMs. We thus propose
to factorize the robot's end-effector velocity into its direction and
magnitude, and model them using Riemannian GMMs. Second, we leverage the
factorized velocities to segment and sequence skills from complex demonstration
trajectories. Through the segmentation, we further align skill trajectories and
hence leverage time as a powerful inductive bias. Third, we present a method to
automatically detect relevant task parameters per skill from visual
observations. Our approach enables learning complex manipulation tasks from
just five demonstrations while using only RGB-D observations. Extensive
experimental evaluations on RLBench demonstrate that our approach achieves
state-of-the-art performance with 20-fold improved sample efficiency. Our
policies generalize across different environments, object instances, and object
positions, while the learned skills are reusable.",http://arxiv.org/pdf/2407.13432v1,,False
Towards Dynamic Feature Acquisition on Medical Time Series by Maximizing Conditional Mutual Information,18/07/2024,"Fedor Sergeev, Paola Malsot, Gunnar Rätsch, Vincent Fortuin","Knowing which features of a multivariate time series to measure and when is a
key task in medicine, wearables, and robotics. Better acquisition policies can
reduce costs while maintaining or even improving the performance of downstream
predictors. Inspired by the maximization of conditional mutual information, we
propose an approach to train acquirers end-to-end using only the downstream
loss. We show that our method outperforms random acquisition policy, matches a
model with an unrestrained budget, but does not yet overtake a static
acquisition strategy. We highlight the assumptions and outline avenues for
future work.",http://arxiv.org/pdf/2407.13429v1,,False
Deep Reinforcement Learning for Multi-Objective Optimization: Enhancing Wind Turbine Energy Generation while Mitigating Noise Emissions,18/07/2024,"Martín de Frutos, Oscar A. Marino, David Huergo, Esteban Ferrer","We develop a torque-pitch control framework using deep reinforcement learning
for wind turbines to optimize the generation of wind turbine energy while
minimizing operational noise. We employ a double deep Q-learning, coupled to a
blade element momentum solver, to enable precise control over wind turbine
parameters. In addition to the blade element momentum, we use the wind turbine
acoustic model of Brooks Pope and Marcolini. Through training with simple
winds, the agent learns optimal control policies that allow efficient control
for complex turbulent winds. Our experiments demonstrate that the reinforcement
learning is able to find optima at the Pareto front, when maximizing energy
while minimizing noise. In addition, the adaptability of the reinforcement
learning agent to changing turbulent wind conditions, underscores its efficacy
for real-world applications. We validate the methodology using a SWT2.3-93 wind
turbine with a rated power of 2.3 MW. We compare the reinforcement learning
control to classic controls to show that they are comparable when not taking
into account noise emissions. When including a maximum limit of 45 dB to the
noise produced (100 meters downwind of the turbine), the extracted yearly
energy decreases by 22%. The methodology is flexible and allows for easy tuning
of the objectives and constraints through the reward definitions, resulting in
a flexible multi-objective optimization framework for wind turbine control.
Overall, our findings highlight the potential of RL-based control strategies to
improve wind turbine efficiency while mitigating noise pollution, thus
advancing sustainable energy generation technologies",http://arxiv.org/pdf/2407.13320v1,,False
Sortability of Time Series Data,18/07/2024,"Christopher Lohse, Jonas Wahl","Evaluating the performance of causal discovery algorithms that aim to find
causal relationships between time-dependent processes remains a challenging
topic. In this paper, we show that certain characteristics of datasets, such as
varsortability (Reisach et al. 2021) and $R^2$-sortability (Reisach et al.
2023), also occur in datasets for autocorrelated stationary time series. We
illustrate this empirically using four types of data: simulated data based on
SVAR models and Erd\H{o}s-R\'enyi graphs, the data used in the 2019
causality-for-climate challenge (Runge et al. 2019), real-world river stream
datasets, and real-world data generated by the Causal Chamber of (Gamella et
al. 2024). To do this, we adapt var- and $R^2$-sortability to time series data.
We also investigate the extent to which the performance of score-based causal
discovery methods goes hand in hand with high sortability. Arguably, our most
surprising finding is that the investigated real-world datasets exhibit high
varsortability and low $R^2$-sortability indicating that scales may carry a
significant amount of causal information.",http://arxiv.org/pdf/2407.13313v1,,False
Robust Multivariate Time Series Forecasting against Intra- and Inter-Series Transitional Shift,18/07/2024,"Hui He, Qi Zhang, Kun Yi, Xiaojun Xue, Shoujin Wang, Liang Hu, Longbing Cao","The non-stationary nature of real-world Multivariate Time Series (MTS) data
presents forecasting models with a formidable challenge of the time-variant
distribution of time series, referred to as distribution shift. Existing
studies on the distribution shift mostly adhere to adaptive normalization
techniques for alleviating temporal mean and covariance shifts or time-variant
modeling for capturing temporal shifts. Despite improving model generalization,
these normalization-based methods often assume a time-invariant transition
between outputs and inputs but disregard specific intra-/inter-series
correlations, while time-variant models overlook the intrinsic causes of the
distribution shift. This limits model expressiveness and interpretability of
tackling the distribution shift for MTS forecasting. To mitigate such a
dilemma, we present a unified Probabilistic Graphical Model to Jointly
capturing intra-/inter-series correlations and modeling the time-variant
transitional distribution, and instantiate a neural framework called JointPGM
for non-stationary MTS forecasting. Specifically, JointPGM first employs
multiple Fourier basis functions to learn dynamic time factors and designs two
distinct learners: intra-series and inter-series learners. The intra-series
learner effectively captures temporal dynamics by utilizing temporal gates,
while the inter-series learner explicitly models spatial dynamics through
multi-hop propagation, incorporating Gumbel-softmax sampling. These two types
of series dynamics are subsequently fused into a latent variable, which is
inversely employed to infer time factors, generate final prediction, and
perform reconstruction. We validate the effectiveness and efficiency of
JointPGM through extensive experiments on six highly non-stationary MTS
datasets, achieving state-of-the-art forecasting performance of MTS
forecasting.",http://arxiv.org/pdf/2407.13194v1,,False
SpaDiT: Diffusion Transformer for Spatial Gene Expression Prediction using scRNA-seq,18/07/2024,"Xiaoyu Li, Fangfang Zhu, Wenwen Min","The rapid development of spatial transcriptomics (ST) technologies is
revolutionizing our understanding of the spatial organization of biological
tissues. Current ST methods, categorized into next-generation sequencing-based
(seq-based) and fluorescence in situ hybridization-based (image-based) methods,
offer innovative insights into the functional dynamics of biological tissues.
However, these methods are limited by their cellular resolution and the
quantity of genes they can detect. To address these limitations, we propose
SpaDiT, a deep learning method that utilizes a diffusion generative model to
integrate scRNA-seq and ST data for the prediction of undetected genes. By
employing a Transformer-based diffusion model, SpaDiT not only accurately
predicts unknown genes but also effectively generates the spatial structure of
ST genes. We have demonstrated the effectiveness of SpaDiT through extensive
experiments on both seq-based and image-based ST data. SpaDiT significantly
contributes to ST gene prediction methods with its innovative approach.
Compared to eight leading baseline methods, SpaDiT achieved state-of-the-art
performance across multiple metrics, highlighting its substantial
bioinformatics contribution.",http://arxiv.org/pdf/2407.13182v1,,False
Reconfigurable Intelligent Surface Aided Vehicular Edge Computing: Joint Phase-shift Optimization and Multi-User Power Allocation,18/07/2024,"Kangwei Qi, Qiong Wu, Pingyi Fan, Nan Cheng, Wen Chen, Khaled B. Letaief","Vehicular edge computing (VEC) is an emerging technology with significant
potential in the field of internet of vehicles (IoV), enabling vehicles to
perform intensive computational tasks locally or offload them to nearby edge
devices. However, the quality of communication links may be severely
deteriorated due to obstacles such as buildings, impeding the offloading
process. To address this challenge, we introduce the use of Reconfigurable
Intelligent Surfaces (RIS), which provide alternative communication pathways to
assist vehicular communication. By dynamically adjusting the phase-shift of the
RIS, the performance of VEC systems can be substantially improved. In this
work, we consider a RIS-assisted VEC system, and design an optimal scheme for
local execution power, offloading power, and RIS phase-shift, where random task
arrivals and channel variations are taken into account. To address the scheme,
we propose an innovative deep reinforcement learning (DRL) framework that
combines the Deep Deterministic Policy Gradient (DDPG) algorithm for optimizing
RIS phase-shift coefficients and the Multi-Agent Deep Deterministic Policy
Gradient (MADDPG) algorithm for optimizing the power allocation of vehicle user
(VU). Simulation results show that our proposed scheme outperforms the
traditional centralized DDPG, Twin Delayed Deep Deterministic Policy Gradient
(TD3) and some typical stochastic schemes.",http://arxiv.org/pdf/2407.13123v1,,False
